{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import JSSP\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Environment Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_env(instance_path):\n",
    "    env_name = \"JSSP-v0\"\n",
    "    env = gym.make(env_name, instance_path = instance_path)\n",
    "    print(\"Environment Created for: \", instance_path)\n",
    "    print(\"Observation space: \\n\", env.observation_space)\n",
    "    print(\"Action space: \\n\", env.action_space)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# small environment: 2 jobs 3 machines 3 operations\n",
    "env_s = create_env(\"../Instances/instance1.txt\") # -53\n",
    "# medium environment: 6 jobs 6 machines 6 operations\n",
    "env_m = create_env(\"../Instances/instance5.txt\") # -55\n",
    "# large environment: 10 jobs 6 machines 6 operations\n",
    "env_l = create_env(\"../Instances/instance4.txt\") # -42\n",
    "# extra large environment: 10 jobs 11 machines 10 operations\n",
    "env_xl = create_env(\"../Instances/instance3.txt\") # -959"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Random Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def random_sampling(env, episodes, print_steps = False):\n",
    "    \"\"\"\n",
    "    baseline solution to FJSSP, randomly selects actions for each episode and selects\n",
    "    best set of actions\n",
    "    :param print_steps: boolean dictating whether to print each step\n",
    "    :param env: FJSSP environment\n",
    "    :param episodes: number of episodes\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    env.reset()\n",
    "    max_score = -100000\n",
    "    max_episode = -1\n",
    "    max_action_list = []\n",
    "    max_time_list = []\n",
    "    for episode in range(1, episodes+1):\n",
    "        env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "        action_list = []\n",
    "        time_list = []\n",
    "        while not done:\n",
    "            #env.render()\n",
    "            action = env.action_space.sample()\n",
    "            if action != env.action_space.n -1:\n",
    "                action_list.append(env.legal_allocation_list[action])\n",
    "                time_list.append(env.time)\n",
    "            n_state, reward, done, info = env.step(action)\n",
    "            score+=reward\n",
    "        if print_steps:\n",
    "            print('Episode:{} Total_reward:{}'.format(episode, score))\n",
    "        if score >= max_score:\n",
    "            max_score = score\n",
    "            max_episode = episode\n",
    "            max_action_list = action_list\n",
    "            max_time_list = time_list\n",
    "    print('From {}th Episode best policy has reward {}'.format(max_episode, max_score))\n",
    "    return max_score, max_action_list, max_time_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def policy(state, Q, epsilon, num_actions):\n",
    "    \"\"\"\n",
    "    function that returns the probability for action choosing based on a given Q-function and epsilon at this state\n",
    "    :param state: tuple of order 2 * number of jobs\n",
    "    :param Q: Q table which is a dictionary with states as first class keys and actions as second class keys\n",
    "    :param epsilon: for epsilon greedy choosing algorithm\n",
    "    :param num_actions: number of legal actions at this state\n",
    "    :return: an array of order num_actions containing probability for action choosing at this state\n",
    "    \"\"\"\n",
    "    # if state has been visited before, use epsilon greedy algorithm to generate probability\n",
    "    if state in Q:\n",
    "        best_action = np.argmax(Q[state])\n",
    "        Action_probabilities = np.ones(num_actions, dtype = float) * epsilon / num_actions\n",
    "        Action_probabilities[best_action] += (1.0 - epsilon)\n",
    "        return Action_probabilities\n",
    "\n",
    "    # if state hasn't been visited before, choose action with equal probability\n",
    "    Action_probabilities = np.ones(num_actions, dtype = float) / num_actions\n",
    "    return Action_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def update_estimation(Q_estimation, estimation_list, total_reward):\n",
    "    \"\"\"\n",
    "    update the Q_estimation table\n",
    "    1. If state action pair is not estimated, set its Q value\n",
    "    2. If pair has been estimated, only update in case of a more optimal reward\n",
    "    :param total_reward: total reward of single episode run\n",
    "    :param Q_estimation: estimation of Q values table\n",
    "    :param estimation_list: list of [state, action, reward]\n",
    "    :return: updated Q_estimation table\n",
    "    \"\"\"\n",
    "    current_reward = total_reward\n",
    "    for state, action, reward in estimation_list:\n",
    "        state_action_estimation = Q_estimation[state][action]\n",
    "        # 1. if state action pair hasn't been estimated\n",
    "        if state_action_estimation == 0:\n",
    "            Q_estimation[state][action] = current_reward\n",
    "        # 2. if current estimation is a more optimal solution\n",
    "        elif state_action_estimation <= current_reward:\n",
    "            Q_estimation[state][action] = current_reward\n",
    "        current_reward -= reward\n",
    "\n",
    "    return Q_estimation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def q_estimation(env, epis, epsilon = .9, print_steps = False):\n",
    "    \"\"\"\n",
    "    q-learning algorithm that returns the best policy, and data for each episode\n",
    "    Once an episode is done, from first state, action pair to the last one:\n",
    "    1. check if current q value is greater than reward estimation\n",
    "    2. if yes, update the q value <- reward estimation\n",
    "    3. if no, reward estimation -= current reward of state action pair\n",
    "    :param print_steps: boolean dictating whether to print each step\n",
    "    :param epsilon: randomness factor, how random the selection is\n",
    "    :param env: JSSP instance\n",
    "    :param epis: number of episodes\n",
    "    :return: Filled Q_table with estimations\n",
    "    \"\"\"\n",
    "    max_score = -100000\n",
    "    max_episode = -1\n",
    "    # 1. Load Environment and Q-table structure\n",
    "    Q_estimation = {}\n",
    "    total_reward_list = [] # rewards per episode calculate\n",
    "    testing_reward_list = [] # rewards per 100 episodes\n",
    "    max_action_list = []\n",
    "    max_time_list = []\n",
    "    decay_rate = epsilon / (epis * .9)\n",
    "\n",
    "    # Q-learning Algorithm\n",
    "    for episode in range(epis+1):\n",
    "        # Reset environment\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        estimation_list = []\n",
    "        action_list = []\n",
    "        time_list = []\n",
    "        # Testing:\n",
    "        if episode % 100 == 0:\n",
    "            while not done:\n",
    "                if state not in Q_estimation:\n",
    "                    Q_estimation[state] = np.zeros(env.action_space.n)\n",
    "                # best action from training\n",
    "                action = np.argmax(Q_estimation[state])\n",
    "                # update action history\n",
    "                action_list.append(env.legal_allocation_list[action])\n",
    "                time_list.append(env.time)\n",
    "                # get new state & reward from environment\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                # store state, action, reward for current episode\n",
    "                # used for q value estimation\n",
    "                estimation_list.append([state, action, reward])\n",
    "                total_reward += reward\n",
    "                state = next_state\n",
    "            # update Q_estimation table\n",
    "            Q_estimation = update_estimation(Q_estimation, estimation_list, total_reward)\n",
    "            # keep policy testing results every 100 episodes\n",
    "            testing_reward_list.append(total_reward)\n",
    "            if print_steps:\n",
    "                print(\"Episode: \" + str(episode) + \" has time \" + str(env.time) + \" has reward \" + str(total_reward))\n",
    "            # keep track of the optimal policy\n",
    "            if total_reward >= max_score:\n",
    "                max_score = total_reward\n",
    "                max_episode = episode\n",
    "                max_action_list = action_list\n",
    "                max_time_list = time_list\n",
    "            continue\n",
    "\n",
    "        # Epsilon decay\n",
    "        if epsilon > decay_rate:\n",
    "            epsilon -= decay_rate\n",
    "        # Training\n",
    "        while not done:\n",
    "            if state not in Q_estimation:\n",
    "                Q_estimation[state] = np.zeros(env.action_space.n)\n",
    "            # generate action choosing probability and choose an action\n",
    "            # if this is a testing episode, epsilon = 0\n",
    "            action_probabilities = policy(state, Q_estimation, epsilon, env.action_space.n)\n",
    "            action = np.random.choice(np.arange(len(action_probabilities)), p = action_probabilities)\n",
    "            # get new state & reward from environment\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            # store state, action, reward for current episode\n",
    "            # used for q value estimation\n",
    "            estimation_list.append([state, action, reward])\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "        Q_estimation = update_estimation(Q_estimation, estimation_list, total_reward)\n",
    "        total_reward_list.append(total_reward)\n",
    "\n",
    "    print('From {}th Episode best policy has makespan {}'.format(max_episode, -max_score))\n",
    "    return total_reward_list, testing_reward_list, max_score, max_action_list, max_time_list\n",
    "\n",
    "# training_reward_list, score, testing_reward_list, max_action, max_time = q_estimation(env3, 30000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_learning(plot_list, title):\n",
    "    plt.plot(plot_list)\n",
    "    plt.title(\"QLearning: \" + title)\n",
    "    plt.xlabel(\"Episode #\")\n",
    "    plt.ylabel(\"Total_reward\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def print_allocations(action_list, time_list):\n",
    "    for i in range(len(action_list)):\n",
    "        print('The allocation chose at time {} is {}'.format(time_list[i], action_list[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env_names = [\"Small\", \"Medium\", \"Large\", \"Extra Large\"]\n",
    "env_list = [env_s, env_m, env_l, env_xl]\n",
    "env_episodes = [500, 5000, 10000, 80000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Random Sampling solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "solutions_rs = [0]*4\n",
    "for i in range(4):\n",
    "    print('Solution found by random selection for {} environment'.format(env_names[i]))\n",
    "    solutions_rs[i] = random_sampling(env_list[i], env_episodes[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Q-Learning solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "solutions_ql = [0]*4\n",
    "solutions_action_list_ql = [[]]*4\n",
    "solutions_time_list_ql = [[]]*4\n",
    "training_lists = [[]]*4\n",
    "testing_lists = [[]]*4"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Small Environment"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "print('Solution found by Q_learning for {} environment'.format(env_names[i]))\n",
    "training_lists[i], testing_lists[i], solutions_ql[i], solutions_action_list_ql[i], solutions_time_list_ql[i] = q_estimation(env_list[i], env_episodes[i])\n",
    "print('Training plot for {} environment'.format(env_names[i]))\n",
    "plot_learning(training_lists[i], \"Training\")\n",
    "print('Testing plot for {} environment'.format(env_names[i]))\n",
    "plot_learning(testing_lists[i], \"Testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Medium Environment"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "i = 1\n",
    "print('Solution found by Q_learning for {} environment'.format(env_names[i]))\n",
    "training_lists[i], testing_lists[i], solutions_ql[i], solutions_action_list_ql[i], solutions_time_list_ql[i] = q_estimation(env_list[i], env_episodes[i])\n",
    "print('Training plot for {} environment'.format(env_names[i]))\n",
    "plot_learning(training_lists[i], \"Training\")\n",
    "print('Testing plot for {} environment'.format(env_names[i]))\n",
    "plot_learning(testing_lists[i], \"Testing\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Large Environment"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "i = 2\n",
    "print('Solution found by Q_learning for {} environment'.format(env_names[i]))\n",
    "training_lists[i], testing_lists[i], solutions_ql[i], solutions_action_list_ql[i], solutions_time_list_ql[i] = q_estimation(env_list[i], env_episodes[i])\n",
    "print('Training plot for {} environment'.format(env_names[i]))\n",
    "plot_learning(training_lists[i], \"Training\")\n",
    "print('Testing plot for {} environment'.format(env_names[i]))\n",
    "plot_learning(testing_lists[i], \"Testing\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Extra Large Environment"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "i = 3\n",
    "print('Solution found by Q_learning for {} environment'.format(env_names[i]))\n",
    "training_lists[i], testing_lists[i], solutions_ql[i], solutions_action_list_ql[i], solutions_time_list_ql[i] = q_estimation(env_list[i], env_episodes[i])\n",
    "print('Training plot for {} environment'.format(env_names[i]))\n",
    "plot_learning(training_lists[i], \"Training\")\n",
    "print('Testing plot for {} environment'.format(env_names[i]))\n",
    "plot_learning(testing_lists[i], \"Testing\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Mk Instances"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Mk_env_list = []\n",
    "Mk_env_names = []\n",
    "Mk_env_episodes = [100000,100000]\n",
    "Mk_training_lists = [[]] * 2\n",
    "Mk_testing_lists = [[]] * 2\n",
    "Mk_solutions_ql = [[]] * 2\n",
    "for i in range(4,6):\n",
    "    env = create_env('../Instances/Mk0{}.fjs'.format(i))\n",
    "    Mk_env_list.append(env)\n",
    "    Mk_env_names.append('Mk0{}'.format(i))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "solutions_rs = [0] * 2\n",
    "a = []\n",
    "b = []\n",
    "for i in range(2):\n",
    "    print('Solution found by Q_learning for {} environment'.format(Mk_env_names[i]))\n",
    "    Mk_training_lists[i], Mk_testing_lists[i], Mk_solutions_ql[i], a, b = q_estimation(Mk_env_list[i], Mk_env_episodes[i])\n",
    "    print('Training plot for {} environment'.format(Mk_env_names[i]))\n",
    "    plot_learning(Mk_training_lists[i], \"Training\")\n",
    "    print('Testing plot for {} environment'.format(Mk_env_names[i]))\n",
    "    plot_learning(Mk_testing_lists[i], \"Testing\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### la Instances"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "la_env_list = []\n",
    "la_env_names = []\n",
    "la_env_episodes = [10000,10000,10000,10000,10000,10000]\n",
    "la_training_lists = [[]] * 5\n",
    "la_testing_lists = [[]] * 5\n",
    "la_solutions_ql = [[]] * 5\n",
    "for i in range(1,6):\n",
    "    env = create_env('../Instances/la0{}.fjs'.format(i))\n",
    "    la_env_list.append(env)\n",
    "    la_env_names.append('la0{}'.format(i))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "solutions_rs = [0] * 5\n",
    "a = []\n",
    "b = []\n",
    "for i in range(5):\n",
    "    print('Solution found by Q_learning for {} environment'.format(la_env_names[i]))\n",
    "    la_training_lists[i], la_testing_lists[i], la_solutions_ql[i], a, b = q_estimation(la_env_list[i], la_env_episodes[i])\n",
    "    print('Training plot for {} environment'.format(la_env_names[i]))\n",
    "    plot_learning(la_training_lists[i], \"Training\")\n",
    "    print('Testing plot for {} environment'.format(la_env_names[i]))\n",
    "    plot_learning(la_testing_lists[i], \"Testing\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Makespan Comparison Table\n",
    "\n",
    "| Env/Algo  | Random Selection | Q_Learning | GA  | OR-Tools |\n",
    "|-----------|-----------------|------------|-----|----------|\n",
    "| Small     | 53              | 53         | 53  | 53       |\n",
    "| Medium    | 58              | 55         | 55  | 55       |\n",
    "| Large     | 47              | 42         | 42  | 40       |\n",
    "| Extra Large | 990             | 933        | 986 | 927      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Solutions for environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Small Environment - 2 jobs 3 machines 3 operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print_allocations(solutions_action_list_ql[0], solutions_time_list_ql[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Medium Environment - 6 jobs 6 machines 6 operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print_allocations(solutions_action_list_ql[1], solutions_time_list_ql[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Large Environment - 10 jobs 6 machines 6 operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print_allocations(solutions_action_list_ql[2], solutions_time_list_ql[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Extra Large Environment - 10 jobs 11 machines 10 operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print_allocations(solutions_action_list_ql[3], solutions_time_list_ql[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}