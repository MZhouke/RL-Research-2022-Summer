{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import JSSP\n",
    "import numpy as np\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Environment Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_env(instance_path):\n",
    "    env_name = \"JSSP-v0\"\n",
    "    env = gym.make(env_name, instance_path = instance_path)\n",
    "    print(\"Environment Created for: \", instance_path)\n",
    "    print(\"Observation space: \\n\", env.observation_space)\n",
    "    print(\"Action space: \\n\", env.action_space)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# small environment: 2 jobs 3 machines 3 operations\n",
    "env_s = create_env(\"../Instances/instance1.txt\") # -53\n",
    "# medium environment: 6 jobs 6 machines 6 operations\n",
    "env_m = create_env(\"../Instances/instance5.txt\") # -55\n",
    "# large environment: 10 jobs 6 machines 6 operations\n",
    "env_l = create_env(\"../Instances/instance4.txt\") # -42\n",
    "# extra large environment: 10 jobs 11 machines 10 operations\n",
    "env_xl = create_env(\"../Instances/instance3.txt\") # -959"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Random Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def random_sampling(env, episodes, print_steps = False):\n",
    "    \"\"\"\n",
    "    baseline solution to FJSSP, randomly selects actions for each episode and selects\n",
    "    best set of actions\n",
    "    :param print_steps: boolean dictating whether to print each step\n",
    "    :param env: FJSSP environment\n",
    "    :param episodes: number of episodes\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    env.reset()\n",
    "    max_score = -100000\n",
    "    max_episode = -1\n",
    "    max_action_list = []\n",
    "    max_time_list = []\n",
    "    for episode in range(1, episodes+1):\n",
    "        env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "        action_list = []\n",
    "        time_list = []\n",
    "        while not done:\n",
    "            #env.render()\n",
    "            action = env.action_space.sample()\n",
    "            if action != env.action_space.n -1:\n",
    "                action_list.append(env.legal_allocation_list[action])\n",
    "                time_list.append(env.time)\n",
    "            n_state, reward, done, info = env.step(action)\n",
    "            score+=reward\n",
    "        if print_steps:\n",
    "            print('Episode:{} Total_reward:{}'.format(episode, score))\n",
    "        if score >= max_score:\n",
    "            max_score = score\n",
    "            max_episode = episode\n",
    "            max_action_list = action_list\n",
    "            max_time_list = time_list\n",
    "    print('From {}th Episode best policy has reward {}'.format(max_episode, max_score))\n",
    "    return max_score, max_action_list, max_time_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def policy(state, Q, epsilon, num_actions):\n",
    "    \"\"\"\n",
    "    function that returns the probability for action choosing based on a given Q-function and epsilon at this state\n",
    "    :param state: tuple of order 2 * number of jobs\n",
    "    :param Q: Q table which is a dictionary with states as first class keys and actions as second class keys\n",
    "    :param epsilon: for epsilon greedy choosing algorithm\n",
    "    :param num_actions: number of legal actions at this state\n",
    "    :return: an array of order num_actions containing probability for action choosing at this state\n",
    "    \"\"\"\n",
    "    # if state has been visited before, use epsilon greedy algorithm to generate probability\n",
    "    if state in Q:\n",
    "        best_action = np.argmax(Q[state])\n",
    "        Action_probabilities = np.ones(num_actions, dtype = float) * epsilon / num_actions\n",
    "        Action_probabilities[best_action] += (1.0 - epsilon)\n",
    "        return Action_probabilities\n",
    "\n",
    "    # if state hasn't been visited before, choose action with equal probability\n",
    "    Action_probabilities = np.ones(num_actions, dtype = float) / num_actions\n",
    "    return Action_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def update_estimation(Q_estimation, estimation_list, total_reward):\n",
    "    \"\"\"\n",
    "    update the Q_estimation table\n",
    "    1. If state action pair is not estimated, set its Q value\n",
    "    2. If pair has been estimated, only update in case of a more optimal reward\n",
    "    :param total_reward: total reward of single episode run\n",
    "    :param Q_estimation: estimation of Q values table\n",
    "    :param estimation_list: list of [state, action, reward]\n",
    "    :return: updated Q_estimation table\n",
    "    \"\"\"\n",
    "    current_reward = total_reward\n",
    "    for state, action, reward in estimation_list:\n",
    "        state_action_estimation = Q_estimation[state][action]\n",
    "        # 1. if state action pair hasn't been estimated\n",
    "        if state_action_estimation == 0:\n",
    "            Q_estimation[state][action] = current_reward\n",
    "        # 2. if current estimation is a more optimal solution\n",
    "        elif state_action_estimation <= current_reward:\n",
    "            Q_estimation[state][action] = current_reward\n",
    "        current_reward -= reward\n",
    "\n",
    "    return Q_estimation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def q_estimation(env, epis, epsilon = .9, print_steps = False):\n",
    "    \"\"\"\n",
    "    q-learning algorithm that returns the best policy, and data for each episode\n",
    "    Once an episode is done, from first state, action pair to the last one:\n",
    "    1. check if current q value is greater than reward estimation\n",
    "    2. if yes, update the q value <- reward estimation\n",
    "    3. if no, reward estimation -= current reward of state action pair\n",
    "    :param print_steps: boolean dictating whether to print each step\n",
    "    :param epsilon: randomness factor, how random the selection is\n",
    "    :param env: JSSP instance\n",
    "    :param epis: number of episodes\n",
    "    :return: Filled Q_table with estimations\n",
    "    \"\"\"\n",
    "    max_score = -100000\n",
    "    max_episode = -1\n",
    "    # 1. Load Environment and Q-table structure\n",
    "    Q_estimation = {}\n",
    "    total_reward_list = [] # rewards per episode calculate\n",
    "    testing_reward_list = [] # rewards per 100 episodes\n",
    "    max_action_list = []\n",
    "    max_time_list = []\n",
    "    decay_rate = epsilon / (epis * .9)\n",
    "\n",
    "    # Q-learning Algorithm\n",
    "    for episode in range(epis+1):\n",
    "        # Reset environment\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        estimation_list = []\n",
    "        action_list = []\n",
    "        time_list = []\n",
    "        # Testing:\n",
    "        if episode % 100 == 0:\n",
    "            while not done:\n",
    "                if state not in Q_estimation:\n",
    "                    Q_estimation[state] = np.zeros(env.action_space.n)\n",
    "                # best action from training\n",
    "                action = np.argmax(Q_estimation[state])\n",
    "                # update action history\n",
    "                action_list.append(env.legal_allocation_list[action])\n",
    "                time_list.append(env.time)\n",
    "                # get new state & reward from environment\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                # store state, action, reward for current episode\n",
    "                # used for q value estimation\n",
    "                estimation_list.append([state, action, reward])\n",
    "                total_reward += reward\n",
    "                state = next_state\n",
    "            # update Q_estimation table\n",
    "            Q_estimation = update_estimation(Q_estimation, estimation_list, total_reward)\n",
    "            # keep policy testing results every 100 episodes\n",
    "            testing_reward_list.append(total_reward)\n",
    "            if print_steps:\n",
    "                print(\"Episode: \" + str(episode) + \" has time \" + str(env.time) + \" has reward \" + str(total_reward))\n",
    "            # keep track of the optimal policy\n",
    "            if total_reward >= max_score:\n",
    "                max_score = total_reward\n",
    "                max_episode = episode\n",
    "                max_action_list = action_list\n",
    "                max_time_list = time_list\n",
    "            continue\n",
    "\n",
    "        # Epsilon decay\n",
    "        if epsilon > decay_rate:\n",
    "            epsilon -= decay_rate\n",
    "        # Training\n",
    "        while not done:\n",
    "            if state not in Q_estimation:\n",
    "                Q_estimation[state] = np.zeros(env.action_space.n)\n",
    "            # generate action choosing probability and choose an action\n",
    "            # if this is a testing episode, epsilon = 0\n",
    "            action_probabilities = policy(state, Q_estimation, epsilon, env.action_space.n)\n",
    "            action = np.random.choice(np.arange(len(action_probabilities)), p = action_probabilities)\n",
    "            # get new state & reward from environment\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            # store state, action, reward for current episode\n",
    "            # used for q value estimation\n",
    "            estimation_list.append([state, action, reward])\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "        Q_estimation = update_estimation(Q_estimation, estimation_list, total_reward)\n",
    "        total_reward_list.append(total_reward)\n",
    "\n",
    "    print('From {}th Episode best policy has reward {}'.format(max_episode, max_score))\n",
    "    return total_reward_list, testing_reward_list, max_score, max_action_list, max_time_list\n",
    "\n",
    "# training_reward_list, score, testing_reward_list, max_action, max_time = q_estimation(env3, 30000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_learning(plot_list, title):\n",
    "    plt.plot(plot_list)\n",
    "    plt.title(\"QLearning: \" + title)\n",
    "    plt.xlabel(\"Episode #\")\n",
    "    plt.ylabel(\"Total_reward\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def print_allocations(action_list, time_list):\n",
    "    for i in range(len(action_list)):\n",
    "        print('The allocation chose at time {} is {}'.format(time_list[i], action_list[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env_names = [\"Small\", \"Medium\", \"Large\", \"Extra Large\"]\n",
    "env_list = [env_s, env_m, env_l, env_xl]\n",
    "env_episodes = [500, 5000, 10000, 50000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Random Sampling solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [23]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m4\u001B[39m):\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSolution found by random selection for \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m environment\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(env_names[i]))\n\u001B[0;32m----> 4\u001B[0m     solutions_rs[i] \u001B[38;5;241m=\u001B[39m \u001B[43mrandom_sampling\u001B[49m\u001B[43m(\u001B[49m\u001B[43menv_list\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43menv_episodes\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[0;32mIn [16]\u001B[0m, in \u001B[0;36mrandom_sampling\u001B[0;34m(env, episodes, print_steps)\u001B[0m\n\u001B[1;32m     25\u001B[0m         action_list\u001B[38;5;241m.\u001B[39mappend(env\u001B[38;5;241m.\u001B[39mlegal_allocation_list[action])\n\u001B[1;32m     26\u001B[0m         time_list\u001B[38;5;241m.\u001B[39mappend(env\u001B[38;5;241m.\u001B[39mtime)\n\u001B[0;32m---> 27\u001B[0m     n_state, reward, done, info \u001B[38;5;241m=\u001B[39m \u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     28\u001B[0m     score\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39mreward\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m print_steps:\n",
      "File \u001B[0;32m/opt/anaconda3/envs/RL-Research-2022/lib/python3.10/site-packages/gym/wrappers/order_enforcing.py:11\u001B[0m, in \u001B[0;36mOrderEnforcing.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep\u001B[39m(\u001B[38;5;28mself\u001B[39m, action):\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_has_reset, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot call env.step() before calling reset()\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m---> 11\u001B[0m     observation, reward, done, info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     12\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m observation, reward, done, info\n",
      "File \u001B[0;32m~/Desktop/Personal Storage/OneDrive/CIS Research/RL-Research-2022-Summer/RL-Research-2022-Summer/JSSP/envs/JSSP_env.py:365\u001B[0m, in \u001B[0;36mJSSPEnv.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m    363\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_obs(), reward, done, {}\n\u001B[1;32m    364\u001B[0m \u001B[38;5;66;03m# 3. update legal allocation list\u001B[39;00m\n\u001B[0;32m--> 365\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate_legal_allocation_list\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    366\u001B[0m \u001B[38;5;66;03m# 4. if next state has legal action other than wait\u001B[39;00m\n\u001B[1;32m    367\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlegal_allocation_list) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m    368\u001B[0m     \u001B[38;5;66;03m# 6. re-initialize action space\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/Personal Storage/OneDrive/CIS Research/RL-Research-2022-Summer/RL-Research-2022-Summer/JSSP/envs/JSSP_env.py:262\u001B[0m, in \u001B[0;36mJSSPEnv.generate_legal_allocation_list\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    259\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m allocation \u001B[38;5;129;01min\u001B[39;00m allocation_list:\n\u001B[1;32m    260\u001B[0m     \u001B[38;5;66;03m# 4. check for duplicate job allocation\u001B[39;00m\n\u001B[1;32m    261\u001B[0m     duplicate_check_list \u001B[38;5;241m=\u001B[39m [machine \u001B[38;5;28;01mfor\u001B[39;00m machine \u001B[38;5;129;01min\u001B[39;00m allocation \u001B[38;5;28;01mif\u001B[39;00m machine \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m--> 262\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munique\u001B[49m\u001B[43m(\u001B[49m\u001B[43mduplicate_check_list\u001B[49m\u001B[43m)\u001B[49m) \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mlen\u001B[39m(duplicate_check_list):\n\u001B[1;32m    263\u001B[0m         legal_allocations_list\u001B[38;5;241m.\u001B[39mappend(np\u001B[38;5;241m.\u001B[39marray(allocation))\n\u001B[1;32m    264\u001B[0m \u001B[38;5;66;03m# 5. check if no job is working\u001B[39;00m\n",
      "File \u001B[0;32m<__array_function__ internals>:180\u001B[0m, in \u001B[0;36munique\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/RL-Research-2022/lib/python3.10/site-packages/numpy/lib/arraysetops.py:272\u001B[0m, in \u001B[0;36munique\u001B[0;34m(ar, return_index, return_inverse, return_counts, axis)\u001B[0m\n\u001B[1;32m    270\u001B[0m ar \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39masanyarray(ar)\n\u001B[1;32m    271\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m axis \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 272\u001B[0m     ret \u001B[38;5;241m=\u001B[39m \u001B[43m_unique1d\u001B[49m\u001B[43m(\u001B[49m\u001B[43mar\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_inverse\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_counts\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    273\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _unpack_tuple(ret)\n\u001B[1;32m    275\u001B[0m \u001B[38;5;66;03m# axis was specified and not None\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/RL-Research-2022/lib/python3.10/site-packages/numpy/lib/arraysetops.py:337\u001B[0m, in \u001B[0;36m_unique1d\u001B[0;34m(ar, return_index, return_inverse, return_counts)\u001B[0m\n\u001B[1;32m    335\u001B[0m mask \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mempty(aux\u001B[38;5;241m.\u001B[39mshape, dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mbool_)\n\u001B[1;32m    336\u001B[0m mask[:\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m--> 337\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m aux\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m aux\u001B[38;5;241m.\u001B[39mdtype\u001B[38;5;241m.\u001B[39mkind \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcfmM\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m np\u001B[38;5;241m.\u001B[39misnan(aux[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]):\n\u001B[1;32m    338\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m aux\u001B[38;5;241m.\u001B[39mdtype\u001B[38;5;241m.\u001B[39mkind \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mc\u001B[39m\u001B[38;5;124m\"\u001B[39m:  \u001B[38;5;66;03m# for complex all NaNs are considered equivalent\u001B[39;00m\n\u001B[1;32m    339\u001B[0m         aux_firstnan \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39msearchsorted(np\u001B[38;5;241m.\u001B[39misnan(aux), \u001B[38;5;28;01mTrue\u001B[39;00m, side\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mleft\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "solutions_rs = [0]*4\n",
    "for i in range(4):\n",
    "    print('Solution found by random selection for {} environment'.format(env_names[i]))\n",
    "    solutions_rs[i] = random_sampling(env_list[i], env_episodes[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Q-Learning solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "solutions_ql = [0]*4\n",
    "solutions_action_list_ql = [[]]*4\n",
    "solutions_time_list_ql = [[]]*4\n",
    "training_lists = [[]]*4\n",
    "testing_lists = [[]]*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    print('Solution found by Q_learning for {} environment'.format(env_names[i]))\n",
    "    training_lists[i], testing_lists[i], solutions_ql[i], solutions_action_list_ql[i], solutions_time_list_ql[i] = q_estimation(env_list[i], env_episodes[i])\n",
    "    print('Training plot for {} environment'.format(env_names[i]))\n",
    "    plot_learning(training_lists[i], \"Training\")\n",
    "    print('Testing plot for {} environment'.format(env_names[i]))\n",
    "    plot_learning(testing_lists[i], \"Testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Makespan Comparison Table\n",
    "\n",
    "| Env/Algo  | Random Selection | Q_Learning |\n",
    "|-----------|-----------------|------------|\n",
    "| Small     | 53              | 53         |\n",
    "| Medium    | 58              | 55         |\n",
    "| Large     | 47              | 42         |\n",
    "| Extra Large | 990             | 953        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Solutions for environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Small Environment - 2 jobs 3 machines 3 operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print_allocations(solutions_action_list_ql[0], solutions_time_list_ql[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Medium Environment - 6 jobs 6 machines 6 operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print_allocations(solutions_action_list_ql[1], solutions_time_list_ql[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Large Environment - 10 jobs 6 machines 6 operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print_allocations(solutions_action_list_ql[2], solutions_time_list_ql[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Extra Large Environment - 10 jobs 11 machines 10 operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print_allocations(solutions_action_list_ql[3], solutions_time_list_ql[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env_la_01 = create_env(\"instance_la01.txt\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "training_reward_list, score, testing_reward_list, max_action, max_time = q_estimation(env_la_01, 50000, print_steps=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}