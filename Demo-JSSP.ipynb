{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import JSSP\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.style\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "#import plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Environment Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_env(instance_path):\n",
    "    env_name = \"JSSP-v0\"\n",
    "    env = gym.make(env_name, instance_path = instance_path)\n",
    "    print(\"Environment Created for: \", instance_path)\n",
    "    print(\"Observation space: \\n\", env.observation_space)\n",
    "    print(\"Action space: \\n\", env.action_space)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Created for:  instance1.txt\n",
      "Observation space: \n",
      " Box([-2 -2  0  0], [2 2 2 2], (4,), int64)\n",
      "Action space: \n",
      " Discrete(8)\n",
      "Environment Created for:  instance3.txt\n",
      "Observation space: \n",
      " Box([-2 -2 -2 -2 -2 -2 -2 -2 -2 -2  0  0  0  0  0  0  0  0  0  0], [10 10 10 10 10 10 10 10 10 10  9  9  9  9  9  9  9  9  9  9], (20,), int64)\n",
      "Action space: \n",
      " Discrete(260)\n",
      "Environment Created for:  instance4.txt\n",
      "Observation space: \n",
      " Box([-2 -2 -2 -2 -2 -2 -2 -2 -2 -2  0  0  0  0  0  0  0  0  0  0], [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5], (20,), int64)\n",
      "Action space: \n",
      " Discrete(756)\n"
     ]
    }
   ],
   "source": [
    "env1 = create_env(\"instance1.txt\")\n",
    "env3 = create_env(\"instance3.txt\")\n",
    "env4 = create_env(\"instance4.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. Random Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def random_sampling(env, episodes):\n",
    "    env.reset()\n",
    "    max_score = -100000\n",
    "    max_episode = -1\n",
    "    max_action_list = []\n",
    "    max_time_list = []\n",
    "    for episode in range(1, episodes+1):\n",
    "        env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "        action_list = []\n",
    "        time_list = []\n",
    "        while not done:\n",
    "            #env.render()\n",
    "            action = env.action_space.sample()\n",
    "            if action != env.action_space.n -1:\n",
    "                action_list.append(env.legal_allocation_list[action])\n",
    "                time_list.append(env.time)\n",
    "                # print('Episode:{} Allocation:{} Time:{}'.format(episode, env.legal_allocation_list[action], env.time))\n",
    "            n_state, reward, done, info = env.step(action)\n",
    "            score+=reward\n",
    "        print('Episode:{} Total_reward:{}'.format(episode, score))\n",
    "        if score >= max_score:\n",
    "            max_score = score\n",
    "            max_episode = episode\n",
    "            max_action_list = action_list\n",
    "            max_time_list = time_list\n",
    "    print('From {}th Episode best policy has reward {}'.format(max_episode, max_score))\n",
    "    for i in range(len(max_action_list)):\n",
    "        print('The allocation chose at time {} is {}'.format(max_time_list[i], max_action_list[i]))\n",
    "\n",
    "random_sampling(env4, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# def createEpsilonGreedyPolicy(Q, epsilon, num_actions):\n",
    "# \t\"\"\"\n",
    "# \tCreates an epsilon-greedy policy based\n",
    "# \ton a given Q-function and epsilon.\n",
    "#\n",
    "# \tReturns a function that takes the state\n",
    "# \tas an input and returns the probabilities\n",
    "# \tfor each action in the form of a numpy array\n",
    "# \tof length of the action space(set of possible actions).\n",
    "# \t\"\"\"\n",
    "def policy(state, Q, epsilon, num_actions):\n",
    "\n",
    "    if state in Q:\n",
    "        best_action = np.argmax(Q[state])\n",
    "        Action_probabilities = np.ones(num_actions, dtype = float) * epsilon / num_actions\n",
    "        Action_probabilities[best_action] += (1.0 - epsilon)\n",
    "        return Action_probabilities\n",
    "\n",
    "    Action_probabilities = np.ones(num_actions, dtype = float) / num_actions\n",
    "    return Action_probabilities"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def update(Q, state, next_state, action, reward, eta, gamma):\n",
    "\n",
    "    if next_state not in Q:\n",
    "        Q_next_state_max = -1\n",
    "    else:\n",
    "        Q_next_state_max = max(Q[next_state])\n",
    "\n",
    "    Q[state][action] = Q[state][action] + eta * (reward + gamma * Q_next_state_max - Q[state][action])\n",
    "\n",
    "    return Q"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def q_learning(env, epis):\n",
    "\n",
    "    max_score = -100000\n",
    "    max_episode = -1\n",
    "    max_action_list = []\n",
    "    max_time_list = []\n",
    "\n",
    "    # 1. Load Environment and Q-table structure\n",
    "    Q_table = {}\n",
    "    # 2. Parameters of Q-learning\n",
    "    eta = .628\n",
    "    gamma = 1\n",
    "    epsilon = .1\n",
    "    decay_rate = .0001\n",
    "    total_reward_list = [] # rewards per episode calculate\n",
    "    # 3. Q-learning Algorithm\n",
    "    for episode in range(epis):\n",
    "        # Reset environment\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        action_list = []\n",
    "        time_list = []\n",
    "\n",
    "        # The Q-Table learning algorithm\n",
    "        while not done:\n",
    "            if epsilon >= decay_rate:\n",
    "                epsilon -= decay_rate\n",
    "            if state not in Q_table:\n",
    "                Q_table[state] = np.zeros(env.action_space.n)\n",
    "            action_probabilities = policy(state, Q_table, epsilon, env.action_space.n)\n",
    "            action = np.random.choice(np.arange(len(action_probabilities)), p = action_probabilities)\n",
    "            if action != env.action_space.n -1:\n",
    "                action_list.append(env.legal_allocation_list[action])\n",
    "                time_list.append(env.time)\n",
    "                #print('Episode:{} Allocation:{} Time:{}'.format(episode, env.legal_allocation_list[action], env.time))\n",
    "            #Get new state & reward from environment\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            #Update Q-Table with new knowledge\n",
    "            Q_table = update(Q_table, state, next_state, action, reward, eta, gamma)\n",
    "            if eta > .01:\n",
    "                eta -= decay_rate\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "        total_reward_list.append(total_reward)\n",
    "        if total_reward >= max_score:\n",
    "            max_score = total_reward\n",
    "            max_episode = episode\n",
    "            max_action_list = action_list\n",
    "            max_time_list = time_list\n",
    "        print(\"Episode: \" + str(episode) + \" has time \" + str(env.time))\n",
    "\n",
    "    print('From {}th Episode best policy has reward {}'.format(max_episode + 1, max_score))\n",
    "    for i in range(len(max_action_list)):\n",
    "        print('The allocation chose at time {} is {}'.format(max_time_list[i], max_action_list[i]))\n",
    "\n",
    "    return total_reward_list, Q_table\n",
    "\n",
    "print()\n",
    "total_reward_list, Q_table = q_learning(env1, 500)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "<function matplotlib.pyplot.show(close=None, block=None)>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(total_reward_list)\n",
    "plt.show\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(total_reward_list)\n",
    "plt.show"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(Q_table)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DQN"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "\n",
    "model = DQN(\"MlpPolicy\", env1, verbose=1)\n",
    "model.learn(total_timesteps=10000, log_interval=4)\n",
    "model.save(\"dqn_cartpole\")\n",
    "\n",
    "del model # remove to demonstrate saving and loading\n",
    "\n",
    "model = DQN.load(\"dqn_cartpole\")\n",
    "\n",
    "obs = env1.reset()\n",
    "for episode in range(500):\n",
    "    env1.reset()\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, info = env1.step(action)\n",
    "        total_reward += reward\n",
    "    print(\"Episode: \" + str(episode) + \" has reward \" + str(total_reward))\n",
    "        # env.render()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Q Learning from web"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def createEpsilonGreedyPolicy(Q, epsilon, num_actions, env):\n",
    "\t\"\"\"\n",
    "\tCreates an epsilon-greedy policy based\n",
    "\ton a given Q-function and epsilon.\n",
    "\n",
    "\tReturns a function that takes the state\n",
    "\tas an input and returns the probabilities\n",
    "\tfor each action in the form of a numpy array\n",
    "\tof length of the action space(set of possible actions).\n",
    "\t\"\"\"\n",
    "\tdef policyFunction(state):\n",
    "\n",
    "\t\tAction_probabilities = np.ones(num_actions,\n",
    "\t\t\t\tdtype = float) * epsilon / num_actions\n",
    "\n",
    "\t\tbest_action = np.argmax(Q[state])\n",
    "\t\tAction_probabilities[best_action] += (1.0 - epsilon)\n",
    "\t\treturn Action_probabilities\n",
    "\n",
    "\treturn policyFunction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def qLearning(env, num_episodes, discount_factor = 1.0,\n",
    "\t\t\t\t\t\t\talpha = 0.6, epsilon = 0.1):\n",
    "\t\"\"\"\n",
    "\tQ-Learning algorithm: Off-policy TD control.\n",
    "\tFinds the optimal greedy policy while improving\n",
    "\tfollowing an epsilon-greedy policy\"\"\"\n",
    "\n",
    "\t# Action value function\n",
    "\t# A nested dictionary that maps\n",
    "\t# state -> (action -> action-value).\n",
    "\tQ = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "\t# Keeps track of useful statistics\n",
    "\tstats = {\n",
    "            \"episode_lengths\" : np.zeros(num_episodes),\n",
    "\t\t    \"episode_rewards\" : np.zeros(num_episodes)}\n",
    "\n",
    "\t# Create an epsilon greedy policy function\n",
    "\t# appropriately for environment action space\n",
    "\tpolicy = createEpsilonGreedyPolicy(Q, epsilon, env.action_space.n, env)\n",
    "\n",
    "\t# For every episode\n",
    "\tfor ith_episode in range(num_episodes):\n",
    "\n",
    "\t\t# Reset the environment and pick the first action\n",
    "\t\tstate = env.reset()\n",
    "\n",
    "\t\tfor t in itertools.count():\n",
    "\n",
    "\t\t\t# get probabilities of all actions from current state\n",
    "\t\t\taction_probabilities = policy(state)\n",
    "\n",
    "\t\t\t# choose action according to\n",
    "\t\t\t# the probability distribution\n",
    "\t\t\taction = np.random.choice(np.arange(\n",
    "\t\t\t\t\tlen(action_probabilities)),\n",
    "\t\t\t\t\tp = action_probabilities)\n",
    "\n",
    "\t\t\t# take action and get reward, transit to next state\n",
    "\t\t\tnext_state, reward, done, _ = env.step(action)\n",
    "\n",
    "\t\t\t# Update statistics\n",
    "\t\t\tstats[\"episode_rewards\"][ith_episode] += reward\n",
    "\t\t\tstats[\"episode_lengths\"][ith_episode] = t\n",
    "\n",
    "\t\t\t# TD Update\n",
    "\t\t\tbest_next_action = np.argmax(Q[next_state])\n",
    "\t\t\ttd_target = reward + discount_factor * Q[next_state][best_next_action]\n",
    "\t\t\ttd_delta = td_target - Q[state][action]\n",
    "\t\t\tQ[state][action] += alpha * td_delta\n",
    "\n",
    "\t\t\t# done is True if episode terminated\n",
    "\t\t\tif done:\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\t\tstate = next_state\n",
    "\n",
    "\treturn Q, stats"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}