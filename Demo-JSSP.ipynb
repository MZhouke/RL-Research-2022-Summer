{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import JSSP\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.style\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "#import plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Environment Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_env(instance_path):\n",
    "    env_name = \"JSSP-v0\"\n",
    "    env = gym.make(env_name, instance_path = instance_path)\n",
    "    print(\"Environment Created for: \", instance_path)\n",
    "    print(\"Observation space: \\n\", env.observation_space)\n",
    "    print(\"Action space: \\n\", env.action_space)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Created for:  instance1.txt\n",
      "Observation space: \n",
      " Box([-2 -2  0  0], [2 2 2 2], (4,), int64)\n",
      "Action space: \n",
      " Discrete(8)\n",
      "Environment Created for:  instance3.txt\n",
      "Observation space: \n",
      " Box([-2 -2 -2 -2 -2 -2 -2 -2 -2 -2  0  0  0  0  0  0  0  0  0  0], [10 10 10 10 10 10 10 10 10 10  9  9  9  9  9  9  9  9  9  9], (20,), int64)\n",
      "Action space: \n",
      " Discrete(260)\n",
      "Environment Created for:  instance4.txt\n",
      "Observation space: \n",
      " Box([-2 -2 -2 -2 -2 -2 -2 -2 -2 -2  0  0  0  0  0  0  0  0  0  0], [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5], (20,), int64)\n",
      "Action space: \n",
      " Discrete(756)\n"
     ]
    }
   ],
   "source": [
    "env1 = create_env(\"instance1.txt\")\n",
    "env3 = create_env(\"instance3.txt\")\n",
    "env4 = create_env(\"instance4.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. Random Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Total_reward:45\n",
      "Episode:2 Total_reward:35\n",
      "Episode:3 Total_reward:71\n",
      "Episode:4 Total_reward:57\n",
      "Episode:5 Total_reward:79\n",
      "Episode:6 Total_reward:81\n",
      "Episode:7 Total_reward:67\n",
      "Episode:8 Total_reward:49\n",
      "Episode:9 Total_reward:83\n",
      "Episode:10 Total_reward:73\n",
      "Episode:11 Total_reward:69\n",
      "Episode:12 Total_reward:89\n",
      "Episode:13 Total_reward:85\n",
      "Episode:14 Total_reward:65\n",
      "Episode:15 Total_reward:89\n",
      "Episode:16 Total_reward:71\n",
      "Episode:17 Total_reward:73\n",
      "Episode:18 Total_reward:55\n",
      "Episode:19 Total_reward:31\n",
      "Episode:20 Total_reward:55\n",
      "Episode:21 Total_reward:59\n",
      "Episode:22 Total_reward:83\n",
      "Episode:23 Total_reward:51\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [4]\u001B[0m, in \u001B[0;36m<cell line: 32>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     29\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(max_action_list)):\n\u001B[1;32m     30\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mThe allocation chose at time \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m is \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(max_time_list[i], max_action_list[i]))\n\u001B[0;32m---> 32\u001B[0m \u001B[43mrandom_sampling\u001B[49m\u001B[43m(\u001B[49m\u001B[43menv4\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[0;32mIn [4]\u001B[0m, in \u001B[0;36mrandom_sampling\u001B[0;34m(env, episodes)\u001B[0m\n\u001B[1;32m     18\u001B[0m         time_list\u001B[38;5;241m.\u001B[39mappend(env\u001B[38;5;241m.\u001B[39mtime)\n\u001B[1;32m     19\u001B[0m         \u001B[38;5;66;03m# print('Episode:{} Allocation:{} Time:{}'.format(episode, env.legal_allocation_list[action], env.time))\u001B[39;00m\n\u001B[0;32m---> 20\u001B[0m     n_state, reward, done, info \u001B[38;5;241m=\u001B[39m \u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     21\u001B[0m     score\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39mreward\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEpisode:\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m Total_reward:\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(episode, score))\n",
      "File \u001B[0;32m/opt/anaconda3/envs/RL-Research-2022/lib/python3.10/site-packages/gym/wrappers/order_enforcing.py:11\u001B[0m, in \u001B[0;36mOrderEnforcing.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep\u001B[39m(\u001B[38;5;28mself\u001B[39m, action):\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_has_reset, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot call env.step() before calling reset()\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m---> 11\u001B[0m     observation, reward, done, info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     12\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m observation, reward, done, info\n",
      "File \u001B[0;32m~/Desktop/Personal Storage/OneDrive/CIS Research/RL-Research-2022-Summer/RL-Research-2022-Summer/JSSP/envs/JSSP_env.py:316\u001B[0m, in \u001B[0;36mJSSPEnv.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m    314\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m done:\n\u001B[1;32m    315\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_obs(), \u001B[38;5;241m200\u001B[39m \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtime, done, {}\n\u001B[0;32m--> 316\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate_legal_allocation_list\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    317\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minitialize_action_space()\n\u001B[1;32m    318\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_obs(), reward, done, {}\n",
      "File \u001B[0;32m~/Desktop/Personal Storage/OneDrive/CIS Research/RL-Research-2022-Summer/RL-Research-2022-Summer/JSSP/envs/JSSP_env.py:236\u001B[0m, in \u001B[0;36mJSSPEnv.generate_legal_allocation_list\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    233\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m allocation \u001B[38;5;129;01min\u001B[39;00m allocation_list:\n\u001B[1;32m    234\u001B[0m     \u001B[38;5;66;03m# check for duplicate job allocation\u001B[39;00m\n\u001B[1;32m    235\u001B[0m     duplicate_check_list \u001B[38;5;241m=\u001B[39m [machine \u001B[38;5;28;01mfor\u001B[39;00m machine \u001B[38;5;129;01min\u001B[39;00m allocation \u001B[38;5;28;01mif\u001B[39;00m machine \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m--> 236\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munique\u001B[49m\u001B[43m(\u001B[49m\u001B[43mduplicate_check_list\u001B[49m\u001B[43m)\u001B[49m) \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mlen\u001B[39m(duplicate_check_list):\n\u001B[1;32m    237\u001B[0m         legal_allocations_list\u001B[38;5;241m.\u001B[39mappend(np\u001B[38;5;241m.\u001B[39marray(allocation))\n\u001B[1;32m    238\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlegal_allocation_list \u001B[38;5;241m=\u001B[39m legal_allocations_list\n",
      "File \u001B[0;32m<__array_function__ internals>:180\u001B[0m, in \u001B[0;36munique\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/RL-Research-2022/lib/python3.10/site-packages/numpy/lib/arraysetops.py:272\u001B[0m, in \u001B[0;36munique\u001B[0;34m(ar, return_index, return_inverse, return_counts, axis)\u001B[0m\n\u001B[1;32m    270\u001B[0m ar \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39masanyarray(ar)\n\u001B[1;32m    271\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m axis \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 272\u001B[0m     ret \u001B[38;5;241m=\u001B[39m \u001B[43m_unique1d\u001B[49m\u001B[43m(\u001B[49m\u001B[43mar\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_inverse\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_counts\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    273\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _unpack_tuple(ret)\n\u001B[1;32m    275\u001B[0m \u001B[38;5;66;03m# axis was specified and not None\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "def random_sampling(env, episodes):\n",
    "    env.reset()\n",
    "    max_score = -100000\n",
    "    max_episode = -1\n",
    "    max_action_list = []\n",
    "    max_time_list = []\n",
    "    for episode in range(1, episodes+1):\n",
    "        env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "        action_list = []\n",
    "        time_list = []\n",
    "        while not done:\n",
    "            #env.render()\n",
    "            action = env.action_space.sample()\n",
    "            if action != env.action_space.n -1:\n",
    "                action_list.append(env.legal_allocation_list[action])\n",
    "                time_list.append(env.time)\n",
    "                # print('Episode:{} Allocation:{} Time:{}'.format(episode, env.legal_allocation_list[action], env.time))\n",
    "            n_state, reward, done, info = env.step(action)\n",
    "            score+=reward\n",
    "        print('Episode:{} Total_reward:{}'.format(episode, score))\n",
    "        if score >= max_score:\n",
    "            max_score = score\n",
    "            max_episode = episode\n",
    "            max_action_list = action_list\n",
    "            max_time_list = time_list\n",
    "    print('From {}th Episode best policy has reward {}'.format(max_episode, max_score))\n",
    "    for i in range(len(max_action_list)):\n",
    "        print('The allocation chose at time {} is {}'.format(max_time_list[i], max_action_list[i]))\n",
    "\n",
    "random_sampling(env4, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# def createEpsilonGreedyPolicy(Q, epsilon, num_actions):\n",
    "# \t\"\"\"\n",
    "# \tCreates an epsilon-greedy policy based\n",
    "# \ton a given Q-function and epsilon.\n",
    "#\n",
    "# \tReturns a function that takes the state\n",
    "# \tas an input and returns the probabilities\n",
    "# \tfor each action in the form of a numpy array\n",
    "# \tof length of the action space(set of possible actions).\n",
    "# \t\"\"\"\n",
    "def policy(state, Q, epsilon, num_actions):\n",
    "\n",
    "    if state in Q:\n",
    "        best_action = np.argmax(Q[state])\n",
    "        Action_probabilities = np.ones(num_actions, dtype = float) * epsilon / num_actions\n",
    "        Action_probabilities[best_action] += (1.0 - epsilon)\n",
    "        return Action_probabilities\n",
    "\n",
    "    Action_probabilities = np.ones(num_actions, dtype = float) / num_actions\n",
    "    return Action_probabilities"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def update(Q, state, next_state, action, reward, eta, gamma):\n",
    "\n",
    "    if next_state not in Q:\n",
    "        Q_next_state_max = -1\n",
    "    else:\n",
    "        Q_next_state_max = max(Q[next_state])\n",
    "\n",
    "    Q[state][action] = Q[state][action] + eta * (reward + gamma * Q_next_state_max - Q[state][action])\n",
    "\n",
    "    return Q"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 has time 65\n",
      "Episode: 1 has time 69\n",
      "Episode: 2 has time 63\n",
      "Episode: 3 has time 75\n",
      "Episode: 4 has time 64\n",
      "Episode: 5 has time 50\n",
      "Episode: 6 has time 72\n",
      "Episode: 7 has time 83\n",
      "Episode: 8 has time 80\n",
      "Episode: 9 has time 79\n",
      "Episode: 10 has time 52\n",
      "Episode: 11 has time 61\n",
      "Episode: 12 has time 77\n",
      "Episode: 13 has time 75\n",
      "Episode: 14 has time 68\n",
      "Episode: 15 has time 64\n",
      "Episode: 16 has time 77\n",
      "Episode: 17 has time 75\n",
      "Episode: 18 has time 77\n",
      "Episode: 19 has time 61\n",
      "Episode: 20 has time 76\n",
      "Episode: 21 has time 83\n",
      "Episode: 22 has time 77\n",
      "Episode: 23 has time 76\n",
      "Episode: 24 has time 76\n",
      "Episode: 25 has time 76\n",
      "Episode: 26 has time 76\n",
      "Episode: 27 has time 76\n",
      "Episode: 28 has time 76\n",
      "Episode: 29 has time 76\n",
      "Episode: 30 has time 76\n",
      "Episode: 31 has time 63\n",
      "Episode: 32 has time 76\n",
      "Episode: 33 has time 76\n",
      "Episode: 34 has time 76\n",
      "Episode: 35 has time 76\n",
      "Episode: 36 has time 69\n",
      "Episode: 37 has time 77\n",
      "Episode: 38 has time 76\n",
      "Episode: 39 has time 76\n",
      "Episode: 40 has time 77\n",
      "Episode: 41 has time 76\n",
      "Episode: 42 has time 76\n",
      "Episode: 43 has time 61\n",
      "Episode: 44 has time 60\n",
      "Episode: 45 has time 76\n",
      "Episode: 46 has time 77\n",
      "Episode: 47 has time 76\n",
      "Episode: 48 has time 76\n",
      "Episode: 49 has time 76\n",
      "Episode: 50 has time 69\n",
      "Episode: 51 has time 76\n",
      "Episode: 52 has time 76\n",
      "Episode: 53 has time 76\n",
      "Episode: 54 has time 66\n",
      "Episode: 55 has time 76\n",
      "Episode: 56 has time 75\n",
      "Episode: 57 has time 76\n",
      "Episode: 58 has time 76\n",
      "Episode: 59 has time 76\n",
      "Episode: 60 has time 76\n",
      "Episode: 61 has time 76\n",
      "Episode: 62 has time 76\n",
      "Episode: 63 has time 77\n",
      "Episode: 64 has time 76\n",
      "Episode: 65 has time 76\n",
      "Episode: 66 has time 76\n",
      "Episode: 67 has time 76\n",
      "Episode: 68 has time 77\n",
      "Episode: 69 has time 76\n",
      "Episode: 70 has time 76\n",
      "Episode: 71 has time 70\n",
      "Episode: 72 has time 76\n",
      "Episode: 73 has time 69\n",
      "Episode: 74 has time 76\n",
      "Episode: 75 has time 76\n",
      "Episode: 76 has time 76\n",
      "Episode: 77 has time 76\n",
      "Episode: 78 has time 76\n",
      "Episode: 79 has time 76\n",
      "Episode: 80 has time 76\n",
      "Episode: 81 has time 76\n",
      "Episode: 82 has time 76\n",
      "Episode: 83 has time 69\n",
      "Episode: 84 has time 76\n",
      "Episode: 85 has time 76\n",
      "Episode: 86 has time 76\n",
      "Episode: 87 has time 76\n",
      "Episode: 88 has time 50\n",
      "Episode: 89 has time 77\n",
      "Episode: 90 has time 77\n",
      "Episode: 91 has time 76\n",
      "Episode: 92 has time 76\n",
      "Episode: 93 has time 76\n",
      "Episode: 94 has time 76\n",
      "Episode: 95 has time 76\n",
      "Episode: 96 has time 76\n",
      "Episode: 97 has time 76\n",
      "Episode: 98 has time 76\n",
      "Episode: 99 has time 76\n",
      "Episode: 100 has time 76\n",
      "Episode: 101 has time 76\n",
      "Episode: 102 has time 76\n",
      "Episode: 103 has time 76\n",
      "Episode: 104 has time 77\n",
      "Episode: 105 has time 77\n",
      "Episode: 106 has time 76\n",
      "Episode: 107 has time 76\n",
      "Episode: 108 has time 76\n",
      "Episode: 109 has time 76\n",
      "Episode: 110 has time 76\n",
      "Episode: 111 has time 77\n",
      "Episode: 112 has time 66\n",
      "Episode: 113 has time 76\n",
      "Episode: 114 has time 76\n",
      "Episode: 115 has time 76\n",
      "Episode: 116 has time 76\n",
      "Episode: 117 has time 77\n",
      "Episode: 118 has time 76\n",
      "Episode: 119 has time 60\n",
      "Episode: 120 has time 77\n",
      "Episode: 121 has time 76\n",
      "Episode: 122 has time 76\n",
      "Episode: 123 has time 76\n",
      "Episode: 124 has time 77\n",
      "Episode: 125 has time 76\n",
      "Episode: 126 has time 66\n",
      "Episode: 127 has time 76\n",
      "Episode: 128 has time 76\n",
      "Episode: 129 has time 76\n",
      "Episode: 130 has time 76\n",
      "Episode: 131 has time 76\n",
      "Episode: 132 has time 77\n",
      "Episode: 133 has time 76\n",
      "Episode: 134 has time 76\n",
      "Episode: 135 has time 76\n",
      "Episode: 136 has time 76\n",
      "Episode: 137 has time 77\n",
      "Episode: 138 has time 76\n",
      "Episode: 139 has time 70\n",
      "Episode: 140 has time 78\n",
      "Episode: 141 has time 76\n",
      "Episode: 142 has time 76\n",
      "Episode: 143 has time 67\n",
      "Episode: 144 has time 76\n",
      "Episode: 145 has time 79\n",
      "Episode: 146 has time 76\n",
      "Episode: 147 has time 76\n",
      "Episode: 148 has time 76\n",
      "Episode: 149 has time 76\n",
      "Episode: 150 has time 76\n",
      "Episode: 151 has time 60\n",
      "Episode: 152 has time 60\n",
      "Episode: 153 has time 76\n",
      "Episode: 154 has time 76\n",
      "Episode: 155 has time 76\n",
      "Episode: 156 has time 76\n",
      "Episode: 157 has time 76\n",
      "Episode: 158 has time 66\n",
      "Episode: 159 has time 76\n",
      "Episode: 160 has time 76\n",
      "Episode: 161 has time 77\n",
      "Episode: 162 has time 76\n",
      "Episode: 163 has time 76\n",
      "Episode: 164 has time 76\n",
      "Episode: 165 has time 76\n",
      "Episode: 166 has time 76\n",
      "Episode: 167 has time 76\n",
      "Episode: 168 has time 76\n",
      "Episode: 169 has time 76\n",
      "Episode: 170 has time 76\n",
      "Episode: 171 has time 77\n",
      "Episode: 172 has time 76\n",
      "Episode: 173 has time 76\n",
      "Episode: 174 has time 76\n",
      "Episode: 175 has time 76\n",
      "Episode: 176 has time 76\n",
      "Episode: 177 has time 76\n",
      "Episode: 178 has time 76\n",
      "Episode: 179 has time 76\n",
      "Episode: 180 has time 78\n",
      "Episode: 181 has time 76\n",
      "Episode: 182 has time 77\n",
      "Episode: 183 has time 76\n",
      "Episode: 184 has time 76\n",
      "Episode: 185 has time 76\n",
      "Episode: 186 has time 76\n",
      "Episode: 187 has time 76\n",
      "Episode: 188 has time 76\n",
      "Episode: 189 has time 76\n",
      "Episode: 190 has time 76\n",
      "Episode: 191 has time 76\n",
      "Episode: 192 has time 76\n",
      "Episode: 193 has time 77\n",
      "Episode: 194 has time 79\n",
      "Episode: 195 has time 76\n",
      "Episode: 196 has time 76\n",
      "Episode: 197 has time 79\n",
      "Episode: 198 has time 76\n",
      "Episode: 199 has time 76\n",
      "Episode: 200 has time 76\n",
      "Episode: 201 has time 76\n",
      "Episode: 202 has time 76\n",
      "Episode: 203 has time 76\n",
      "Episode: 204 has time 76\n",
      "Episode: 205 has time 76\n",
      "Episode: 206 has time 76\n",
      "Episode: 207 has time 76\n",
      "Episode: 208 has time 76\n",
      "Episode: 209 has time 43\n",
      "Episode: 210 has time 76\n",
      "Episode: 211 has time 76\n",
      "Episode: 212 has time 76\n",
      "Episode: 213 has time 76\n",
      "Episode: 214 has time 76\n",
      "Episode: 215 has time 76\n",
      "Episode: 216 has time 76\n",
      "Episode: 217 has time 76\n",
      "Episode: 218 has time 77\n",
      "Episode: 219 has time 66\n",
      "Episode: 220 has time 76\n",
      "Episode: 221 has time 76\n",
      "Episode: 222 has time 76\n",
      "Episode: 223 has time 76\n",
      "Episode: 224 has time 76\n",
      "Episode: 225 has time 76\n",
      "Episode: 226 has time 76\n",
      "Episode: 227 has time 76\n",
      "Episode: 228 has time 76\n",
      "Episode: 229 has time 76\n",
      "Episode: 230 has time 76\n",
      "Episode: 231 has time 76\n",
      "Episode: 232 has time 76\n",
      "Episode: 233 has time 76\n",
      "Episode: 234 has time 66\n",
      "Episode: 235 has time 76\n",
      "Episode: 236 has time 76\n",
      "Episode: 237 has time 76\n",
      "Episode: 238 has time 90\n",
      "Episode: 239 has time 76\n",
      "Episode: 240 has time 60\n",
      "Episode: 241 has time 76\n",
      "Episode: 242 has time 76\n",
      "Episode: 243 has time 76\n",
      "Episode: 244 has time 76\n",
      "Episode: 245 has time 66\n",
      "Episode: 246 has time 77\n",
      "Episode: 247 has time 76\n",
      "Episode: 248 has time 76\n",
      "Episode: 249 has time 77\n",
      "Episode: 250 has time 77\n",
      "Episode: 251 has time 77\n",
      "Episode: 252 has time 76\n",
      "Episode: 253 has time 76\n",
      "Episode: 254 has time 76\n",
      "Episode: 255 has time 60\n",
      "Episode: 256 has time 76\n",
      "Episode: 257 has time 76\n",
      "Episode: 258 has time 66\n",
      "Episode: 259 has time 76\n",
      "Episode: 260 has time 76\n",
      "Episode: 261 has time 78\n",
      "Episode: 262 has time 76\n",
      "Episode: 263 has time 78\n",
      "Episode: 264 has time 76\n",
      "Episode: 265 has time 76\n",
      "Episode: 266 has time 77\n",
      "Episode: 267 has time 76\n",
      "Episode: 268 has time 76\n",
      "Episode: 269 has time 76\n",
      "Episode: 270 has time 76\n",
      "Episode: 271 has time 76\n",
      "Episode: 272 has time 76\n",
      "Episode: 273 has time 76\n",
      "Episode: 274 has time 76\n",
      "Episode: 275 has time 76\n",
      "Episode: 276 has time 69\n",
      "Episode: 277 has time 76\n",
      "Episode: 278 has time 76\n",
      "Episode: 279 has time 76\n",
      "Episode: 280 has time 77\n",
      "Episode: 281 has time 76\n",
      "Episode: 282 has time 76\n",
      "Episode: 283 has time 77\n",
      "Episode: 284 has time 76\n",
      "Episode: 285 has time 76\n",
      "Episode: 286 has time 76\n",
      "Episode: 287 has time 76\n",
      "Episode: 288 has time 66\n",
      "Episode: 289 has time 76\n",
      "Episode: 290 has time 66\n",
      "Episode: 291 has time 76\n",
      "Episode: 292 has time 76\n",
      "Episode: 293 has time 76\n",
      "Episode: 294 has time 77\n",
      "Episode: 295 has time 76\n",
      "Episode: 296 has time 76\n",
      "Episode: 297 has time 76\n",
      "Episode: 298 has time 76\n",
      "Episode: 299 has time 76\n",
      "Episode: 300 has time 76\n",
      "Episode: 301 has time 76\n",
      "Episode: 302 has time 76\n",
      "Episode: 303 has time 76\n",
      "Episode: 304 has time 77\n",
      "Episode: 305 has time 76\n",
      "Episode: 306 has time 76\n",
      "Episode: 307 has time 76\n",
      "Episode: 308 has time 76\n",
      "Episode: 309 has time 76\n",
      "Episode: 310 has time 76\n",
      "Episode: 311 has time 77\n",
      "Episode: 312 has time 76\n",
      "Episode: 313 has time 76\n",
      "Episode: 314 has time 76\n",
      "Episode: 315 has time 76\n",
      "Episode: 316 has time 76\n",
      "Episode: 317 has time 76\n",
      "Episode: 318 has time 69\n",
      "Episode: 319 has time 76\n",
      "Episode: 320 has time 76\n",
      "Episode: 321 has time 76\n",
      "Episode: 322 has time 76\n",
      "Episode: 323 has time 76\n",
      "Episode: 324 has time 76\n",
      "Episode: 325 has time 76\n",
      "Episode: 326 has time 60\n",
      "Episode: 327 has time 76\n",
      "Episode: 328 has time 77\n",
      "Episode: 329 has time 76\n",
      "Episode: 330 has time 60\n",
      "Episode: 331 has time 76\n",
      "Episode: 332 has time 76\n",
      "Episode: 333 has time 76\n",
      "Episode: 334 has time 77\n",
      "Episode: 335 has time 76\n",
      "Episode: 336 has time 76\n",
      "Episode: 337 has time 69\n",
      "Episode: 338 has time 76\n",
      "Episode: 339 has time 76\n",
      "Episode: 340 has time 76\n",
      "Episode: 341 has time 76\n",
      "Episode: 342 has time 76\n",
      "Episode: 343 has time 76\n",
      "Episode: 344 has time 76\n",
      "Episode: 345 has time 76\n",
      "Episode: 346 has time 81\n",
      "Episode: 347 has time 76\n",
      "Episode: 348 has time 76\n",
      "Episode: 349 has time 76\n",
      "Episode: 350 has time 77\n",
      "Episode: 351 has time 59\n",
      "Episode: 352 has time 76\n",
      "Episode: 353 has time 76\n",
      "Episode: 354 has time 76\n",
      "Episode: 355 has time 69\n",
      "Episode: 356 has time 76\n",
      "Episode: 357 has time 76\n",
      "Episode: 358 has time 76\n",
      "Episode: 359 has time 76\n",
      "Episode: 360 has time 76\n",
      "Episode: 361 has time 76\n",
      "Episode: 362 has time 76\n",
      "Episode: 363 has time 66\n",
      "Episode: 364 has time 76\n",
      "Episode: 365 has time 76\n",
      "Episode: 366 has time 66\n",
      "Episode: 367 has time 76\n",
      "Episode: 368 has time 76\n",
      "Episode: 369 has time 76\n",
      "Episode: 370 has time 60\n",
      "Episode: 371 has time 61\n",
      "Episode: 372 has time 76\n",
      "Episode: 373 has time 75\n",
      "Episode: 374 has time 76\n",
      "Episode: 375 has time 68\n",
      "Episode: 376 has time 76\n",
      "Episode: 377 has time 76\n",
      "Episode: 378 has time 76\n",
      "Episode: 379 has time 76\n",
      "Episode: 380 has time 79\n",
      "Episode: 381 has time 76\n",
      "Episode: 382 has time 76\n",
      "Episode: 383 has time 76\n",
      "Episode: 384 has time 76\n",
      "Episode: 385 has time 76\n",
      "Episode: 386 has time 76\n",
      "Episode: 387 has time 76\n",
      "Episode: 388 has time 76\n",
      "Episode: 389 has time 76\n",
      "Episode: 390 has time 66\n",
      "Episode: 391 has time 76\n",
      "Episode: 392 has time 76\n",
      "Episode: 393 has time 75\n",
      "Episode: 394 has time 76\n",
      "Episode: 395 has time 77\n",
      "Episode: 396 has time 76\n",
      "Episode: 397 has time 77\n",
      "Episode: 398 has time 76\n",
      "Episode: 399 has time 76\n",
      "Episode: 400 has time 76\n",
      "Episode: 401 has time 76\n",
      "Episode: 402 has time 76\n",
      "Episode: 403 has time 76\n",
      "Episode: 404 has time 76\n",
      "Episode: 405 has time 76\n",
      "Episode: 406 has time 76\n",
      "Episode: 407 has time 76\n",
      "Episode: 408 has time 76\n",
      "Episode: 409 has time 76\n",
      "Episode: 410 has time 76\n",
      "Episode: 411 has time 60\n",
      "Episode: 412 has time 66\n",
      "Episode: 413 has time 76\n",
      "Episode: 414 has time 83\n",
      "Episode: 415 has time 76\n",
      "Episode: 416 has time 76\n",
      "Episode: 417 has time 77\n",
      "Episode: 418 has time 76\n",
      "Episode: 419 has time 69\n",
      "Episode: 420 has time 76\n",
      "Episode: 421 has time 76\n",
      "Episode: 422 has time 77\n",
      "Episode: 423 has time 76\n",
      "Episode: 424 has time 76\n",
      "Episode: 425 has time 76\n",
      "Episode: 426 has time 90\n",
      "Episode: 427 has time 76\n",
      "Episode: 428 has time 76\n",
      "Episode: 429 has time 76\n",
      "Episode: 430 has time 76\n",
      "Episode: 431 has time 76\n",
      "Episode: 432 has time 76\n",
      "Episode: 433 has time 78\n",
      "Episode: 434 has time 76\n",
      "Episode: 435 has time 76\n",
      "Episode: 436 has time 67\n",
      "Episode: 437 has time 76\n",
      "Episode: 438 has time 69\n",
      "Episode: 439 has time 76\n",
      "Episode: 440 has time 75\n",
      "Episode: 441 has time 76\n",
      "Episode: 442 has time 76\n",
      "Episode: 443 has time 77\n",
      "Episode: 444 has time 67\n",
      "Episode: 445 has time 78\n",
      "Episode: 446 has time 69\n",
      "Episode: 447 has time 78\n",
      "Episode: 448 has time 76\n",
      "Episode: 449 has time 61\n",
      "Episode: 450 has time 77\n",
      "Episode: 451 has time 77\n",
      "Episode: 452 has time 77\n",
      "Episode: 453 has time 77\n",
      "Episode: 454 has time 77\n",
      "Episode: 455 has time 77\n",
      "Episode: 456 has time 77\n",
      "Episode: 457 has time 76\n",
      "Episode: 458 has time 60\n",
      "Episode: 459 has time 68\n",
      "Episode: 460 has time 74\n",
      "Episode: 461 has time 76\n",
      "Episode: 462 has time 77\n",
      "Episode: 463 has time 78\n",
      "Episode: 464 has time 76\n",
      "Episode: 465 has time 69\n",
      "Episode: 466 has time 77\n",
      "Episode: 467 has time 69\n",
      "Episode: 468 has time 77\n",
      "Episode: 469 has time 77\n",
      "Episode: 470 has time 77\n",
      "Episode: 471 has time 78\n",
      "Episode: 472 has time 79\n",
      "Episode: 473 has time 77\n",
      "Episode: 474 has time 76\n",
      "Episode: 475 has time 77\n",
      "Episode: 476 has time 77\n",
      "Episode: 477 has time 62\n",
      "Episode: 478 has time 77\n",
      "Episode: 479 has time 77\n",
      "Episode: 480 has time 67\n",
      "Episode: 481 has time 77\n",
      "Episode: 482 has time 77\n",
      "Episode: 483 has time 77\n",
      "Episode: 484 has time 70\n",
      "Episode: 485 has time 76\n",
      "Episode: 486 has time 76\n",
      "Episode: 487 has time 77\n",
      "Episode: 488 has time 79\n",
      "Episode: 489 has time 77\n",
      "Episode: 490 has time 60\n",
      "Episode: 491 has time 78\n",
      "Episode: 492 has time 76\n",
      "Episode: 493 has time 76\n",
      "Episode: 494 has time 78\n",
      "Episode: 495 has time 77\n",
      "Episode: 496 has time 77\n",
      "Episode: 497 has time 76\n",
      "Episode: 498 has time 77\n",
      "Episode: 499 has time 77\n",
      "From 210th Episode best policy has reward 65\n",
      "The allocation chose at time 0 is [-1  0]\n",
      "The allocation chose at time 10 is [0 1]\n",
      "The allocation chose at time 20 is [ 2 -1]\n",
      "The allocation chose at time 28 is [-1  1]\n"
     ]
    }
   ],
   "source": [
    "def q_learning(env, epis):\n",
    "\n",
    "    max_score = -100000\n",
    "    max_episode = -1\n",
    "    max_action_list = []\n",
    "    max_time_list = []\n",
    "\n",
    "    # 1. Load Environment and Q-table structure\n",
    "    Q_table = {}\n",
    "    # 2. Parameters of Q-learning\n",
    "    eta = .628\n",
    "    gamma = 1\n",
    "    epsilon = .1\n",
    "    decay_rate = .0001\n",
    "    total_reward_list = [] # rewards per episode calculate\n",
    "    # 3. Q-learning Algorithm\n",
    "    for episode in range(epis):\n",
    "        # Reset environment\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        action_list = []\n",
    "        time_list = []\n",
    "\n",
    "        # The Q-Table learning algorithm\n",
    "        while not done:\n",
    "            # if epsilon >= decay_rate:\n",
    "            #     epsilon -= decay_rate\n",
    "            if state not in Q_table:\n",
    "                Q_table[state] = np.zeros(env.action_space.n)\n",
    "            action_probabilities = policy(state, Q_table, epsilon, env.action_space.n)\n",
    "            action = np.random.choice(np.arange(len(action_probabilities)), p = action_probabilities)\n",
    "            if action != env.action_space.n -1:\n",
    "                action_list.append(env.legal_allocation_list[action])\n",
    "                time_list.append(env.time)\n",
    "                #print('Episode:{} Allocation:{} Time:{}'.format(episode, env.legal_allocation_list[action], env.time))\n",
    "            #Get new state & reward from environment\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            #Update Q-Table with new knowledge\n",
    "            Q_table = update(Q_table, state, next_state, action, reward, eta, gamma)\n",
    "            if eta > .01:\n",
    "                eta -= decay_rate\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "        total_reward_list.append(total_reward)\n",
    "        if total_reward >= max_score:\n",
    "            max_score = total_reward\n",
    "            max_episode = episode\n",
    "            max_action_list = action_list\n",
    "            max_time_list = time_list\n",
    "        print(\"Episode: \" + str(episode) + \" has time \" + str(env.time))\n",
    "\n",
    "    print('From {}th Episode best policy has reward {}'.format(max_episode + 1, max_score))\n",
    "    for i in range(len(max_action_list)):\n",
    "        print('The allocation chose at time {} is {}'.format(max_time_list[i], max_action_list[i]))\n",
    "\n",
    "    return total_reward_list, Q_table\n",
    "\n",
    "total_reward_list, Q_table = q_learning(env1, 500)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "<function matplotlib.pyplot.show(close=None, block=None)>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(total_reward_list)\n",
    "plt.show\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(total_reward_list)\n",
    "plt.show"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(Q_table)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DQN"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "\n",
    "model = DQN(\"MlpPolicy\", env1, verbose=1)\n",
    "model.learn(total_timesteps=10000, log_interval=4)\n",
    "model.save(\"dqn_cartpole\")\n",
    "\n",
    "del model # remove to demonstrate saving and loading\n",
    "\n",
    "model = DQN.load(\"dqn_cartpole\")\n",
    "\n",
    "obs = env1.reset()\n",
    "for episode in range(500):\n",
    "    env1.reset()\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, info = env1.step(action)\n",
    "        total_reward += reward\n",
    "    print(\"Episode: \" + str(episode) + \" has reward \" + str(total_reward))\n",
    "        # env.render()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Q Learning from web"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def createEpsilonGreedyPolicy(Q, epsilon, num_actions, env):\n",
    "\t\"\"\"\n",
    "\tCreates an epsilon-greedy policy based\n",
    "\ton a given Q-function and epsilon.\n",
    "\n",
    "\tReturns a function that takes the state\n",
    "\tas an input and returns the probabilities\n",
    "\tfor each action in the form of a numpy array\n",
    "\tof length of the action space(set of possible actions).\n",
    "\t\"\"\"\n",
    "\tdef policyFunction(state):\n",
    "\n",
    "\t\tAction_probabilities = np.ones(num_actions,\n",
    "\t\t\t\tdtype = float) * epsilon / num_actions\n",
    "\n",
    "\t\tbest_action = np.argmax(Q[state])\n",
    "\t\tAction_probabilities[best_action] += (1.0 - epsilon)\n",
    "\t\treturn Action_probabilities\n",
    "\n",
    "\treturn policyFunction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def qLearning(env, num_episodes, discount_factor = 1.0,\n",
    "\t\t\t\t\t\t\talpha = 0.6, epsilon = 0.1):\n",
    "\t\"\"\"\n",
    "\tQ-Learning algorithm: Off-policy TD control.\n",
    "\tFinds the optimal greedy policy while improving\n",
    "\tfollowing an epsilon-greedy policy\"\"\"\n",
    "\n",
    "\t# Action value function\n",
    "\t# A nested dictionary that maps\n",
    "\t# state -> (action -> action-value).\n",
    "\tQ = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "\t# Keeps track of useful statistics\n",
    "\tstats = {\n",
    "            \"episode_lengths\" : np.zeros(num_episodes),\n",
    "\t\t    \"episode_rewards\" : np.zeros(num_episodes)}\n",
    "\n",
    "\t# Create an epsilon greedy policy function\n",
    "\t# appropriately for environment action space\n",
    "\tpolicy = createEpsilonGreedyPolicy(Q, epsilon, env.action_space.n, env)\n",
    "\n",
    "\t# For every episode\n",
    "\tfor ith_episode in range(num_episodes):\n",
    "\n",
    "\t\t# Reset the environment and pick the first action\n",
    "\t\tstate = env.reset()\n",
    "\n",
    "\t\tfor t in itertools.count():\n",
    "\n",
    "\t\t\t# get probabilities of all actions from current state\n",
    "\t\t\taction_probabilities = policy(state)\n",
    "\n",
    "\t\t\t# choose action according to\n",
    "\t\t\t# the probability distribution\n",
    "\t\t\taction = np.random.choice(np.arange(\n",
    "\t\t\t\t\tlen(action_probabilities)),\n",
    "\t\t\t\t\tp = action_probabilities)\n",
    "\n",
    "\t\t\t# take action and get reward, transit to next state\n",
    "\t\t\tnext_state, reward, done, _ = env.step(action)\n",
    "\n",
    "\t\t\t# Update statistics\n",
    "\t\t\tstats[\"episode_rewards\"][ith_episode] += reward\n",
    "\t\t\tstats[\"episode_lengths\"][ith_episode] = t\n",
    "\n",
    "\t\t\t# TD Update\n",
    "\t\t\tbest_next_action = np.argmax(Q[next_state])\n",
    "\t\t\ttd_target = reward + discount_factor * Q[next_state][best_next_action]\n",
    "\t\t\ttd_delta = td_target - Q[state][action]\n",
    "\t\t\tQ[state][action] += alpha * td_delta\n",
    "\n",
    "\t\t\t# done is True if episode terminated\n",
    "\t\t\tif done:\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\t\tstate = next_state\n",
    "\n",
    "\treturn Q, stats"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Q, stats = qLearning(env1, 1000)\n",
    "plt.plot(stats[\"episode_lengths\"])\n",
    "plt.plot(stats[\"episode_rewards\"])\n",
    "plt.show"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(np.append(np.array([1,2]),np.array([1,2])))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sum([1 for machine in np.array([-1, -1, -1, 2]) if machine >= 0])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Q, stats = qLearning(env1, 1000)\n",
    "plt.plot(stats[\"episode_lengths\"])\n",
    "plt.plot(stats[\"episode_rewards\"])\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Q Learning from web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def createEpsilonGreedyPolicy(Q, epsilon, num_actions, env):\n",
    "\t\"\"\"\n",
    "\tCreates an epsilon-greedy policy based\n",
    "\ton a given Q-function and epsilon.\n",
    "\n",
    "\tReturns a function that takes the state\n",
    "\tas an input and returns the probabilities\n",
    "\tfor each action in the form of a numpy array\n",
    "\tof length of the action space(set of possible actions).\n",
    "\t\"\"\"\n",
    "\tdef policyFunction(state):\n",
    "\n",
    "\t\tAction_probabilities = np.ones(num_actions,\n",
    "\t\t\t\tdtype = float) * epsilon / num_actions\n",
    "\n",
    "\t\tbest_action = np.argmax(Q[state])\n",
    "\t\tAction_probabilities[best_action] += (1.0 - epsilon)\n",
    "\t\treturn Action_probabilities\n",
    "\n",
    "\treturn policyFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def qLearning(env, num_episodes, discount_factor = 1.0,\n",
    "\t\t\t\t\t\t\talpha = 0.6, epsilon = 0.1):\n",
    "\t\"\"\"\n",
    "\tQ-Learning algorithm: Off-policy TD control.\n",
    "\tFinds the optimal greedy policy while improving\n",
    "\tfollowing an epsilon-greedy policy\"\"\"\n",
    "\n",
    "\t# Action value function\n",
    "\t# A nested dictionary that maps\n",
    "\t# state -> (action -> action-value).\n",
    "\tQ = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "\t# Keeps track of useful statistics\n",
    "\tstats = {\n",
    "            \"episode_lengths\" : np.zeros(num_episodes),\n",
    "\t\t    \"episode_rewards\" : np.zeros(num_episodes)}\n",
    "\n",
    "\t# Create an epsilon greedy policy function\n",
    "\t# appropriately for environment action space\n",
    "\tpolicy = createEpsilonGreedyPolicy(Q, epsilon, env.action_space.n, env)\n",
    "\n",
    "\t# For every episode\n",
    "\tfor ith_episode in range(num_episodes):\n",
    "\n",
    "\t\t# Reset the environment and pick the first action\n",
    "\t\tstate = env.reset()\n",
    "\n",
    "\t\tfor t in itertools.count():\n",
    "\n",
    "\t\t\t# get probabilities of all actions from current state\n",
    "\t\t\taction_probabilities = policy(state)\n",
    "\n",
    "\t\t\t# choose action according to\n",
    "\t\t\t# the probability distribution\n",
    "\t\t\taction = np.random.choice(np.arange(\n",
    "\t\t\t\t\tlen(action_probabilities)),\n",
    "\t\t\t\t\tp = action_probabilities)\n",
    "\n",
    "\t\t\t# take action and get reward, transit to next state\n",
    "\t\t\tnext_state, reward, done, _ = env.step(action)\n",
    "\n",
    "\t\t\t# Update statistics\n",
    "\t\t\tstats[\"episode_rewards\"][ith_episode] += reward\n",
    "\t\t\tstats[\"episode_lengths\"][ith_episode] = t\n",
    "\n",
    "\t\t\t# TD Update\n",
    "\t\t\tbest_next_action = np.argmax(Q[next_state])\n",
    "\t\t\ttd_target = reward + discount_factor * Q[next_state][best_next_action]\n",
    "\t\t\ttd_delta = td_target - Q[state][action]\n",
    "\t\t\tQ[state][action] += alpha * td_delta\n",
    "\n",
    "\t\t\t# done is True if episode terminated\n",
    "\t\t\tif done:\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\t\tstate = next_state\n",
    "\n",
    "\treturn Q, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Q, stats = qLearning(env1, 1000)\n",
    "plt.plot(stats[\"episode_lengths\"])\n",
    "plt.plot(stats[\"episode_rewards\"])\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Q Learning from web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def createEpsilonGreedyPolicy(Q, epsilon, num_actions, env):\n",
    "\t\"\"\"\n",
    "\tCreates an epsilon-greedy policy based\n",
    "\ton a given Q-function and epsilon.\n",
    "\n",
    "\tReturns a function that takes the state\n",
    "\tas an input and returns the probabilities\n",
    "\tfor each action in the form of a numpy array\n",
    "\tof length of the action space(set of possible actions).\n",
    "\t\"\"\"\n",
    "\tdef policyFunction(state):\n",
    "\n",
    "\t\tAction_probabilities = np.ones(num_actions,\n",
    "\t\t\t\tdtype = float) * epsilon / num_actions\n",
    "\n",
    "\t\tbest_action = np.argmax(Q[state])\n",
    "\t\tAction_probabilities[best_action] += (1.0 - epsilon)\n",
    "\t\treturn Action_probabilities\n",
    "\n",
    "\treturn policyFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def qLearning(env, num_episodes, discount_factor = 1.0,\n",
    "\t\t\t\t\t\t\talpha = 0.6, epsilon = 0.1):\n",
    "\t\"\"\"\n",
    "\tQ-Learning algorithm: Off-policy TD control.\n",
    "\tFinds the optimal greedy policy while improving\n",
    "\tfollowing an epsilon-greedy policy\"\"\"\n",
    "\n",
    "\t# Action value function\n",
    "\t# A nested dictionary that maps\n",
    "\t# state -> (action -> action-value).\n",
    "\tQ = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "\t# Keeps track of useful statistics\n",
    "\tstats = {\n",
    "            \"episode_lengths\" : np.zeros(num_episodes),\n",
    "\t\t    \"episode_rewards\" : np.zeros(num_episodes)}\n",
    "\n",
    "\t# Create an epsilon greedy policy function\n",
    "\t# appropriately for environment action space\n",
    "\tpolicy = createEpsilonGreedyPolicy(Q, epsilon, env.action_space.n, env)\n",
    "\n",
    "\t# For every episode\n",
    "\tfor ith_episode in range(num_episodes):\n",
    "\n",
    "\t\t# Reset the environment and pick the first action\n",
    "\t\tstate = env.reset()\n",
    "\n",
    "\t\tfor t in itertools.count():\n",
    "\n",
    "\t\t\t# get probabilities of all actions from current state\n",
    "\t\t\taction_probabilities = policy(state)\n",
    "\n",
    "\t\t\t# choose action according to\n",
    "\t\t\t# the probability distribution\n",
    "\t\t\taction = np.random.choice(np.arange(\n",
    "\t\t\t\t\tlen(action_probabilities)),\n",
    "\t\t\t\t\tp = action_probabilities)\n",
    "\n",
    "\t\t\t# take action and get reward, transit to next state\n",
    "\t\t\tnext_state, reward, done, _ = env.step(action)\n",
    "\n",
    "\t\t\t# Update statistics\n",
    "\t\t\tstats[\"episode_rewards\"][ith_episode] += reward\n",
    "\t\t\tstats[\"episode_lengths\"][ith_episode] = t\n",
    "\n",
    "\t\t\t# TD Update\n",
    "\t\t\tbest_next_action = np.argmax(Q[next_state])\n",
    "\t\t\ttd_target = reward + discount_factor * Q[next_state][best_next_action]\n",
    "\t\t\ttd_delta = td_target - Q[state][action]\n",
    "\t\t\tQ[state][action] += alpha * td_delta\n",
    "\n",
    "\t\t\t# done is True if episode terminated\n",
    "\t\t\tif done:\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\t\tstate = next_state\n",
    "\n",
    "\treturn Q, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def createEpsilonGreedyPolicy(Q, epsilon, num_actions, env):\n",
    "\t\"\"\"\n",
    "\tCreates an epsilon-greedy policy based\n",
    "\ton a given Q-function and epsilon.\n",
    "\n",
    "\tReturns a function that takes the state\n",
    "\tas an input and returns the probabilities\n",
    "\tfor each action in the form of a numpy array\n",
    "\tof length of the action space(set of possible actions).\n",
    "\t\"\"\"\n",
    "\tdef policyFunction(state):\n",
    "\n",
    "\t\tAction_probabilities = np.ones(num_actions,\n",
    "\t\t\t\tdtype = float) * epsilon / num_actions\n",
    "\n",
    "\t\tbest_action = np.argmax(Q[state])\n",
    "\t\tAction_probabilities[best_action] += (1.0 - epsilon)\n",
    "\t\treturn Action_probabilities\n",
    "\n",
    "\treturn policyFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def qLearning(env, num_episodes, discount_factor = 1.0,\n",
    "\t\t\t\t\t\t\talpha = 0.6, epsilon = 0.1):\n",
    "\t\"\"\"\n",
    "\tQ-Learning algorithm: Off-policy TD control.\n",
    "\tFinds the optimal greedy policy while improving\n",
    "\tfollowing an epsilon-greedy policy\"\"\"\n",
    "\n",
    "\t# Action value function\n",
    "\t# A nested dictionary that maps\n",
    "\t# state -> (action -> action-value).\n",
    "\tQ = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "\t# Keeps track of useful statistics\n",
    "\tstats = {\n",
    "            \"episode_lengths\" : np.zeros(num_episodes),\n",
    "\t\t    \"episode_rewards\" : np.zeros(num_episodes)}\n",
    "\n",
    "\t# Create an epsilon greedy policy function\n",
    "\t# appropriately for environment action space\n",
    "\tpolicy = createEpsilonGreedyPolicy(Q, epsilon, env.action_space.n, env)\n",
    "\n",
    "\t# For every episode\n",
    "\tfor ith_episode in range(num_episodes):\n",
    "\n",
    "\t\t# Reset the environment and pick the first action\n",
    "\t\tstate = env.reset()\n",
    "\n",
    "\t\tfor t in itertools.count():\n",
    "\n",
    "\t\t\t# get probabilities of all actions from current state\n",
    "\t\t\taction_probabilities = policy(state)\n",
    "\n",
    "\t\t\t# choose action according to\n",
    "\t\t\t# the probability distribution\n",
    "\t\t\taction = np.random.choice(np.arange(\n",
    "\t\t\t\t\tlen(action_probabilities)),\n",
    "\t\t\t\t\tp = action_probabilities)\n",
    "\n",
    "\t\t\t# take action and get reward, transit to next state\n",
    "\t\t\tnext_state, reward, done, _ = env.step(action)\n",
    "\n",
    "\t\t\t# Update statistics\n",
    "\t\t\tstats[\"episode_rewards\"][ith_episode] += reward\n",
    "\t\t\tstats[\"episode_lengths\"][ith_episode] = t\n",
    "\n",
    "\t\t\t# TD Update\n",
    "\t\t\tbest_next_action = np.argmax(Q[next_state])\n",
    "\t\t\ttd_target = reward + discount_factor * Q[next_state][best_next_action]\n",
    "\t\t\ttd_delta = td_target - Q[state][action]\n",
    "\t\t\tQ[state][action] += alpha * td_delta\n",
    "\n",
    "\t\t\t# done is True if episode terminated\n",
    "\t\t\tif done:\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\t\tstate = next_state\n",
    "\n",
    "\treturn Q, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Q, stats = qLearning(env1, 1000)\n",
    "plt.plot(stats[\"episode_lengths\"])\n",
    "plt.plot(stats[\"episode_rewards\"])\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Q Learning from web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def createEpsilonGreedyPolicy(Q, epsilon, num_actions, env):\n",
    "\t\"\"\"\n",
    "\tCreates an epsilon-greedy policy based\n",
    "\ton a given Q-function and epsilon.\n",
    "\n",
    "\tReturns a function that takes the state\n",
    "\tas an input and returns the probabilities\n",
    "\tfor each action in the form of a numpy array\n",
    "\tof length of the action space(set of possible actions).\n",
    "\t\"\"\"\n",
    "\tdef policyFunction(state):\n",
    "\n",
    "\t\tAction_probabilities = np.ones(num_actions,\n",
    "\t\t\t\tdtype = float) * epsilon / num_actions\n",
    "\n",
    "\t\tbest_action = np.argmax(Q[state])\n",
    "\t\tAction_probabilities[best_action] += (1.0 - epsilon)\n",
    "\t\treturn Action_probabilities\n",
    "\n",
    "\treturn policyFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def qLearning(env, num_episodes, discount_factor = 1.0,\n",
    "\t\t\t\t\t\t\talpha = 0.6, epsilon = 0.1):\n",
    "\t\"\"\"\n",
    "\tQ-Learning algorithm: Off-policy TD control.\n",
    "\tFinds the optimal greedy policy while improving\n",
    "\tfollowing an epsilon-greedy policy\"\"\"\n",
    "\n",
    "\t# Action value function\n",
    "\t# A nested dictionary that maps\n",
    "\t# state -> (action -> action-value).\n",
    "\tQ = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "\t# Keeps track of useful statistics\n",
    "\tstats = {\n",
    "            \"episode_lengths\" : np.zeros(num_episodes),\n",
    "\t\t    \"episode_rewards\" : np.zeros(num_episodes)}\n",
    "\n",
    "\t# Create an epsilon greedy policy function\n",
    "\t# appropriately for environment action space\n",
    "\tpolicy = createEpsilonGreedyPolicy(Q, epsilon, env.action_space.n, env)\n",
    "\n",
    "\t# For every episode\n",
    "\tfor ith_episode in range(num_episodes):\n",
    "\n",
    "\t\t# Reset the environment and pick the first action\n",
    "\t\tstate = env.reset()\n",
    "\n",
    "\t\tfor t in itertools.count():\n",
    "\n",
    "\t\t\t# get probabilities of all actions from current state\n",
    "\t\t\taction_probabilities = policy(state)\n",
    "\n",
    "\t\t\t# choose action according to\n",
    "\t\t\t# the probability distribution\n",
    "\t\t\taction = np.random.choice(np.arange(\n",
    "\t\t\t\t\tlen(action_probabilities)),\n",
    "\t\t\t\t\tp = action_probabilities)\n",
    "\n",
    "\t\t\t# take action and get reward, transit to next state\n",
    "\t\t\tnext_state, reward, done, _ = env.step(action)\n",
    "\n",
    "\t\t\t# Update statistics\n",
    "\t\t\tstats[\"episode_rewards\"][ith_episode] += reward\n",
    "\t\t\tstats[\"episode_lengths\"][ith_episode] = t\n",
    "\n",
    "\t\t\t# TD Update\n",
    "\t\t\tbest_next_action = np.argmax(Q[next_state])\n",
    "\t\t\ttd_target = reward + discount_factor * Q[next_state][best_next_action]\n",
    "\t\t\ttd_delta = td_target - Q[state][action]\n",
    "\t\t\tQ[state][action] += alpha * td_delta\n",
    "\n",
    "\t\t\t# done is True if episode terminated\n",
    "\t\t\tif done:\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\t\tstate = next_state\n",
    "\n",
    "\treturn Q, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Q, stats = qLearning(env1, 1000)\n",
    "plt.plot(stats[\"episode_lengths\"])\n",
    "plt.plot(stats[\"episode_rewards\"])\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}