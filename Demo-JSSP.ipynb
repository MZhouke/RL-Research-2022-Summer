{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import JSSP\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import matplotlib.style\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "#import plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Environment Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_env(instance_path):\n",
    "    env_name = \"JSSP-v0\"\n",
    "    env = gym.make(env_name, instance_path = instance_path)\n",
    "    print(\"Environment Created for: \", instance_path)\n",
    "    print(\"Observation space: \\n\", env.observation_space)\n",
    "    print(\"Action space: \\n\", env.action_space)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env1 = create_env(\"instance1.txt\") #-53\n",
    "env3 = create_env(\"instance3.txt\") #-981\n",
    "env4 = create_env(\"instance4.txt\")\n",
    "env5 = create_env(\"instance5.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. Random Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def random_sampling(env, episodes):\n",
    "    env.reset()\n",
    "    max_score = -100000\n",
    "    max_episode = -1\n",
    "    max_action_list = []\n",
    "    max_time_list = []\n",
    "    for episode in range(1, episodes+1):\n",
    "        env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "        action_list = []\n",
    "        time_list = []\n",
    "        while not done:\n",
    "            #env.render()\n",
    "            action = env.action_space.sample()\n",
    "            if action != env.action_space.n -1:\n",
    "                action_list.append(env.legal_allocation_list[action])\n",
    "                time_list.append(env.time)\n",
    "                # print('Episode:{} Allocation:{} Time:{}'.format(episode, env.legal_allocation_list[action], env.time))\n",
    "            n_state, reward, done, info = env.step(action)\n",
    "            score+=reward\n",
    "        # print('Episode:{} Total_reward:{}'.format(episode, score))\n",
    "        if score >= max_score:\n",
    "            max_score = score\n",
    "            max_episode = episode\n",
    "            max_action_list = action_list\n",
    "            max_time_list = time_list\n",
    "    print('From {}th Episode best policy has reward {}'.format(max_episode, max_score))\n",
    "    for i in range(len(max_action_list)):\n",
    "        print('The allocation chose at time {} is {}'.format(max_time_list[i], max_action_list[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def policy(state, Q, epsilon, num_actions):\n",
    "    \"\"\"\n",
    "    function that returns the probability for action choosing based on a given Q-function and epsilon at this state\n",
    "    :param state: tuple of order 2 * number of jobs\n",
    "    :param Q: Q table which is a dictionary with states as first class keys and actions as second class keys\n",
    "    :param epsilon: for epsilon greedy choosing algorithm\n",
    "    :param num_actions: number of legal actions at this state\n",
    "    :return: an array of order num_actions containing probability for action choosing at this state\n",
    "    \"\"\"\n",
    "    # if state has been visited before, use epsilon greedy algorithm to generate probability\n",
    "    if state in Q:\n",
    "        best_action = np.argmax(Q[state])\n",
    "        Action_probabilities = np.ones(num_actions, dtype = float) * epsilon / num_actions\n",
    "        Action_probabilities[best_action] += (1.0 - epsilon)\n",
    "        return Action_probabilities\n",
    "\n",
    "    # if state hasn't been visited before, choose action with equal probability\n",
    "    Action_probabilities = np.ones(num_actions, dtype = float) / num_actions\n",
    "    return Action_probabilities"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def update(Q, state, next_state, action, reward, eta, gamma, threshold):\n",
    "    \"\"\"\n",
    "    Q-table updating step\n",
    "    :param Q: Q table which is a dictionary with states as first class keys and actions as second class keys\n",
    "    :param state: current state which is a tuple of order 2 * number of jobs\n",
    "    :param next_state: next state which is a tuple of order 2 * number of jobs\n",
    "    :param action: action between current state and next state\n",
    "    :param reward: reward for the action\n",
    "    :param eta: learning rate\n",
    "    :param gamma: discounted factor\n",
    "    :return: updated Q table\n",
    "    \"\"\"\n",
    "    # if next state has no data, set the maximum Q value to be 0\n",
    "    if next_state not in Q:\n",
    "        Q_next_state_max = 0\n",
    "    else:\n",
    "        Q_next_state_max = max(Q[next_state])\n",
    "\n",
    "    # update the Q table iff the difference is larger than a threshold\n",
    "    Q_update = eta * (reward + gamma * Q_next_state_max - Q[state][action])\n",
    "    if abs(Q_update) >= threshold:\n",
    "        Q[state][action] = Q[state][action] + Q_update\n",
    "\n",
    "    return Q"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def q_learning(env, epis, eta = .628, gamma = 1, epsilon = .3, threshold = .001):\n",
    "    \"\"\"\n",
    "    q-learning algorithm that returns the best policy, and data for each episode\n",
    "    :param threshold: Threshold for checking convergence of Q_table\n",
    "    :param eta: learning rate, how much you accept the new value vs the old value\n",
    "    :param gamma: discount factor, balance immediate and future reward\n",
    "    :param epsilon: randomness factor, how random the selection is\n",
    "    :param decay_rate: how much less randomness for each episode:\n",
    "    :param env: JSSP instance\n",
    "    :param epis: number of episodes\n",
    "    :return: a list of total reward for each episode, the final Q_table, and a list of total reward every 100 episodes\n",
    "    \"\"\"\n",
    "    max_score = -100000\n",
    "    max_episode = -1\n",
    "    max_action_list = []\n",
    "    max_time_list = []\n",
    "    decay_rate = epsilon / epis\n",
    "\n",
    "    # 1. Load Environment and Q-table structure\n",
    "    Q_table = {}\n",
    "    total_reward_list = [] # rewards per episode calculate\n",
    "    testing_reward_list = [] # rewards per 100 episodes\n",
    "    # 3. Q-learning Algorithm\n",
    "    for episode in range(epis):\n",
    "        # Reset environment\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        action_list = []\n",
    "        time_list = []\n",
    "\n",
    "        # Epsilon decay\n",
    "        if epsilon > decay_rate:\n",
    "            epsilon -= decay_rate\n",
    "        # The Q-Table learning algorithm\n",
    "        while not done:\n",
    "            # initialize state in Q table\n",
    "            if state not in Q_table:\n",
    "                Q_table[state] = np.zeros(env.action_space.n)\n",
    "            # generate action choosing probability and choose an action\n",
    "            # if this is a testing episode, epsilon = 1\n",
    "            action_probabilities = policy(state, Q_table, 0 if (episode % 100 == 0) else epsilon, env.action_space.n)\n",
    "            action = np.random.choice(np.arange(len(action_probabilities)), p = action_probabilities)\n",
    "            # update action history\n",
    "            action_list.append(env.legal_allocation_list[action])\n",
    "            time_list.append(env.time)\n",
    "            # get new state & reward from environment\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            #Update Q-Table with new knowledge\n",
    "            Q_table = update(Q_table, state, next_state, action, reward, eta, gamma, threshold)\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "        if state not in Q_table:\n",
    "                Q_table[state] = np.zeros(1)\n",
    "        total_reward_list.append(total_reward)\n",
    "        # keep policy testing results every 100 episodes\n",
    "        if episode % 100 == 0:\n",
    "            testing_reward_list.append(total_reward)\n",
    "            print(\"Episode: \" + str(episode) + \" has time \" + str(env.time) + \" has reward \" + str(total_reward))\n",
    "        # keep track of the optimal policy\n",
    "        if total_reward >= max_score:\n",
    "            max_score = total_reward\n",
    "            max_episode = episode\n",
    "            max_action_list = action_list\n",
    "            max_time_list = time_list\n",
    "\n",
    "    print('From {}th Episode best policy has reward {}'.format(max_episode + 1, max_score))\n",
    "    for i in range(len(max_action_list)):\n",
    "        print('The allocation chose at time {} is {}'.format(max_time_list[i], max_action_list[i]))\n",
    "\n",
    "    return total_reward_list, Q_table, testing_reward_list\n",
    "\n",
    "training_reward_list, Q_table, testing_reward_list = q_learning(env5, 500000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Analysis"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Random Sampling"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "random_sampling(env1, 100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## QLearning"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "training_reward_list, Q_table, testing_reward_list = q_learning(env1, 5000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(training_reward_list)\n",
    "plt.title(\"QLearning: Training\")\n",
    "plt.xlabel(\"Episode #\")\n",
    "plt.ylabel(\"Total_reward\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(testing_reward_list)\n",
    "plt.title(\"QLearning: Testing\")\n",
    "plt.xlabel(\"Episode #\")\n",
    "plt.ylabel(\"Total_reward\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(Q_table)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}