{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import JSSP\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.style\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "#import plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Environment Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_env(instance_path):\n",
    "    env_name = \"JSSP-v0\"\n",
    "    env = gym.make(env_name, instance_path = instance_path)\n",
    "    print(\"Environment Created for: \", instance_path)\n",
    "    print(\"Observation space: \\n\", env.observation_space)\n",
    "    print(\"Action space: \\n\", env.action_space)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env1 = create_env(\"instance1.txt\")\n",
    "env3 = create_env(\"instance3.txt\")\n",
    "env4 = create_env(\"instance4.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. Random Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def random_sampling(env, episodes):\n",
    "    env.reset()\n",
    "    max_score = -100000\n",
    "    max_episode = -1\n",
    "    max_action_list = []\n",
    "    max_time_list = []\n",
    "    for episode in range(1, episodes+1):\n",
    "        env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "        action_list = []\n",
    "        time_list = []\n",
    "        while not done:\n",
    "            #env.render()\n",
    "            action = env.action_space.sample()\n",
    "            if action != env.action_space.n -1:\n",
    "                action_list.append(env.legal_allocation_list[action])\n",
    "                time_list.append(env.time)\n",
    "                print('Episode:{} Allocation:{} Time:{}'.format(episode, env.legal_allocation_list[action], env.time))\n",
    "            n_state, reward, done, info = env.step(action)\n",
    "            score+=reward\n",
    "        print('Episode:{} Total_reward:{}'.format(episode, score))\n",
    "        if score >= max_score:\n",
    "            max_score = score\n",
    "            max_episode = episode\n",
    "            max_action_list = action_list\n",
    "            max_time_list = time_list\n",
    "    print('From {}th Episode best policy has reward {}'.format(max_episode, max_score))\n",
    "    for i in range(len(max_action_list)):\n",
    "        print('The allocation chose at time {} is {}'.format(max_time_list[i], max_action_list[i]))\n",
    "\n",
    "random_sampling(env4, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# def createEpsilonGreedyPolicy(Q, epsilon, num_actions):\n",
    "# \t\"\"\"\n",
    "# \tCreates an epsilon-greedy policy based\n",
    "# \ton a given Q-function and epsilon.\n",
    "#\n",
    "# \tReturns a function that takes the state\n",
    "# \tas an input and returns the probabilities\n",
    "# \tfor each action in the form of a numpy array\n",
    "# \tof length of the action space(set of possible actions).\n",
    "# \t\"\"\"\n",
    "def policy(state, Q, epsilon, num_actions):\n",
    "\n",
    "    if state in Q:\n",
    "        best_action = np.argmax(Q[state])\n",
    "        Action_probabilities = np.ones(num_actions, dtype = float) * epsilon / num_actions\n",
    "        Action_probabilities[best_action] += (1.0 - epsilon)\n",
    "        return Action_probabilities\n",
    "\n",
    "    Action_probabilities = np.ones(num_actions, dtype = float) / num_actions\n",
    "    return Action_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def update(Q, state, next_state, action, reward, eta, gamma):\n",
    "\n",
    "    if next_state not in Q:\n",
    "        Q_next_state_max = 0\n",
    "    else:\n",
    "        Q_next_state_max = max(Q[next_state])\n",
    "\n",
    "    Q[state][action] = Q[state][action] + eta * (reward + gamma * Q_next_state_max - Q[state][action])\n",
    "\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def q_learning(env, epis):\n",
    "\n",
    "    max_score = -100000\n",
    "    max_episode = -1\n",
    "    max_action_list = []\n",
    "    max_time_list = []\n",
    "\n",
    "    # 1. Load Environment and Q-table structure\n",
    "    Q_table = {}\n",
    "    # 2. Parameters of Q-learning\n",
    "    eta = .628\n",
    "    gamma = .9\n",
    "    epsilon = .1\n",
    "    decay_rate = .0001\n",
    "    total_reward_list = [] # rewards per episode calculate\n",
    "    # 3. Q-learning Algorithm\n",
    "    for episode in range(epis):\n",
    "        # Reset environment\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        action_list = []\n",
    "        time_list = []\n",
    "\n",
    "        # The Q-Table learning algorithm\n",
    "        while not done:\n",
    "            if state not in Q_table:\n",
    "                Q_table[state] = np.zeros(env.action_space.n)\n",
    "            action_probabilities = policy(state, Q_table, epsilon, env.action_space.n)\n",
    "            action = np.random.choice(np.arange(len(action_probabilities)), p = action_probabilities)\n",
    "            if action != env.action_space.n -1:\n",
    "                action_list.append(env.legal_allocation_list[action])\n",
    "                time_list.append(env.time)\n",
    "                #print('Episode:{} Allocation:{} Time:{}'.format(episode, env.legal_allocation_list[action], env.time))\n",
    "            #Get new state & reward from environment\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            #Update Q-Table with new knowledge\n",
    "            Q_table = update(Q_table, state, next_state, action, reward, eta, gamma)\n",
    "            if eta > .01:\n",
    "                eta -= decay_rate\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "        total_reward_list.append(total_reward)\n",
    "        if total_reward >= max_score:\n",
    "            max_score = total_reward\n",
    "            max_episode = episode\n",
    "            max_action_list = action_list\n",
    "            max_time_list = time_list\n",
    "        print(\"Episode: \" + str(episode) + \" has reward \" + str(total_reward))\n",
    "\n",
    "    print('From {}th Episode best policy has reward {}'.format(max_episode, max_score))\n",
    "    for i in range(len(max_action_list)):\n",
    "        print('The allocation chose at time {} is {}'.format(max_time_list[i], max_action_list[i]))\n",
    "\n",
    "    return total_reward_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "total_reward_list = q_learning(env3, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(total_reward_list)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(total_reward_list)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "random_sampling(env3, 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Q Learning from web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def createEpsilonGreedyPolicy(Q, epsilon, num_actions, env):\n",
    "\t\"\"\"\n",
    "\tCreates an epsilon-greedy policy based\n",
    "\ton a given Q-function and epsilon.\n",
    "\n",
    "\tReturns a function that takes the state\n",
    "\tas an input and returns the probabilities\n",
    "\tfor each action in the form of a numpy array\n",
    "\tof length of the action space(set of possible actions).\n",
    "\t\"\"\"\n",
    "\tdef policyFunction(state):\n",
    "\n",
    "\t\tAction_probabilities = np.ones(num_actions,\n",
    "\t\t\t\tdtype = float) * epsilon / num_actions\n",
    "\n",
    "\t\tbest_action = np.argmax(Q[state])\n",
    "\t\tAction_probabilities[best_action] += (1.0 - epsilon)\n",
    "\t\treturn Action_probabilities\n",
    "\n",
    "\treturn policyFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def qLearning(env, num_episodes, discount_factor = 1.0,\n",
    "\t\t\t\t\t\t\talpha = 0.6, epsilon = 0.1):\n",
    "\t\"\"\"\n",
    "\tQ-Learning algorithm: Off-policy TD control.\n",
    "\tFinds the optimal greedy policy while improving\n",
    "\tfollowing an epsilon-greedy policy\"\"\"\n",
    "\n",
    "\t# Action value function\n",
    "\t# A nested dictionary that maps\n",
    "\t# state -> (action -> action-value).\n",
    "\tQ = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "\t# Keeps track of useful statistics\n",
    "\tstats = {\n",
    "            \"episode_lengths\" : np.zeros(num_episodes),\n",
    "\t\t    \"episode_rewards\" : np.zeros(num_episodes)}\n",
    "\n",
    "\t# Create an epsilon greedy policy function\n",
    "\t# appropriately for environment action space\n",
    "\tpolicy = createEpsilonGreedyPolicy(Q, epsilon, env.action_space.n, env)\n",
    "\n",
    "\t# For every episode\n",
    "\tfor ith_episode in range(num_episodes):\n",
    "\n",
    "\t\t# Reset the environment and pick the first action\n",
    "\t\tstate = env.reset()\n",
    "\n",
    "\t\tfor t in itertools.count():\n",
    "\n",
    "\t\t\t# get probabilities of all actions from current state\n",
    "\t\t\taction_probabilities = policy(state)\n",
    "\n",
    "\t\t\t# choose action according to\n",
    "\t\t\t# the probability distribution\n",
    "\t\t\taction = np.random.choice(np.arange(\n",
    "\t\t\t\t\tlen(action_probabilities)),\n",
    "\t\t\t\t\tp = action_probabilities)\n",
    "\n",
    "\t\t\t# take action and get reward, transit to next state\n",
    "\t\t\tnext_state, reward, done, _ = env.step(action)\n",
    "\n",
    "\t\t\t# Update statistics\n",
    "\t\t\tstats[\"episode_rewards\"][ith_episode] += reward\n",
    "\t\t\tstats[\"episode_lengths\"][ith_episode] = t\n",
    "\n",
    "\t\t\t# TD Update\n",
    "\t\t\tbest_next_action = np.argmax(Q[next_state])\n",
    "\t\t\ttd_target = reward + discount_factor * Q[next_state][best_next_action]\n",
    "\t\t\ttd_delta = td_target - Q[state][action]\n",
    "\t\t\tQ[state][action] += alpha * td_delta\n",
    "\n",
    "\t\t\t# done is True if episode terminated\n",
    "\t\t\tif done:\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\t\tstate = next_state\n",
    "\n",
    "\treturn Q, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Q, stats = qLearning(env1, 1000)\n",
    "plt.plot(stats[\"episode_lengths\"])\n",
    "plt.plot(stats[\"episode_rewards\"])\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Q Learning from web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def createEpsilonGreedyPolicy(Q, epsilon, num_actions, env):\n",
    "\t\"\"\"\n",
    "\tCreates an epsilon-greedy policy based\n",
    "\ton a given Q-function and epsilon.\n",
    "\n",
    "\tReturns a function that takes the state\n",
    "\tas an input and returns the probabilities\n",
    "\tfor each action in the form of a numpy array\n",
    "\tof length of the action space(set of possible actions).\n",
    "\t\"\"\"\n",
    "\tdef policyFunction(state):\n",
    "\n",
    "\t\tAction_probabilities = np.ones(num_actions,\n",
    "\t\t\t\tdtype = float) * epsilon / num_actions\n",
    "\n",
    "\t\tbest_action = np.argmax(Q[state])\n",
    "\t\tAction_probabilities[best_action] += (1.0 - epsilon)\n",
    "\t\treturn Action_probabilities\n",
    "\n",
    "\treturn policyFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def qLearning(env, num_episodes, discount_factor = 1.0,\n",
    "\t\t\t\t\t\t\talpha = 0.6, epsilon = 0.1):\n",
    "\t\"\"\"\n",
    "\tQ-Learning algorithm: Off-policy TD control.\n",
    "\tFinds the optimal greedy policy while improving\n",
    "\tfollowing an epsilon-greedy policy\"\"\"\n",
    "\n",
    "\t# Action value function\n",
    "\t# A nested dictionary that maps\n",
    "\t# state -> (action -> action-value).\n",
    "\tQ = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "\t# Keeps track of useful statistics\n",
    "\tstats = {\n",
    "            \"episode_lengths\" : np.zeros(num_episodes),\n",
    "\t\t    \"episode_rewards\" : np.zeros(num_episodes)}\n",
    "\n",
    "\t# Create an epsilon greedy policy function\n",
    "\t# appropriately for environment action space\n",
    "\tpolicy = createEpsilonGreedyPolicy(Q, epsilon, env.action_space.n, env)\n",
    "\n",
    "\t# For every episode\n",
    "\tfor ith_episode in range(num_episodes):\n",
    "\n",
    "\t\t# Reset the environment and pick the first action\n",
    "\t\tstate = env.reset()\n",
    "\n",
    "\t\tfor t in itertools.count():\n",
    "\n",
    "\t\t\t# get probabilities of all actions from current state\n",
    "\t\t\taction_probabilities = policy(state)\n",
    "\n",
    "\t\t\t# choose action according to\n",
    "\t\t\t# the probability distribution\n",
    "\t\t\taction = np.random.choice(np.arange(\n",
    "\t\t\t\t\tlen(action_probabilities)),\n",
    "\t\t\t\t\tp = action_probabilities)\n",
    "\n",
    "\t\t\t# take action and get reward, transit to next state\n",
    "\t\t\tnext_state, reward, done, _ = env.step(action)\n",
    "\n",
    "\t\t\t# Update statistics\n",
    "\t\t\tstats[\"episode_rewards\"][ith_episode] += reward\n",
    "\t\t\tstats[\"episode_lengths\"][ith_episode] = t\n",
    "\n",
    "\t\t\t# TD Update\n",
    "\t\t\tbest_next_action = np.argmax(Q[next_state])\n",
    "\t\t\ttd_target = reward + discount_factor * Q[next_state][best_next_action]\n",
    "\t\t\ttd_delta = td_target - Q[state][action]\n",
    "\t\t\tQ[state][action] += alpha * td_delta\n",
    "\n",
    "\t\t\t# done is True if episode terminated\n",
    "\t\t\tif done:\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\t\tstate = next_state\n",
    "\n",
    "\treturn Q, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def createEpsilonGreedyPolicy(Q, epsilon, num_actions, env):\n",
    "\t\"\"\"\n",
    "\tCreates an epsilon-greedy policy based\n",
    "\ton a given Q-function and epsilon.\n",
    "\n",
    "\tReturns a function that takes the state\n",
    "\tas an input and returns the probabilities\n",
    "\tfor each action in the form of a numpy array\n",
    "\tof length of the action space(set of possible actions).\n",
    "\t\"\"\"\n",
    "\tdef policyFunction(state):\n",
    "\n",
    "\t\tAction_probabilities = np.ones(num_actions,\n",
    "\t\t\t\tdtype = float) * epsilon / num_actions\n",
    "\n",
    "\t\tbest_action = np.argmax(Q[state])\n",
    "\t\tAction_probabilities[best_action] += (1.0 - epsilon)\n",
    "\t\treturn Action_probabilities\n",
    "\n",
    "\treturn policyFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def qLearning(env, num_episodes, discount_factor = 1.0,\n",
    "\t\t\t\t\t\t\talpha = 0.6, epsilon = 0.1):\n",
    "\t\"\"\"\n",
    "\tQ-Learning algorithm: Off-policy TD control.\n",
    "\tFinds the optimal greedy policy while improving\n",
    "\tfollowing an epsilon-greedy policy\"\"\"\n",
    "\n",
    "\t# Action value function\n",
    "\t# A nested dictionary that maps\n",
    "\t# state -> (action -> action-value).\n",
    "\tQ = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "\t# Keeps track of useful statistics\n",
    "\tstats = {\n",
    "            \"episode_lengths\" : np.zeros(num_episodes),\n",
    "\t\t    \"episode_rewards\" : np.zeros(num_episodes)}\n",
    "\n",
    "\t# Create an epsilon greedy policy function\n",
    "\t# appropriately for environment action space\n",
    "\tpolicy = createEpsilonGreedyPolicy(Q, epsilon, env.action_space.n, env)\n",
    "\n",
    "\t# For every episode\n",
    "\tfor ith_episode in range(num_episodes):\n",
    "\n",
    "\t\t# Reset the environment and pick the first action\n",
    "\t\tstate = env.reset()\n",
    "\n",
    "\t\tfor t in itertools.count():\n",
    "\n",
    "\t\t\t# get probabilities of all actions from current state\n",
    "\t\t\taction_probabilities = policy(state)\n",
    "\n",
    "\t\t\t# choose action according to\n",
    "\t\t\t# the probability distribution\n",
    "\t\t\taction = np.random.choice(np.arange(\n",
    "\t\t\t\t\tlen(action_probabilities)),\n",
    "\t\t\t\t\tp = action_probabilities)\n",
    "\n",
    "\t\t\t# take action and get reward, transit to next state\n",
    "\t\t\tnext_state, reward, done, _ = env.step(action)\n",
    "\n",
    "\t\t\t# Update statistics\n",
    "\t\t\tstats[\"episode_rewards\"][ith_episode] += reward\n",
    "\t\t\tstats[\"episode_lengths\"][ith_episode] = t\n",
    "\n",
    "\t\t\t# TD Update\n",
    "\t\t\tbest_next_action = np.argmax(Q[next_state])\n",
    "\t\t\ttd_target = reward + discount_factor * Q[next_state][best_next_action]\n",
    "\t\t\ttd_delta = td_target - Q[state][action]\n",
    "\t\t\tQ[state][action] += alpha * td_delta\n",
    "\n",
    "\t\t\t# done is True if episode terminated\n",
    "\t\t\tif done:\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\t\tstate = next_state\n",
    "\n",
    "\treturn Q, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Q, stats = qLearning(env1, 1000)\n",
    "plt.plot(stats[\"episode_lengths\"])\n",
    "plt.plot(stats[\"episode_rewards\"])\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Q Learning from web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def createEpsilonGreedyPolicy(Q, epsilon, num_actions, env):\n",
    "\t\"\"\"\n",
    "\tCreates an epsilon-greedy policy based\n",
    "\ton a given Q-function and epsilon.\n",
    "\n",
    "\tReturns a function that takes the state\n",
    "\tas an input and returns the probabilities\n",
    "\tfor each action in the form of a numpy array\n",
    "\tof length of the action space(set of possible actions).\n",
    "\t\"\"\"\n",
    "\tdef policyFunction(state):\n",
    "\n",
    "\t\tAction_probabilities = np.ones(num_actions,\n",
    "\t\t\t\tdtype = float) * epsilon / num_actions\n",
    "\n",
    "\t\tbest_action = np.argmax(Q[state])\n",
    "\t\tAction_probabilities[best_action] += (1.0 - epsilon)\n",
    "\t\treturn Action_probabilities\n",
    "\n",
    "\treturn policyFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def qLearning(env, num_episodes, discount_factor = 1.0,\n",
    "\t\t\t\t\t\t\talpha = 0.6, epsilon = 0.1):\n",
    "\t\"\"\"\n",
    "\tQ-Learning algorithm: Off-policy TD control.\n",
    "\tFinds the optimal greedy policy while improving\n",
    "\tfollowing an epsilon-greedy policy\"\"\"\n",
    "\n",
    "\t# Action value function\n",
    "\t# A nested dictionary that maps\n",
    "\t# state -> (action -> action-value).\n",
    "\tQ = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "\t# Keeps track of useful statistics\n",
    "\tstats = {\n",
    "            \"episode_lengths\" : np.zeros(num_episodes),\n",
    "\t\t    \"episode_rewards\" : np.zeros(num_episodes)}\n",
    "\n",
    "\t# Create an epsilon greedy policy function\n",
    "\t# appropriately for environment action space\n",
    "\tpolicy = createEpsilonGreedyPolicy(Q, epsilon, env.action_space.n, env)\n",
    "\n",
    "\t# For every episode\n",
    "\tfor ith_episode in range(num_episodes):\n",
    "\n",
    "\t\t# Reset the environment and pick the first action\n",
    "\t\tstate = env.reset()\n",
    "\n",
    "\t\tfor t in itertools.count():\n",
    "\n",
    "\t\t\t# get probabilities of all actions from current state\n",
    "\t\t\taction_probabilities = policy(state)\n",
    "\n",
    "\t\t\t# choose action according to\n",
    "\t\t\t# the probability distribution\n",
    "\t\t\taction = np.random.choice(np.arange(\n",
    "\t\t\t\t\tlen(action_probabilities)),\n",
    "\t\t\t\t\tp = action_probabilities)\n",
    "\n",
    "\t\t\t# take action and get reward, transit to next state\n",
    "\t\t\tnext_state, reward, done, _ = env.step(action)\n",
    "\n",
    "\t\t\t# Update statistics\n",
    "\t\t\tstats[\"episode_rewards\"][ith_episode] += reward\n",
    "\t\t\tstats[\"episode_lengths\"][ith_episode] = t\n",
    "\n",
    "\t\t\t# TD Update\n",
    "\t\t\tbest_next_action = np.argmax(Q[next_state])\n",
    "\t\t\ttd_target = reward + discount_factor * Q[next_state][best_next_action]\n",
    "\t\t\ttd_delta = td_target - Q[state][action]\n",
    "\t\t\tQ[state][action] += alpha * td_delta\n",
    "\n",
    "\t\t\t# done is True if episode terminated\n",
    "\t\t\tif done:\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\t\tstate = next_state\n",
    "\n",
    "\treturn Q, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Q, stats = qLearning(env1, 1000)\n",
    "plt.plot(stats[\"episode_lengths\"])\n",
    "plt.plot(stats[\"episode_rewards\"])\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Q Learning from web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def createEpsilonGreedyPolicy(Q, epsilon, num_actions, env):\n",
    "\t\"\"\"\n",
    "\tCreates an epsilon-greedy policy based\n",
    "\ton a given Q-function and epsilon.\n",
    "\n",
    "\tReturns a function that takes the state\n",
    "\tas an input and returns the probabilities\n",
    "\tfor each action in the form of a numpy array\n",
    "\tof length of the action space(set of possible actions).\n",
    "\t\"\"\"\n",
    "\tdef policyFunction(state):\n",
    "\n",
    "\t\tAction_probabilities = np.ones(num_actions,\n",
    "\t\t\t\tdtype = float) * epsilon / num_actions\n",
    "\n",
    "\t\tbest_action = np.argmax(Q[state])\n",
    "\t\tAction_probabilities[best_action] += (1.0 - epsilon)\n",
    "\t\treturn Action_probabilities\n",
    "\n",
    "\treturn policyFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def qLearning(env, num_episodes, discount_factor = 1.0,\n",
    "\t\t\t\t\t\t\talpha = 0.6, epsilon = 0.1):\n",
    "\t\"\"\"\n",
    "\tQ-Learning algorithm: Off-policy TD control.\n",
    "\tFinds the optimal greedy policy while improving\n",
    "\tfollowing an epsilon-greedy policy\"\"\"\n",
    "\n",
    "\t# Action value function\n",
    "\t# A nested dictionary that maps\n",
    "\t# state -> (action -> action-value).\n",
    "\tQ = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "\t# Keeps track of useful statistics\n",
    "\tstats = {\n",
    "            \"episode_lengths\" : np.zeros(num_episodes),\n",
    "\t\t    \"episode_rewards\" : np.zeros(num_episodes)}\n",
    "\n",
    "\t# Create an epsilon greedy policy function\n",
    "\t# appropriately for environment action space\n",
    "\tpolicy = createEpsilonGreedyPolicy(Q, epsilon, env.action_space.n, env)\n",
    "\n",
    "\t# For every episode\n",
    "\tfor ith_episode in range(num_episodes):\n",
    "\n",
    "\t\t# Reset the environment and pick the first action\n",
    "\t\tstate = env.reset()\n",
    "\n",
    "\t\tfor t in itertools.count():\n",
    "\n",
    "\t\t\t# get probabilities of all actions from current state\n",
    "\t\t\taction_probabilities = policy(state)\n",
    "\n",
    "\t\t\t# choose action according to\n",
    "\t\t\t# the probability distribution\n",
    "\t\t\taction = np.random.choice(np.arange(\n",
    "\t\t\t\t\tlen(action_probabilities)),\n",
    "\t\t\t\t\tp = action_probabilities)\n",
    "\n",
    "\t\t\t# take action and get reward, transit to next state\n",
    "\t\t\tnext_state, reward, done, _ = env.step(action)\n",
    "\n",
    "\t\t\t# Update statistics\n",
    "\t\t\tstats[\"episode_rewards\"][ith_episode] += reward\n",
    "\t\t\tstats[\"episode_lengths\"][ith_episode] = t\n",
    "\n",
    "\t\t\t# TD Update\n",
    "\t\t\tbest_next_action = np.argmax(Q[next_state])\n",
    "\t\t\ttd_target = reward + discount_factor * Q[next_state][best_next_action]\n",
    "\t\t\ttd_delta = td_target - Q[state][action]\n",
    "\t\t\tQ[state][action] += alpha * td_delta\n",
    "\n",
    "\t\t\t# done is True if episode terminated\n",
    "\t\t\tif done:\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\t\tstate = next_state\n",
    "\n",
    "\treturn Q, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def createEpsilonGreedyPolicy(Q, epsilon, num_actions, env):\n",
    "\t\"\"\"\n",
    "\tCreates an epsilon-greedy policy based\n",
    "\ton a given Q-function and epsilon.\n",
    "\n",
    "\tReturns a function that takes the state\n",
    "\tas an input and returns the probabilities\n",
    "\tfor each action in the form of a numpy array\n",
    "\tof length of the action space(set of possible actions).\n",
    "\t\"\"\"\n",
    "\tdef policyFunction(state):\n",
    "\n",
    "\t\tAction_probabilities = np.ones(num_actions,\n",
    "\t\t\t\tdtype = float) * epsilon / num_actions\n",
    "\n",
    "\t\tbest_action = np.argmax(Q[state])\n",
    "\t\tAction_probabilities[best_action] += (1.0 - epsilon)\n",
    "\t\treturn Action_probabilities\n",
    "\n",
    "\treturn policyFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def qLearning(env, num_episodes, discount_factor = 1.0,\n",
    "\t\t\t\t\t\t\talpha = 0.6, epsilon = 0.1):\n",
    "\t\"\"\"\n",
    "\tQ-Learning algorithm: Off-policy TD control.\n",
    "\tFinds the optimal greedy policy while improving\n",
    "\tfollowing an epsilon-greedy policy\"\"\"\n",
    "\n",
    "\t# Action value function\n",
    "\t# A nested dictionary that maps\n",
    "\t# state -> (action -> action-value).\n",
    "\tQ = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "\t# Keeps track of useful statistics\n",
    "\tstats = {\n",
    "            \"episode_lengths\" : np.zeros(num_episodes),\n",
    "\t\t    \"episode_rewards\" : np.zeros(num_episodes)}\n",
    "\n",
    "\t# Create an epsilon greedy policy function\n",
    "\t# appropriately for environment action space\n",
    "\tpolicy = createEpsilonGreedyPolicy(Q, epsilon, env.action_space.n, env)\n",
    "\n",
    "\t# For every episode\n",
    "\tfor ith_episode in range(num_episodes):\n",
    "\n",
    "\t\t# Reset the environment and pick the first action\n",
    "\t\tstate = env.reset()\n",
    "\n",
    "\t\tfor t in itertools.count():\n",
    "\n",
    "\t\t\t# get probabilities of all actions from current state\n",
    "\t\t\taction_probabilities = policy(state)\n",
    "\n",
    "\t\t\t# choose action according to\n",
    "\t\t\t# the probability distribution\n",
    "\t\t\taction = np.random.choice(np.arange(\n",
    "\t\t\t\t\tlen(action_probabilities)),\n",
    "\t\t\t\t\tp = action_probabilities)\n",
    "\n",
    "\t\t\t# take action and get reward, transit to next state\n",
    "\t\t\tnext_state, reward, done, _ = env.step(action)\n",
    "\n",
    "\t\t\t# Update statistics\n",
    "\t\t\tstats[\"episode_rewards\"][ith_episode] += reward\n",
    "\t\t\tstats[\"episode_lengths\"][ith_episode] = t\n",
    "\n",
    "\t\t\t# TD Update\n",
    "\t\t\tbest_next_action = np.argmax(Q[next_state])\n",
    "\t\t\ttd_target = reward + discount_factor * Q[next_state][best_next_action]\n",
    "\t\t\ttd_delta = td_target - Q[state][action]\n",
    "\t\t\tQ[state][action] += alpha * td_delta\n",
    "\n",
    "\t\t\t# done is True if episode terminated\n",
    "\t\t\tif done:\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\t\tstate = next_state\n",
    "\n",
    "\treturn Q, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Q, stats = qLearning(env1, 1000)\n",
    "plt.plot(stats[\"episode_lengths\"])\n",
    "plt.plot(stats[\"episode_rewards\"])\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Q Learning from web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def createEpsilonGreedyPolicy(Q, epsilon, num_actions, env):\n",
    "\t\"\"\"\n",
    "\tCreates an epsilon-greedy policy based\n",
    "\ton a given Q-function and epsilon.\n",
    "\n",
    "\tReturns a function that takes the state\n",
    "\tas an input and returns the probabilities\n",
    "\tfor each action in the form of a numpy array\n",
    "\tof length of the action space(set of possible actions).\n",
    "\t\"\"\"\n",
    "\tdef policyFunction(state):\n",
    "\n",
    "\t\tAction_probabilities = np.ones(num_actions,\n",
    "\t\t\t\tdtype = float) * epsilon / num_actions\n",
    "\n",
    "\t\tbest_action = np.argmax(Q[state])\n",
    "\t\tAction_probabilities[best_action] += (1.0 - epsilon)\n",
    "\t\treturn Action_probabilities\n",
    "\n",
    "\treturn policyFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def qLearning(env, num_episodes, discount_factor = 1.0,\n",
    "\t\t\t\t\t\t\talpha = 0.6, epsilon = 0.1):\n",
    "\t\"\"\"\n",
    "\tQ-Learning algorithm: Off-policy TD control.\n",
    "\tFinds the optimal greedy policy while improving\n",
    "\tfollowing an epsilon-greedy policy\"\"\"\n",
    "\n",
    "\t# Action value function\n",
    "\t# A nested dictionary that maps\n",
    "\t# state -> (action -> action-value).\n",
    "\tQ = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "\t# Keeps track of useful statistics\n",
    "\tstats = {\n",
    "            \"episode_lengths\" : np.zeros(num_episodes),\n",
    "\t\t    \"episode_rewards\" : np.zeros(num_episodes)}\n",
    "\n",
    "\t# Create an epsilon greedy policy function\n",
    "\t# appropriately for environment action space\n",
    "\tpolicy = createEpsilonGreedyPolicy(Q, epsilon, env.action_space.n, env)\n",
    "\n",
    "\t# For every episode\n",
    "\tfor ith_episode in range(num_episodes):\n",
    "\n",
    "\t\t# Reset the environment and pick the first action\n",
    "\t\tstate = env.reset()\n",
    "\n",
    "\t\tfor t in itertools.count():\n",
    "\n",
    "\t\t\t# get probabilities of all actions from current state\n",
    "\t\t\taction_probabilities = policy(state)\n",
    "\n",
    "\t\t\t# choose action according to\n",
    "\t\t\t# the probability distribution\n",
    "\t\t\taction = np.random.choice(np.arange(\n",
    "\t\t\t\t\tlen(action_probabilities)),\n",
    "\t\t\t\t\tp = action_probabilities)\n",
    "\n",
    "\t\t\t# take action and get reward, transit to next state\n",
    "\t\t\tnext_state, reward, done, _ = env.step(action)\n",
    "\n",
    "\t\t\t# Update statistics\n",
    "\t\t\tstats[\"episode_rewards\"][ith_episode] += reward\n",
    "\t\t\tstats[\"episode_lengths\"][ith_episode] = t\n",
    "\n",
    "\t\t\t# TD Update\n",
    "\t\t\tbest_next_action = np.argmax(Q[next_state])\n",
    "\t\t\ttd_target = reward + discount_factor * Q[next_state][best_next_action]\n",
    "\t\t\ttd_delta = td_target - Q[state][action]\n",
    "\t\t\tQ[state][action] += alpha * td_delta\n",
    "\n",
    "\t\t\t# done is True if episode terminated\n",
    "\t\t\tif done:\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\t\tstate = next_state\n",
    "\n",
    "\treturn Q, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Q, stats = qLearning(env1, 1000)\n",
    "plt.plot(stats[\"episode_lengths\"])\n",
    "plt.plot(stats[\"episode_rewards\"])\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
