{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import JSSP\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import matplotlib.style\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "from collections import defaultdict\n",
    "#import plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Environment Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_env(instance_path, initial_state_data=None):\n",
    "    env_name = \"JSSP-v0\"\n",
    "    env = gym.make(env_name, instance_path = instance_path, initial_state_data=initial_state_data)\n",
    "    print(\"Environment Created for: \", instance_path)\n",
    "    print(\"Observation space: \\n\", env.observation_space)\n",
    "    print(\"Action space: \\n\", env.action_space)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env1 = create_env(\"instance1.txt\") #-53\n",
    "env3 = create_env(\"instance3.txt\") #-981\n",
    "env4 = create_env(\"instance4.txt\")\n",
    "env5 = create_env(\"instance5.txt\")\n",
    "env5_1 = create_env(\"instance5_1.txt\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "render test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env5.reset()\n",
    "for i in range(20):\n",
    "    env5.render()\n",
    "    time.sleep(1)\n",
    "    action = env5.action_space.sample()\n",
    "    n_state, reward, done, info = env5.step(action)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Baselines"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Random Sampling"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def random_sampling(env, episodes):\n",
    "    env.reset()\n",
    "    max_score = -100000\n",
    "    max_episode = -1\n",
    "    max_action_list = []\n",
    "    max_time_list = []\n",
    "    for episode in range(1, episodes+1):\n",
    "        env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "        action_list = []\n",
    "        time_list = []\n",
    "        while not done:\n",
    "            env.render(future_data_allowed=False)\n",
    "            action = env.action_space.sample()\n",
    "            if action != env.action_space.n -1:\n",
    "                action_list.append(env.legal_allocation_list[action])\n",
    "                time_list.append(env.time)\n",
    "                # print('Episode:{} Allocation:{} Time:{}'.format(episode, env.legal_allocation_list[action], env.time))\n",
    "            n_state, reward, done, info = env.step(action)\n",
    "            score+=reward\n",
    "        # print('Episode:{} Total_reward:{}'.format(episode, score))\n",
    "        if score >= max_score:\n",
    "            max_score = score\n",
    "            max_episode = episode\n",
    "            max_action_list = action_list\n",
    "            max_time_list = time_list\n",
    "    print('From {}th Episode best policy has reward {}'.format(max_episode, max_score))\n",
    "    for i in range(len(max_action_list)):\n",
    "        print('The allocation chose at time {} is {}'.format(max_time_list[i], max_action_list[i]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Q-Learning"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_machine_allocation(state):\n",
    "    \"\"\"\n",
    "    function that returns the machine allocation information at current state\n",
    "    :param state: current state which is a tuple of order 2 * number of jobs\n",
    "    :return: a list of order of number of jobs containing the machine allocation information at current state\n",
    "    \"\"\"\n",
    "    state = list(state)\n",
    "    # machine_allocation = state[:len(state) // 3]\n",
    "    machine_allocation = state[:len(state) // 2]\n",
    "    return machine_allocation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_operation_status(state):\n",
    "    \"\"\"\n",
    "    function that returns the operation status information at current state\n",
    "    :param state: current state which is a tuple of order 2 * number of jobs\n",
    "    :return: a list of order of number of jobs containing the operation status information at current state\n",
    "    \"\"\"\n",
    "    state = list(state)\n",
    "    # operation_status = state[len(state) // 3:2*len(state) // 3]\n",
    "    operation_status = state[len(state) // 2:]\n",
    "    return operation_status"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_minimum_left_time(job, operation_index, job_operation_map):\n",
    "    \"\"\"\n",
    "    function that returns the minimum time for current job to finish all operations\n",
    "    It will search through all leftover operations and sum up the minimum\n",
    "    :param job: job index\n",
    "    :param operation_index: which operation is this job doing or waiting to do\n",
    "    :param job_operation_map: the dictionary that covers information about the time each machine will take to finish each operation of each job\n",
    "    :return: minimum time for this job to finish all leftover operations\n",
    "    \"\"\"\n",
    "    map = job_operation_map[job] # part of the job_operation_map that is related to current job\n",
    "    minimum_time = 0\n",
    "    operation_no = sorted(map)[-1]\n",
    "    if operation_index == operation_no:\n",
    "        return 1\n",
    "    for operation in range(int(operation_index) + 1, operation_no + 1):\n",
    "        minimum_time += np.min([time for time in map[operation] if time != -1])\n",
    "    return minimum_time"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def heuristic_makespan(state, env):\n",
    "    \"\"\"\n",
    "    function that returns the minimum time for current state to finish all jobs\n",
    "    It will search through all jobs and return the maximum of minimum left time of all jobs\n",
    "    :param state: current state which is a tuple of order 2 * number of jobs\n",
    "    :param env: JSSP instance\n",
    "    :return: heuristic minimum time for current state to finish all jobs\n",
    "    \"\"\"\n",
    "    operations = get_operation_status(state)\n",
    "    minimum_makespan = 0\n",
    "    for job in range(env.job_total):\n",
    "        job_minimum_left_time = get_minimum_left_time(job, operations[job], env.job_operation_map)\n",
    "        # Choose the maximum leftover time\n",
    "        if job_minimum_left_time > minimum_makespan:\n",
    "            minimum_makespan = job_minimum_left_time\n",
    "    return minimum_makespan"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def update(Q, state, next_state, action, reward, eta, gamma, env):\n",
    "    \"\"\"\n",
    "    Q-table updating step\n",
    "    :param Q: Q table which is a dictionary with states as first class keys and actions as second class keys\n",
    "    :param state: current state which is a tuple of order 2 * number of jobs\n",
    "    :param next_state: next state which is a tuple of order 2 * number of jobs\n",
    "    :param action: action between current state and next state\n",
    "    :param reward: reward for the action\n",
    "    :param eta: learning rate\n",
    "    :param gamma: discounted factor\n",
    "    :param env: JSSP instance\n",
    "    :return: updated Q table\n",
    "    \"\"\"\n",
    "    machine_allocation = get_machine_allocation(next_state)\n",
    "    # If Q-table hasn't reached the next state before, using the heuristic function to approximate the maximum Q-value of the next state\n",
    "    if next_state not in Q:\n",
    "        Q_next_state_max = -heuristic_makespan(next_state, env)\n",
    "    else:\n",
    "        # If all jobs are waiting or done with all operations in the next state, we don't take the Q-value of wait action into consideration\n",
    "        if jobs_all_waiting(machine_allocation):\n",
    "            # Make sure the next state is not the done state\n",
    "            if len(Q[next_state]) > 1:\n",
    "                Q_next_state_max = max(Q[next_state][:-1])\n",
    "            else:\n",
    "                Q_next_state_max = 0\n",
    "        else:\n",
    "            Q_next_state_max = max(Q[next_state])\n",
    "\n",
    "    # Updating step\n",
    "    Q[state][action] = Q[state][action] + eta * (reward + gamma * Q_next_state_max - Q[state][action])\n",
    "\n",
    "    return Q"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def jobs_all_waiting(machine_allocation):\n",
    "    \"\"\"\n",
    "    function that checks if all jobs are waiting or done with all operations\n",
    "    :param machine_allocation: list of machine allocation data which using job as index\n",
    "    :return: boolean value of if all jobs are waiting or done with all operations\n",
    "    \"\"\"\n",
    "    machine_allocation = np.array(machine_allocation)\n",
    "    # if the job has machine allocation number -1, then it is waiting\n",
    "    # if the job has machine allocation number -2, then it is done with all operations\n",
    "    return np.all(np.logical_or(machine_allocation == -1, machine_allocation == -2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_smart_action(state, Q, epsilon, num_actions):\n",
    "    \"\"\"\n",
    "    function that choose action in a smart way\n",
    "    If all jobs are waiting or done with all operations, then we will not choose the last action(wait action) since it is definitely a bad move.\n",
    "    :param state: current state which is a tuple of order 2 * number of jobs\n",
    "    :param Q: Q table which is a dictionary with states as first class keys and actions as second class keys\n",
    "    :param epsilon: randomness factor, how random the selection is\n",
    "    :param num_actions: number of actions for the current state\n",
    "    :return: an index of which action will be chosen\n",
    "    \"\"\"\n",
    "    machine_allocation = get_machine_allocation(state)\n",
    "    # If the state hasn't been reached yet, choose the action randomly\n",
    "    if state not in Q:\n",
    "        action_probabilities = np.ones(num_actions, dtype = float) / num_actions\n",
    "        return action_probabilities\n",
    "    else:\n",
    "        # If in the state all jobs are waiting or done with all operation, choose action based on epsilon-greedy algorithm while making sure we won't choose the wait action at all\n",
    "        if jobs_all_waiting(machine_allocation):\n",
    "            best_action = np.argmax(Q[state][:-1])\n",
    "            action_probabilities = np.ones(num_actions, dtype = float) * epsilon / (num_actions - 1)\n",
    "            action_probabilities[best_action] += (1.0 - epsilon)\n",
    "            action_probabilities[-1] = 0\n",
    "        else:\n",
    "            best_action = np.argmax(Q[state])\n",
    "            action_probabilities = np.ones(num_actions, dtype = float) * epsilon / num_actions\n",
    "            action_probabilities[best_action] += (1.0 - epsilon)\n",
    "    action = np.random.choice(np.arange(len(action_probabilities)), p = action_probabilities)\n",
    "    return action"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def q_learning(env, epis, eta = .628, gamma = 1, epsilon = .1):\n",
    "    \"\"\"\n",
    "    q-learning algorithm that returns the best policy, and data for each episode\n",
    "    :param env: JSSP instance\n",
    "    :param epis: number of episodes\n",
    "    :param eta: learning rate, how much you accept the new value vs the old value\n",
    "    :param gamma: discount factor, balance immediate and future reward\n",
    "    :param epsilon: randomness factor, how random the selection is\n",
    "    :return: a list of total reward for each episode, the final Q_table, and a list of total reward every 100 episodes\n",
    "    \"\"\"\n",
    "    max_score = -100000\n",
    "    max_episode = -1\n",
    "    max_action_list = []\n",
    "    max_time_list = []\n",
    "\n",
    "    # Parameters of Q-learning\n",
    "    Q_table = {}\n",
    "    current_epsilon = .1\n",
    "    total_reward_list = [] # rewards per episode calculate\n",
    "    testing_reward_list = []\n",
    "\n",
    "    # Q-learning Algorithm\n",
    "    for episode in range(epis):\n",
    "        # Reset environment\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        action_list = []\n",
    "        time_list = []\n",
    "\n",
    "        # The Q-Table learning algorithm\n",
    "        while not done:\n",
    "            if state not in Q_table:\n",
    "                Q_table[state] = np.negative(np.full(env.action_space.n, heuristic_makespan(state, env), dtype = float))\n",
    "            action = get_smart_action(state, Q_table, current_epsilon, env.action_space.n)\n",
    "            # Only record the action in history if it is not a wait action\n",
    "            if action != env.action_space.n -1:\n",
    "                action_list.append(env.legal_allocation_list[action])\n",
    "                time_list.append(env.time)\n",
    "                #print('Episode:{} Allocation:{} Time:{}'.format(episode, env.legal_allocation_list[action], env.time))\n",
    "            # Get new state & reward from environment\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            # Update Q-Table with new knowledge\n",
    "            Q_table = update(Q_table, state, next_state, action, reward, eta, gamma, env)\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "        # Update the final done state\n",
    "        if state not in Q_table:\n",
    "            Q_table[state] = np.array([0])\n",
    "        # Record policy result and store it as policy testing result every 100 episodes\n",
    "        total_reward_list.append(total_reward)\n",
    "        if episode % 100 == 0:\n",
    "            testing_reward_list.append(total_reward)\n",
    "        # Record the best policy\n",
    "        if total_reward >= max_score:\n",
    "            max_score = total_reward\n",
    "            max_episode = episode\n",
    "            max_action_list = action_list\n",
    "            max_time_list = time_list\n",
    "        print(\"Episode: \" + str(episode) + \" has time \" + str(env.time))\n",
    "        # Set the epsilon to 0 every 1000 episodes\n",
    "        if episode % 1000 == 0 and episode % 2000 != 0:\n",
    "            current_epsilon = 0\n",
    "        if episode % 1000 == 0 and episode % 2000 == 0:\n",
    "            current_epsilon = epsilon\n",
    "\n",
    "    print('From {}th Episode best policy has reward {}'.format(max_episode + 1, max_score))\n",
    "    for i in range(len(max_action_list)):\n",
    "        print('The allocation chose at time {} is {}'.format(max_time_list[i], max_action_list[i]))\n",
    "\n",
    "    return total_reward_list, Q_table, testing_reward_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "second version"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_action(state, Q, epsilon):\n",
    "    \"\"\"\n",
    "    function that choose action in a smart way\n",
    "    If all jobs are waiting or done with all operations, then we will not choose the last action(wait action) since it is definitely a bad move.\n",
    "    :param state: current state which is a tuple of order 2 * number of jobs\n",
    "    :param Q: Q table which is a dictionary with states as first class keys and actions as second class keys\n",
    "    :param epsilon: randomness factor, how random the selection is\n",
    "    :param num_actions: number of actions for the current state\n",
    "    :return: an index of which action will be chosen\n",
    "    \"\"\"\n",
    "    num_actions = len(Q[state])\n",
    "    # If in the state all jobs are waiting or done with all operation, choose action based on epsilon-greedy algorithm while making sure we won't choose the wait action at all\n",
    "    best_action = np.argmax(Q[state])\n",
    "    action_probabilities = np.ones(num_actions, dtype = float) * epsilon / num_actions\n",
    "    action_probabilities[best_action] += (1.0 - epsilon)\n",
    "    action = np.random.choice(np.arange(len(action_probabilities)), p = action_probabilities)\n",
    "    return action"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def update(Q, state, next_state, action, reward, eta, gamma, env):\n",
    "    \"\"\"\n",
    "    Q-table updating step\n",
    "    :param Q: Q table which is a dictionary with states as first class keys and actions as second class keys\n",
    "    :param state: current state which is a tuple of order 2 * number of jobs\n",
    "    :param next_state: next state which is a tuple of order 2 * number of jobs\n",
    "    :param action: action between current state and next state\n",
    "    :param reward: reward for the action\n",
    "    :param eta: learning rate\n",
    "    :param gamma: discounted factor\n",
    "    :param env: JSSP instance\n",
    "    :return: updated Q table\n",
    "    \"\"\"\n",
    "    # If Q-table hasn't reached the next state before, using the heuristic function to approximate the maximum Q-value of the next state\n",
    "    if next_state not in Q:\n",
    "        Q_next_state_max = -heuristic_makespan(next_state, env)\n",
    "    else:\n",
    "        Q_next_state_max = max(Q[next_state])\n",
    "\n",
    "    # Updating step\n",
    "    Q[state][action] += eta * (reward + gamma * Q_next_state_max - Q[state][action])\n",
    "\n",
    "    return Q"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def q_learning(env, epis, eta = .628, gamma = 1, epsilon = .1):\n",
    "    \"\"\"\n",
    "    q-learning algorithm that returns the best policy, and data for each episode\n",
    "    :param env: JSSP instance\n",
    "    :param epis: number of episodes\n",
    "    :param eta: learning rate, how much you accept the new value vs the old value\n",
    "    :param gamma: discount factor, balance immediate and future reward\n",
    "    :param epsilon: randomness factor, how random the selection is\n",
    "    :return: a list of total reward for each episode, the final Q_table, and a list of total reward every 100 episodes\n",
    "    \"\"\"\n",
    "    max_score = -100000\n",
    "    max_episode = -1\n",
    "    max_action_list = []\n",
    "    max_time_list = []\n",
    "\n",
    "    # Parameters of Q-learning\n",
    "    Q_table = {}\n",
    "    current_epsilon = .1\n",
    "    total_reward_list = [] # rewards per episode calculate\n",
    "    testing_reward_list = []\n",
    "\n",
    "    # Q-learning Algorithm\n",
    "    for episode in range(epis):\n",
    "        # Reset environment\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        action_list = []\n",
    "        time_list = []\n",
    "\n",
    "        # The Q-Table learning algorithm\n",
    "        while not done:\n",
    "            if state not in Q_table:\n",
    "                if jobs_all_waiting(env.state[env.job_machine_allocation]):\n",
    "                    Q_table[state] = np.negative(np.full(env.action_space.n - 1, heuristic_makespan(state, env), dtype = float))\n",
    "                else:\n",
    "                    Q_table[state] = np.negative(np.full(env.action_space.n, heuristic_makespan(state, env), dtype = float))\n",
    "            action = get_action(state, Q_table, current_epsilon)\n",
    "            # Only record the action in history if it is not a wait action\n",
    "            if action != env.action_space.n -1:\n",
    "                action_list.append(env.legal_allocation_list[action])\n",
    "                time_list.append(env.time)\n",
    "                #print('Episode:{} Allocation:{} Time:{}'.format(episode, env.legal_allocation_list[action], env.time))\n",
    "            # Get new state & reward from environment\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            # Update Q-Table with new knowledge\n",
    "            Q_table = update(Q_table, state, next_state, action, reward, eta, gamma, env)\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "        # Update the final done state\n",
    "        if state not in Q_table:\n",
    "            Q_table[state] = np.array([0])\n",
    "        # Record policy result and store it as policy testing result every 100 episodes\n",
    "        total_reward_list.append(total_reward)\n",
    "        if episode % 100 == 0:\n",
    "            testing_reward_list.append(total_reward)\n",
    "        # Record the best policy\n",
    "        if total_reward >= max_score:\n",
    "            max_score = total_reward\n",
    "            max_episode = episode\n",
    "            max_action_list = action_list\n",
    "            max_time_list = time_list\n",
    "        print(\"Episode: \" + str(episode) + \" has time \" + str(env.time))\n",
    "        # Set the epsilon to 0 every 1000 episodes\n",
    "        if episode % 1000 == 0 and episode % 2000 != 0:\n",
    "            current_epsilon = 0\n",
    "        if episode % 1000 == 0 and episode % 2000 == 0:\n",
    "            current_epsilon = epsilon\n",
    "\n",
    "    print('From {}th Episode best policy has reward {}'.format(max_episode + 1, max_score))\n",
    "    for i in range(len(max_action_list)):\n",
    "        print('The allocation chose at time {} is {}'.format(max_time_list[i], max_action_list[i]))\n",
    "\n",
    "    return total_reward_list, Q_table, testing_reward_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "third version"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def q_learning(env, epochs, eta = .628, gamma = 1, epsilon = .1, max_epsilon_greedy_episodes = 1000, max_convergence_episodes = 50000, convergence_guarantee_episodes = 100):\n",
    "    \"\"\"\n",
    "    q-learning algorithm that returns the best policy, and data for each episode\n",
    "    :param env: JSSP instance\n",
    "    :param epis: number of episodes\n",
    "    :param eta: learning rate, how much you accept the new value vs the old value\n",
    "    :param gamma: discount factor, balance immediate and future reward\n",
    "    :param epsilon: randomness factor, how random the selection is\n",
    "    :return: a list of total reward for each episode, the final Q_table, and a list of total reward every 100 episodes\n",
    "    \"\"\"\n",
    "    max_score = -100000\n",
    "    max_episode = -1\n",
    "    max_action_list = []\n",
    "    max_time_list = []\n",
    "\n",
    "    # Parameters of Q-learning\n",
    "    Q_table = {}\n",
    "    total_reward_list = [] # rewards per episode calculate\n",
    "    testing_reward_list = []\n",
    "    episode_no = 1\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Q-learning Algorithm\n",
    "        current_epsilon = epsilon\n",
    "        for episode in range(max_epsilon_greedy_episodes):\n",
    "            # Reset environment\n",
    "            state = env.reset()\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            action_list = []\n",
    "            time_list = []\n",
    "\n",
    "            # The Q-Table learning algorithm\n",
    "            while not done:\n",
    "                if state not in Q_table:\n",
    "                    if jobs_all_waiting(env.state[env.job_machine_allocation]):\n",
    "                        Q_table[state] = np.negative(np.full(env.action_space.n - 1, heuristic_makespan(state, env), dtype = float))\n",
    "                    else:\n",
    "                        Q_table[state] = np.negative(np.full(env.action_space.n, heuristic_makespan(state, env), dtype = float))\n",
    "                action = get_action(state, Q_table, current_epsilon)\n",
    "                # Only record the action in history if it is not a wait action\n",
    "                if action != env.action_space.n -1:\n",
    "                    action_list.append(env.legal_allocation_list[action])\n",
    "                    time_list.append(env.time)\n",
    "                    #print('Episode:{} Allocation:{} Time:{}'.format(episode, env.legal_allocation_list[action], env.time))\n",
    "                # Get new state & reward from environment\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                # Update Q-Table with new knowledge\n",
    "                Q_table = update(Q_table, state, next_state, action, reward, eta, gamma, env)\n",
    "                total_reward += reward\n",
    "                state = next_state\n",
    "            # Update the final done state\n",
    "            if state not in Q_table:\n",
    "                Q_table[state] = np.array([0])\n",
    "            # Record policy result and store it as policy testing result every 100 episodes\n",
    "            total_reward_list.append(total_reward)\n",
    "            episode_no += 1\n",
    "            if episode % 100 == 0:\n",
    "                testing_reward_list.append(total_reward)\n",
    "            # Record the best policy\n",
    "            if total_reward >= max_score:\n",
    "                max_score = total_reward\n",
    "                max_episode = episode\n",
    "                max_action_list = action_list\n",
    "                max_time_list = time_list\n",
    "            print(\"Episode: \" + str(episode) + \" has time \" + str(env.time))\n",
    "\n",
    "        current_epsilon = 0\n",
    "        convergence_no = 1\n",
    "        for episode in range(max_convergence_episodes):\n",
    "            # Reset environment\n",
    "            state = env.reset()\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            action_list = []\n",
    "            time_list = []\n",
    "\n",
    "            # The Q-Table learning algorithm\n",
    "            while not done:\n",
    "                if state not in Q_table:\n",
    "                    if jobs_all_waiting(env.state[env.job_machine_allocation]):\n",
    "                        Q_table[state] = np.negative(np.full(env.action_space.n - 1, heuristic_makespan(state, env), dtype = float))\n",
    "                    else:\n",
    "                        Q_table[state] = np.negative(np.full(env.action_space.n, heuristic_makespan(state, env), dtype = float))\n",
    "                action = get_action(state, Q_table, current_epsilon)\n",
    "                # Only record the action in history if it is not a wait action\n",
    "                if action != env.action_space.n -1:\n",
    "                    action_list.append(env.legal_allocation_list[action])\n",
    "                    time_list.append(env.time)\n",
    "                    #print('Episode:{} Allocation:{} Time:{}'.format(episode, env.legal_allocation_list[action], env.time))\n",
    "                # Get new state & reward from environment\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                # Update Q-Table with new knowledge\n",
    "                Q_table = update(Q_table, state, next_state, action, reward, eta, gamma, env)\n",
    "                total_reward += reward\n",
    "                state = next_state\n",
    "            # Update the final done state\n",
    "            if state not in Q_table:\n",
    "                Q_table[state] = np.array([0])\n",
    "            if total_reward == total_reward_list[-1]:\n",
    "                convergence_no += 1\n",
    "            else:\n",
    "                convergence_no = 1\n",
    "            # Record policy result and store it as policy testing result every 100 episodes\n",
    "            total_reward_list.append(total_reward)\n",
    "            if episode % 100 == 0:\n",
    "                testing_reward_list.append(total_reward)\n",
    "            # Record the best policy\n",
    "            if total_reward >= max_score:\n",
    "                max_score = total_reward\n",
    "                max_episode = episode\n",
    "                max_action_list = action_list\n",
    "                max_time_list = time_list\n",
    "            print(\"Episode: \" + str(episode) + \" has time \" + str(env.time))\n",
    "            if convergence_no == convergence_guarantee_episodes:\n",
    "                break\n",
    "\n",
    "    print('From {}th Episode best policy has reward {}'.format(max_episode + 1, max_score))\n",
    "    for i in range(len(max_action_list)):\n",
    "        print('The allocation chose at time {} is {}'.format(max_time_list[i], max_action_list[i]))\n",
    "\n",
    "    return total_reward_list, Q_table, testing_reward_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "fourth version"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_action(state, Q, Q_indicator, epsilon):\n",
    "    \"\"\"\n",
    "    function that choose action in a smart way\n",
    "    If all jobs are waiting or done with all operations, then we will not choose the last action(wait action) since it is definitely a bad move.\n",
    "    :param state: current state which is a tuple of order 2 * number of jobs\n",
    "    :param Q: Q table which is a dictionary with states as first class keys and actions as second class keys\n",
    "    :param epsilon: randomness factor, how random the selection is\n",
    "    :param num_actions: number of actions for the current state\n",
    "    :return: an index of which action will be chosen\n",
    "    \"\"\"\n",
    "    not_fully_explored_actions_index = np.logical_not(Q_indicator[state])\n",
    "    not_fully_explored_num_actions = np.sum(not_fully_explored_actions_index)\n",
    "    if not_fully_explored_num_actions == 0:\n",
    "        epsilon = 0\n",
    "        not_fully_explored_num_actions = 1\n",
    "    # If in the state all jobs are waiting or done with all operation, choose action based on epsilon-greedy algorithm while making sure we won't choose the wait action at all\n",
    "    best_action = np.argmax(Q[state])\n",
    "    action_probabilities = not_fully_explored_actions_index * epsilon / not_fully_explored_num_actions\n",
    "    action_probabilities[best_action] += (1.0 - epsilon)\n",
    "    action = np.random.choice(np.arange(len(action_probabilities)), p = action_probabilities)\n",
    "    return action"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def update(Q, state, next_state, action, reward, eta, gamma, env):\n",
    "    \"\"\"\n",
    "    Q-table updating step\n",
    "    :param Q: Q table which is a dictionary with states as first class keys and actions as second class keys\n",
    "    :param state: current state which is a tuple of order 2 * number of jobs\n",
    "    :param next_state: next state which is a tuple of order 2 * number of jobs\n",
    "    :param action: action between current state and next state\n",
    "    :param reward: reward for the action\n",
    "    :param eta: learning rate\n",
    "    :param gamma: discounted factor\n",
    "    :param env: JSSP instance\n",
    "    :return: updated Q table\n",
    "    \"\"\"\n",
    "    # If Q-table hasn't reached the next state before, using the heuristic function to approximate the maximum Q-value of the next state\n",
    "    if next_state not in Q:\n",
    "        Q_next_state_max = -heuristic_makespan(next_state, env)\n",
    "    else:\n",
    "        Q_next_state_max = max(Q[next_state])\n",
    "\n",
    "    # Updating step\n",
    "    learning = reward + gamma * Q_next_state_max - Q[state][action]\n",
    "    Q[state][action] += eta * learning\n",
    "\n",
    "    return Q, learning"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def q_learning(env, epochs, eta = .628, gamma = 1, epsilon = .1, max_epsilon_greedy_episodes = 1000, max_convergence_episodes = 50000, convergence_guarantee_episodes = 100):\n",
    "    \"\"\"\n",
    "    q-learning algorithm that returns the best policy, and data for each episode\n",
    "    :param env: JSSP instance\n",
    "    :param epis: number of episodes\n",
    "    :param eta: learning rate, how much you accept the new value vs the old value\n",
    "    :param gamma: discount factor, balance immediate and future reward\n",
    "    :param epsilon: randomness factor, how random the selection is\n",
    "    :return: a list of total reward for each episode, the final Q_table, and a list of total reward every 100 episodes\n",
    "    \"\"\"\n",
    "    max_score = -100000\n",
    "    max_episode = -1\n",
    "    max_action_list = []\n",
    "    max_time_list = []\n",
    "\n",
    "    # Parameters of Q-learning\n",
    "    Q_table = {}\n",
    "    Q_convergence_indicator_table = {}\n",
    "    total_reward_list = [] # rewards per episode calculate\n",
    "    testing_reward_list = []\n",
    "    episode_no = 1\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Q-learning Algorithm\n",
    "        current_epsilon = epsilon\n",
    "        for episode in range(max_epsilon_greedy_episodes):\n",
    "            # Reset environment\n",
    "            state = env.reset()\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            action_list = []\n",
    "            time_list = []\n",
    "\n",
    "            # The Q-Table learning algorithm\n",
    "            while not done:\n",
    "                if state not in Q_table:\n",
    "                    if jobs_all_waiting(env.state[env.job_machine_allocation]):\n",
    "                        Q_table[state] = np.negative(np.full(env.action_space.n - 1, heuristic_makespan(state, env), dtype = float))\n",
    "                        Q_convergence_indicator_table[state] = np.full(env.action_space.n - 1, False)\n",
    "                    else:\n",
    "                        Q_table[state] = np.negative(np.full(env.action_space.n, heuristic_makespan(state, env), dtype = float))\n",
    "                        Q_convergence_indicator_table[state] = np.full(env.action_space.n, False)\n",
    "                action = get_action(state, Q_table, Q_convergence_indicator_table, current_epsilon)\n",
    "                # Only record the action in history if it is not a wait action\n",
    "                if action != env.action_space.n -1:\n",
    "                    action_list.append(env.legal_allocation_list[action])\n",
    "                    time_list.append(env.time)\n",
    "                    #print('Episode:{} Allocation:{} Time:{}'.format(episode, env.legal_allocation_list[action], env.time))\n",
    "                # Get new state & reward from environment\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                # Update Q-Table with new knowledge\n",
    "                Q_table, learning = update(Q_table, state, next_state, action, reward, eta, gamma, env)\n",
    "                if abs(learning) <= .001 and next_state in Q_convergence_indicator_table and np.all(Q_convergence_indicator_table[next_state]):\n",
    "                    Q_convergence_indicator_table[state][action] = True\n",
    "                if Q_convergence_indicator_table[state][action] == True:\n",
    "                    total_reward += round(reward + max(Q_table[next_state]))\n",
    "                    break\n",
    "                total_reward += reward\n",
    "                state = next_state\n",
    "            # Update the final done state\n",
    "            if state not in Q_table:\n",
    "                Q_table[state] = np.array([0])\n",
    "                Q_convergence_indicator_table[state] = np.array([True])\n",
    "            # Record policy result and store it as policy testing result every 100 episodes\n",
    "            total_reward_list.append(total_reward)\n",
    "            episode_no += 1\n",
    "            if episode % 100 == 0:\n",
    "                testing_reward_list.append(total_reward)\n",
    "            # Record the best policy\n",
    "            if total_reward >= max_score:\n",
    "                max_score = total_reward\n",
    "                max_episode = episode\n",
    "                max_action_list = action_list\n",
    "                max_time_list = time_list\n",
    "            print(\"Episode: \" + str(episode) + \" has time \" + str(env.time))\n",
    "\n",
    "        current_epsilon = 0\n",
    "        convergence_no = 1\n",
    "        for episode in range(max_convergence_episodes):\n",
    "            # Reset environment\n",
    "            state = env.reset()\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            action_list = []\n",
    "            time_list = []\n",
    "\n",
    "            # The Q-Table learning algorithm\n",
    "            while not done:\n",
    "                if state not in Q_table:\n",
    "                    if jobs_all_waiting(env.state[env.job_machine_allocation]):\n",
    "                        Q_table[state] = np.negative(np.full(env.action_space.n - 1, heuristic_makespan(state, env), dtype = float))\n",
    "                        Q_convergence_indicator_table[state] = np.full(env.action_space.n - 1, False)\n",
    "                    else:\n",
    "                        Q_table[state] = np.negative(np.full(env.action_space.n, heuristic_makespan(state, env), dtype = float))\n",
    "                        Q_convergence_indicator_table[state] = np.full(env.action_space.n, False)\n",
    "                action = get_action(state, Q_table, Q_convergence_indicator_table, current_epsilon)\n",
    "                # Only record the action in history if it is not a wait action\n",
    "                if action != env.action_space.n -1:\n",
    "                    action_list.append(env.legal_allocation_list[action])\n",
    "                    time_list.append(env.time)\n",
    "                    #print('Episode:{} Allocation:{} Time:{}'.format(episode, env.legal_allocation_list[action], env.time))\n",
    "                # Get new state & reward from environment\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                # Update Q-Table with new knowledge\n",
    "                Q_table, learning = update(Q_table, state, next_state, action, reward, eta, gamma, env)\n",
    "                if abs(learning) <= .001 and next_state in Q_convergence_indicator_table and np.all(Q_convergence_indicator_table[next_state]):\n",
    "                    Q_convergence_indicator_table[state][action] = True\n",
    "                if Q_convergence_indicator_table[state][action] == True:\n",
    "                    total_reward += round(reward + max(Q_table[next_state]))\n",
    "                    break\n",
    "                total_reward += reward\n",
    "                state = next_state\n",
    "            # Update the final done state\n",
    "            if state not in Q_table:\n",
    "                Q_table[state] = np.array([0])\n",
    "            if total_reward == total_reward_list[-1]:\n",
    "                convergence_no += 1\n",
    "            else:\n",
    "                convergence_no = 1\n",
    "            # Record policy result and store it as policy testing result every 100 episodes\n",
    "            total_reward_list.append(total_reward)\n",
    "            if episode % 100 == 0:\n",
    "                testing_reward_list.append(total_reward)\n",
    "            # Record the best policy\n",
    "            if total_reward >= max_score:\n",
    "                max_score = total_reward\n",
    "                max_episode = episode\n",
    "                max_action_list = action_list\n",
    "                max_time_list = time_list\n",
    "            print(\"Episode: \" + str(episode) + \" has time \" + str(env.time))\n",
    "            if convergence_no == convergence_guarantee_episodes:\n",
    "                break\n",
    "\n",
    "    print('From {}th Episode best policy has reward {}'.format(max_episode + 1, max_score))\n",
    "    for i in range(len(max_action_list)):\n",
    "        print('The allocation chose at time {} is {}'.format(max_time_list[i], max_action_list[i]))\n",
    "\n",
    "    return total_reward_list, Q_table, testing_reward_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "instance separation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def q_learning(env, epochs, eta = .628, gamma = 1, epsilon = .1, max_epsilon_greedy_episodes = 1000, max_convergence_episodes = 50000, convergence_guarantee_episodes = 100):\n",
    "    \"\"\"\n",
    "    q-learning algorithm that returns the best policy, and data for each episode\n",
    "    :param env: JSSP instance\n",
    "    :param epis: number of episodes\n",
    "    :param eta: learning rate, how much you accept the new value vs the old value\n",
    "    :param gamma: discount factor, balance immediate and future reward\n",
    "    :param epsilon: randomness factor, how random the selection is\n",
    "    :return: a list of total reward for each episode, the final Q_table, and a list of total reward every 100 episodes\n",
    "    \"\"\"\n",
    "    max_score = -100000\n",
    "    max_episode = -1\n",
    "    max_action_list = []\n",
    "    max_time_list = []\n",
    "    max_jobs_history = []\n",
    "\n",
    "    # Parameters of Q-learning\n",
    "    Q_table = {}\n",
    "    total_reward_list = [] # rewards per episode calculate\n",
    "    testing_reward_list = []\n",
    "    episode_no = 1\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Q-learning Algorithm\n",
    "        current_epsilon = epsilon\n",
    "        for episode in range(max_epsilon_greedy_episodes):\n",
    "            # Reset environment\n",
    "            state = env.reset()\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            action_list = []\n",
    "            time_list = []\n",
    "\n",
    "            # The Q-Table learning algorithm\n",
    "            while not done:\n",
    "                if state not in Q_table:\n",
    "                    if jobs_all_waiting(env.state[env.job_machine_allocation]):\n",
    "                        Q_table[state] = np.negative(np.full(env.action_space.n - 1, heuristic_makespan(state, env), dtype = float))\n",
    "                    else:\n",
    "                        Q_table[state] = np.negative(np.full(env.action_space.n, heuristic_makespan(state, env), dtype = float))\n",
    "                # print(state)\n",
    "                action = get_action(state, Q_table, current_epsilon)\n",
    "                # Only record the action in history if it is not a wait action\n",
    "                if action != env.action_space.n -1:\n",
    "                    action_list.append(env.legal_allocation_list[action])\n",
    "                    time_list.append(env.time)\n",
    "                    #print('Episode:{} Allocation:{} Time:{}'.format(episode, env.legal_allocation_list[action], env.time))\n",
    "                # Get new state & reward from environment\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                # Update Q-Table with new knowledge\n",
    "                Q_table = update(Q_table, state, next_state, action, reward, eta, gamma, env)\n",
    "                total_reward += reward\n",
    "                state = next_state\n",
    "            # Update the final done state\n",
    "            if state not in Q_table:\n",
    "                Q_table[state] = np.array([0])\n",
    "            # Record policy result and store it as policy testing result every 100 episodes\n",
    "            total_reward_list.append(total_reward)\n",
    "            episode_no += 1\n",
    "            if episode % 100 == 0:\n",
    "                testing_reward_list.append(total_reward)\n",
    "            # Record the best policy\n",
    "            if total_reward >= max_score:\n",
    "                max_score = total_reward\n",
    "                max_episode = episode\n",
    "                max_action_list = action_list\n",
    "                max_time_list = time_list\n",
    "                max_jobs_history = env.jobs_history\n",
    "            print(\"Episode: \" + str(episode) + \" has time \" + str(env.time))\n",
    "\n",
    "        current_epsilon = 0\n",
    "        convergence_no = 1\n",
    "        for episode in range(max_convergence_episodes):\n",
    "            # Reset environment\n",
    "            state = env.reset()\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            action_list = []\n",
    "            time_list = []\n",
    "\n",
    "            # The Q-Table learning algorithm\n",
    "            while not done:\n",
    "                if state not in Q_table:\n",
    "                    if jobs_all_waiting(env.state[env.job_machine_allocation]):\n",
    "                        Q_table[state] = np.negative(np.full(env.action_space.n - 1, heuristic_makespan(state, env), dtype = float))\n",
    "                    else:\n",
    "                        Q_table[state] = np.negative(np.full(env.action_space.n, heuristic_makespan(state, env), dtype = float))\n",
    "                action = get_action(state, Q_table, current_epsilon)\n",
    "                # Only record the action in history if it is not a wait action\n",
    "                if action != env.action_space.n -1:\n",
    "                    action_list.append(env.legal_allocation_list[action])\n",
    "                    time_list.append(env.time)\n",
    "                    #print('Episode:{} Allocation:{} Time:{}'.format(episode, env.legal_allocation_list[action], env.time))\n",
    "                # Get new state & reward from environment\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                # Update Q-Table with new knowledge\n",
    "                Q_table = update(Q_table, state, next_state, action, reward, eta, gamma, env)\n",
    "                total_reward += reward\n",
    "                state = next_state\n",
    "            # Update the final done state\n",
    "            if state not in Q_table:\n",
    "                Q_table[state] = np.array([0])\n",
    "            if total_reward == total_reward_list[-1]:\n",
    "                convergence_no += 1\n",
    "            else:\n",
    "                convergence_no = 1\n",
    "            # Record policy result and store it as policy testing result every 100 episodes\n",
    "            total_reward_list.append(total_reward)\n",
    "            if episode % 100 == 0:\n",
    "                testing_reward_list.append(total_reward)\n",
    "            # Record the best policy\n",
    "            if total_reward >= max_score:\n",
    "                max_score = total_reward\n",
    "                max_episode = episode\n",
    "                max_action_list = action_list\n",
    "                max_time_list = time_list\n",
    "                max_jobs_history = env.jobs_history\n",
    "            print(\"Episode: \" + str(episode) + \" has time \" + str(env.time))\n",
    "            if convergence_no == convergence_guarantee_episodes:\n",
    "                break\n",
    "\n",
    "    print('From {}th Episode best policy has reward {}'.format(max_episode + 1, max_score))\n",
    "    for i in range(len(max_action_list)):\n",
    "        print('The allocation chose at time {} is {}'.format(max_time_list[i], max_action_list[i]))\n",
    "\n",
    "    return total_reward_list, Q_table, testing_reward_list, max_jobs_history"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_end_data(separation_history_data):\n",
    "    end_times = [int(job_history[-1][-1]) for job_history in separation_history_data]\n",
    "    # end_times = [str(int(x)) for x in end_times]\n",
    "    # end_times = ' '.join(end_times) + '\\n'\n",
    "    end_machines = [job_history[-1][0] for job_history in separation_history_data]\n",
    "    # end_machines = [str(x) for x in end_machines]\n",
    "    # end_machines = ' '.join(end_machines) + '\\n'\n",
    "    return end_times, end_machines"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def write_in_job_data(file, full_job_data, start_operation, writein_total_operations):\n",
    "    full_job_data = full_job_data.split()\n",
    "    full_job_data = [int(x) for x in full_job_data]\n",
    "    operation_total = full_job_data[0]\n",
    "    del full_job_data[0]\n",
    "    assert operation_total > start_operation, 'Intermediate mode: Wrong starting operation'\n",
    "    assert operation_total > start_operation + writein_total_operations - 1, 'Intermediate mode: Wrong total operations'\n",
    "    for _ in range(start_operation):\n",
    "        available_machine_total = full_job_data[0]\n",
    "        del full_job_data[:2 * available_machine_total + 1]\n",
    "    writein_data = [writein_total_operations]\n",
    "    for _ in range(writein_total_operations):\n",
    "        available_machine_total = full_job_data[0]\n",
    "        writein_data.extend(full_job_data[:2 * available_machine_total + 1])\n",
    "        del full_job_data[:2 * available_machine_total + 1]\n",
    "    writein_data = [str(x) for x in writein_data]\n",
    "    writein_data = ' '.join(writein_data) + '\\n'\n",
    "    file.write(writein_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_separate_instance(file_name, separation, previous_separation = None, previous_separation_history_data = None):\n",
    "    file = open(file_name, 'r')\n",
    "    file_new = open('separation_instance.txt', 'w')\n",
    "    lines = file.readlines()\n",
    "    file.close()\n",
    "    job_total, machine_total = [int(x) for x in lines[0].split()]\n",
    "    start_operations = np.zeros(job_total, dtype = int)\n",
    "    dummy_operations_data = None\n",
    "    if previous_separation:\n",
    "        assert previous_separation_history_data, 'No data for previous separation'\n",
    "        start_operations += previous_separation\n",
    "        previous_end_times, previous_end_machines = get_end_data(previous_separation_history_data)\n",
    "        dummy_operations_data = [previous_end_times, previous_end_machines]\n",
    "        # file_new.writelines(dummy_operations_data)\n",
    "    file_new.write(lines[0])\n",
    "    for i in range(job_total):\n",
    "        write_in_job_data(file_new, lines[i+1], start_operations[i], separation[i])\n",
    "    file_new.close()\n",
    "    print('DOD is ', dummy_operations_data)\n",
    "    env = create_env('separation_instance.txt', initial_state_data = dummy_operations_data)\n",
    "    os.remove('separation_instance.txt')\n",
    "    return env"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env5_1 = create_separate_instance('instance5.txt', [3,3,3,3,3,3])\n",
    "training_reward_list, Q_table, testing_reward_list, max_jobs_history = q_learning(env5_1, 3)\n",
    "env5_2 = create_separate_instance('instance5.txt', [3,3,3,3,3,3], previous_separation = [3,3,3,3,3,3], previous_separation_history_data = max_jobs_history)\n",
    "training_reward_list, Q_table, testing_reward_list, max_jobs_history = q_learning(env5_2, 3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Analysis"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Random Sampling"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "random_sampling(env1, 100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## QLearning"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "training_reward_list, Q_table, testing_reward_list = q_learning(env1, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(training_reward_list)\n",
    "plt.title(\"QLearning: Training\")\n",
    "plt.xlabel(\"Episode #\")\n",
    "plt.ylabel(\"Total_reward\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(testing_reward_list)\n",
    "plt.title(\"QLearning: Testing\")\n",
    "plt.xlabel(\"Episode #\")\n",
    "plt.ylabel(\"Total_reward\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(Q_table)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}