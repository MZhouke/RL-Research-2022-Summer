{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import JSSP\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.style\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "#import plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Environment Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_env(instance_path):\n",
    "    env_name = \"JSSP-v0\"\n",
    "    env = gym.make(env_name, instance_path = instance_path)\n",
    "    print(\"Environment Created for: \", instance_path)\n",
    "    print(\"Observation space: \\n\", env.observation_space)\n",
    "    print(\"Action space: \\n\", env.action_space)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Created for:  instance1.txt\n",
      "Observation space: \n",
      " Box([-2 -2  0  0  0  0], [  2   2   2   2 100 100], (6,), int64)\n",
      "Action space: \n",
      " Discrete(8)\n",
      "Environment Created for:  instance3.txt\n",
      "Observation space: \n",
      " Box([-2 -2 -2 -2 -2 -2 -2 -2 -2 -2  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0], [ 10  10  10  10  10  10  10  10  10  10   9   9   9   9   9   9   9   9\n",
      "   9   9 100 100 100 100 100 100 100 100 100 100], (30,), int64)\n",
      "Action space: \n",
      " Discrete(260)\n",
      "Environment Created for:  instance4.txt\n",
      "Observation space: \n",
      " Box([-2 -2 -2 -2 -2 -2 -2 -2 -2 -2  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0], [  5   5   5   5   5   5   5   5   5   5   5   5   5   5   5   5   5   5\n",
      "   5   5 100 100 100 100 100 100 100 100 100 100], (30,), int64)\n",
      "Action space: \n",
      " Discrete(756)\n"
     ]
    }
   ],
   "source": [
    "env1 = create_env(\"instance1.txt\")\n",
    "env3 = create_env(\"instance3.txt\")\n",
    "env4 = create_env(\"instance4.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. Random Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def random_sampling(env, episodes):\n",
    "    env.reset()\n",
    "    max_score = -100000\n",
    "    max_episode = -1\n",
    "    max_action_list = []\n",
    "    max_time_list = []\n",
    "    for episode in range(1, episodes+1):\n",
    "        env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "        action_list = []\n",
    "        time_list = []\n",
    "        while not done:\n",
    "            #env.render()\n",
    "            action = env.action_space.sample()\n",
    "            if action != env.action_space.n -1:\n",
    "                action_list.append(env.legal_allocation_list[action])\n",
    "                time_list.append(env.time)\n",
    "                # print('Episode:{} Allocation:{} Time:{}'.format(episode, env.legal_allocation_list[action], env.time))\n",
    "            n_state, reward, done, info = env.step(action)\n",
    "            score+=reward\n",
    "        print('Episode:{} Total_reward:{}'.format(episode, score))\n",
    "        if score >= max_score:\n",
    "            max_score = score\n",
    "            max_episode = episode\n",
    "            max_action_list = action_list\n",
    "            max_time_list = time_list\n",
    "    print('From {}th Episode best policy has reward {}'.format(max_episode, max_score))\n",
    "    for i in range(len(max_action_list)):\n",
    "        print('The allocation chose at time {} is {}'.format(max_time_list[i], max_action_list[i]))\n",
    "\n",
    "random_sampling(env4, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# def createEpsilonGreedyPolicy(Q, epsilon, num_actions):\n",
    "# \t\"\"\"\n",
    "# \tCreates an epsilon-greedy policy based\n",
    "# \ton a given Q-function and epsilon.\n",
    "#\n",
    "# \tReturns a function that takes the state\n",
    "# \tas an input and returns the probabilities\n",
    "# \tfor each action in the form of a numpy array\n",
    "# \tof length of the action space(set of possible actions).\n",
    "# \t\"\"\"\n",
    "def policy(state, Q, epsilon, num_actions):\n",
    "\n",
    "    if state in Q:\n",
    "        best_action = np.argmax(Q[state])\n",
    "        Action_probabilities = np.ones(num_actions, dtype = float) * epsilon / num_actions\n",
    "        Action_probabilities[best_action] += (1.0 - epsilon)\n",
    "        return Action_probabilities\n",
    "\n",
    "    Action_probabilities = np.ones(num_actions, dtype = float) / num_actions\n",
    "    return Action_probabilities"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def update(Q, state, next_state, action, reward, eta, gamma):\n",
    "\n",
    "    if next_state not in Q:\n",
    "        Q_next_state_max = -1\n",
    "    else:\n",
    "        Q_next_state_max = max(Q[next_state])\n",
    "\n",
    "    Q[state][action] = Q[state][action] + eta * (reward + gamma * Q_next_state_max - Q[state][action])\n",
    "\n",
    "    return Q"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 has time 65\n",
      "Episode: 1 has time 69\n",
      "Episode: 2 has time 51\n",
      "Episode: 3 has time 76\n",
      "Episode: 4 has time 53\n",
      "Episode: 5 has time 52\n",
      "Episode: 6 has time 76\n",
      "Episode: 7 has time 69\n",
      "Episode: 8 has time 77\n",
      "Episode: 9 has time 77\n",
      "Episode: 10 has time 53\n",
      "Episode: 11 has time 63\n",
      "Episode: 12 has time 67\n",
      "Episode: 13 has time 62\n",
      "Episode: 14 has time 65\n",
      "Episode: 15 has time 79\n",
      "Episode: 16 has time 56\n",
      "Episode: 17 has time 84\n",
      "Episode: 18 has time 63\n",
      "Episode: 19 has time 69\n",
      "Episode: 20 has time 75\n",
      "Episode: 21 has time 69\n",
      "Episode: 22 has time 62\n",
      "Episode: 23 has time 75\n",
      "Episode: 24 has time 76\n",
      "Episode: 25 has time 70\n",
      "Episode: 26 has time 62\n",
      "Episode: 27 has time 62\n",
      "Episode: 28 has time 80\n",
      "Episode: 29 has time 64\n",
      "Episode: 30 has time 68\n",
      "Episode: 31 has time 81\n",
      "Episode: 32 has time 66\n",
      "Episode: 33 has time 61\n",
      "Episode: 34 has time 94\n",
      "Episode: 35 has time 60\n",
      "Episode: 36 has time 78\n",
      "Episode: 37 has time 65\n",
      "Episode: 38 has time 81\n",
      "Episode: 39 has time 65\n",
      "Episode: 40 has time 74\n",
      "Episode: 41 has time 75\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [11]\u001B[0m, in \u001B[0;36m<cell line: 59>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     55\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mThe allocation chose at time \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m is \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(max_time_list[i], max_action_list[i]))\n\u001B[1;32m     57\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m total_reward_list, Q_table\n\u001B[0;32m---> 59\u001B[0m total_reward_list, Q_table \u001B[38;5;241m=\u001B[39m \u001B[43mq_learning\u001B[49m\u001B[43m(\u001B[49m\u001B[43menv1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m500\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[0;32mIn [11]\u001B[0m, in \u001B[0;36mq_learning\u001B[0;34m(env, epis)\u001B[0m\n\u001B[1;32m     49\u001B[0m         max_action_list \u001B[38;5;241m=\u001B[39m action_list\n\u001B[1;32m     50\u001B[0m         max_time_list \u001B[38;5;241m=\u001B[39m time_list\n\u001B[0;32m---> 51\u001B[0m     \u001B[38;5;28;43mprint\u001B[39;49m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpisode: \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(episode) \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m has time \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(env\u001B[38;5;241m.\u001B[39mtime))\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mFrom \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124mth Episode best policy has reward \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(max_episode \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m, max_score))\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(max_action_list)):\n",
      "Input \u001B[0;32mIn [11]\u001B[0m, in \u001B[0;36mq_learning\u001B[0;34m(env, epis)\u001B[0m\n\u001B[1;32m     49\u001B[0m         max_action_list \u001B[38;5;241m=\u001B[39m action_list\n\u001B[1;32m     50\u001B[0m         max_time_list \u001B[38;5;241m=\u001B[39m time_list\n\u001B[0;32m---> 51\u001B[0m     \u001B[38;5;28;43mprint\u001B[39;49m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpisode: \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(episode) \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m has time \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(env\u001B[38;5;241m.\u001B[39mtime))\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mFrom \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124mth Episode best policy has reward \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(max_episode \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m, max_score))\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(max_action_list)):\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_310_64.pyx:1179\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_310_64.SafeCallWrapper.__call__\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_310_64.pyx:620\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_310_64.PyDBFrame.trace_dispatch\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_310_64.pyx:929\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_310_64.PyDBFrame.trace_dispatch\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_310_64.pyx:920\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_310_64.PyDBFrame.trace_dispatch\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_310_64.pyx:317\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_310_64.PyDBFrame.do_wait_suspend\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/Library/Application Support/JetBrains/Toolbox/apps/PyCharm-P/ch-0/222.3345.131/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py:1160\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[1;32m   1157\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[1;32m   1159\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[0;32m-> 1160\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Application Support/JetBrains/Toolbox/apps/PyCharm-P/ch-0/222.3345.131/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py:1175\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[1;32m   1172\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[1;32m   1174\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[0;32m-> 1175\u001B[0m         \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1177\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[1;32m   1179\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "def q_learning(env, epis):\n",
    "\n",
    "    max_score = -100000\n",
    "    max_episode = -1\n",
    "    max_action_list = []\n",
    "    max_time_list = []\n",
    "\n",
    "    # 1. Load Environment and Q-table structure\n",
    "    Q_table = {}\n",
    "    # 2. Parameters of Q-learning\n",
    "    eta = .628\n",
    "    gamma = 1\n",
    "    epsilon = .1\n",
    "    decay_rate = .0001\n",
    "    total_reward_list = [] # rewards per episode calculate\n",
    "    # 3. Q-learning Algorithm\n",
    "    for episode in range(epis):\n",
    "        # Reset environment\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        action_list = []\n",
    "        time_list = []\n",
    "\n",
    "        # The Q-Table learning algorithm\n",
    "        while not done:\n",
    "            if epsilon >= decay_rate:\n",
    "                epsilon -= decay_rate\n",
    "            if state not in Q_table:\n",
    "                Q_table[state] = np.zeros(env.action_space.n)\n",
    "            action_probabilities = policy(state, Q_table, epsilon, env.action_space.n)\n",
    "            action = np.random.choice(np.arange(len(action_probabilities)), p = action_probabilities)\n",
    "            if action != env.action_space.n -1:\n",
    "                action_list.append(env.legal_allocation_list[action])\n",
    "                time_list.append(env.time)\n",
    "                #print('Episode:{} Allocation:{} Time:{}'.format(episode, env.legal_allocation_list[action], env.time))\n",
    "            #Get new state & reward from environment\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            #Update Q-Table with new knowledge\n",
    "            Q_table = update(Q_table, state, next_state, action, reward, eta, gamma)\n",
    "            if eta > .01:\n",
    "                eta -= decay_rate\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "        total_reward_list.append(total_reward)\n",
    "        if total_reward >= max_score:\n",
    "            max_score = total_reward\n",
    "            max_episode = episode\n",
    "            max_action_list = action_list\n",
    "            max_time_list = time_list\n",
    "        print(\"Episode: \" + str(episode) + \" has time \" + str(env.time))\n",
    "\n",
    "    print('From {}th Episode best policy has reward {}'.format(max_episode + 1, max_score))\n",
    "    for i in range(len(max_action_list)):\n",
    "        print('The allocation chose at time {} is {}'.format(max_time_list[i], max_action_list[i]))\n",
    "\n",
    "    return total_reward_list, Q_table\n",
    "\n",
    "total_reward_list, Q_table = q_learning(env1, 500)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "<function matplotlib.pyplot.show(close=None, block=None)>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(total_reward_list)\n",
    "plt.show\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(total_reward_list)\n",
    "plt.show"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(Q_table)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DQN"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "\n",
    "model = DQN(\"MlpPolicy\", env1, verbose=1)\n",
    "model.learn(total_timesteps=10000, log_interval=4)\n",
    "model.save(\"dqn_cartpole\")\n",
    "\n",
    "del model # remove to demonstrate saving and loading\n",
    "\n",
    "model = DQN.load(\"dqn_cartpole\")\n",
    "\n",
    "obs = env1.reset()\n",
    "for episode in range(500):\n",
    "    env1.reset()\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, info = env1.step(action)\n",
    "        total_reward += reward\n",
    "    print(\"Episode: \" + str(episode) + \" has reward \" + str(total_reward))\n",
    "        # env.render()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Q Learning from web"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def createEpsilonGreedyPolicy(Q, epsilon, num_actions, env):\n",
    "\t\"\"\"\n",
    "\tCreates an epsilon-greedy policy based\n",
    "\ton a given Q-function and epsilon.\n",
    "\n",
    "\tReturns a function that takes the state\n",
    "\tas an input and returns the probabilities\n",
    "\tfor each action in the form of a numpy array\n",
    "\tof length of the action space(set of possible actions).\n",
    "\t\"\"\"\n",
    "\tdef policyFunction(state):\n",
    "\n",
    "\t\tAction_probabilities = np.ones(num_actions,\n",
    "\t\t\t\tdtype = float) * epsilon / num_actions\n",
    "\n",
    "\t\tbest_action = np.argmax(Q[state])\n",
    "\t\tAction_probabilities[best_action] += (1.0 - epsilon)\n",
    "\t\treturn Action_probabilities\n",
    "\n",
    "\treturn policyFunction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def qLearning(env, num_episodes, discount_factor = 1.0,\n",
    "\t\t\t\t\t\t\talpha = 0.6, epsilon = 0.1):\n",
    "\t\"\"\"\n",
    "\tQ-Learning algorithm: Off-policy TD control.\n",
    "\tFinds the optimal greedy policy while improving\n",
    "\tfollowing an epsilon-greedy policy\"\"\"\n",
    "\n",
    "\t# Action value function\n",
    "\t# A nested dictionary that maps\n",
    "\t# state -> (action -> action-value).\n",
    "\tQ = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "\t# Keeps track of useful statistics\n",
    "\tstats = {\n",
    "            \"episode_lengths\" : np.zeros(num_episodes),\n",
    "\t\t    \"episode_rewards\" : np.zeros(num_episodes)}\n",
    "\n",
    "\t# Create an epsilon greedy policy function\n",
    "\t# appropriately for environment action space\n",
    "\tpolicy = createEpsilonGreedyPolicy(Q, epsilon, env.action_space.n, env)\n",
    "\n",
    "\t# For every episode\n",
    "\tfor ith_episode in range(num_episodes):\n",
    "\n",
    "\t\t# Reset the environment and pick the first action\n",
    "\t\tstate = env.reset()\n",
    "\n",
    "\t\tfor t in itertools.count():\n",
    "\n",
    "\t\t\t# get probabilities of all actions from current state\n",
    "\t\t\taction_probabilities = policy(state)\n",
    "\n",
    "\t\t\t# choose action according to\n",
    "\t\t\t# the probability distribution\n",
    "\t\t\taction = np.random.choice(np.arange(\n",
    "\t\t\t\t\tlen(action_probabilities)),\n",
    "\t\t\t\t\tp = action_probabilities)\n",
    "\n",
    "\t\t\t# take action and get reward, transit to next state\n",
    "\t\t\tnext_state, reward, done, _ = env.step(action)\n",
    "\n",
    "\t\t\t# Update statistics\n",
    "\t\t\tstats[\"episode_rewards\"][ith_episode] += reward\n",
    "\t\t\tstats[\"episode_lengths\"][ith_episode] = t\n",
    "\n",
    "\t\t\t# TD Update\n",
    "\t\t\tbest_next_action = np.argmax(Q[next_state])\n",
    "\t\t\ttd_target = reward + discount_factor * Q[next_state][best_next_action]\n",
    "\t\t\ttd_delta = td_target - Q[state][action]\n",
    "\t\t\tQ[state][action] += alpha * td_delta\n",
    "\n",
    "\t\t\t# done is True if episode terminated\n",
    "\t\t\tif done:\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\t\tstate = next_state\n",
    "\n",
    "\treturn Q, stats"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Q, stats = qLearning(env1, 1000)\n",
    "plt.plot(stats[\"episode_lengths\"])\n",
    "plt.plot(stats[\"episode_rewards\"])\n",
    "plt.show"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(np.append(np.array([1,2]),np.array([1,2])))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sum([1 for machine in np.array([-1, -1, -1, 2]) if machine >= 0])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Q, stats = qLearning(env1, 1000)\n",
    "plt.plot(stats[\"episode_lengths\"])\n",
    "plt.plot(stats[\"episode_rewards\"])\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Q Learning from web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def createEpsilonGreedyPolicy(Q, epsilon, num_actions, env):\n",
    "\t\"\"\"\n",
    "\tCreates an epsilon-greedy policy based\n",
    "\ton a given Q-function and epsilon.\n",
    "\n",
    "\tReturns a function that takes the state\n",
    "\tas an input and returns the probabilities\n",
    "\tfor each action in the form of a numpy array\n",
    "\tof length of the action space(set of possible actions).\n",
    "\t\"\"\"\n",
    "\tdef policyFunction(state):\n",
    "\n",
    "\t\tAction_probabilities = np.ones(num_actions,\n",
    "\t\t\t\tdtype = float) * epsilon / num_actions\n",
    "\n",
    "\t\tbest_action = np.argmax(Q[state])\n",
    "\t\tAction_probabilities[best_action] += (1.0 - epsilon)\n",
    "\t\treturn Action_probabilities\n",
    "\n",
    "\treturn policyFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def qLearning(env, num_episodes, discount_factor = 1.0,\n",
    "\t\t\t\t\t\t\talpha = 0.6, epsilon = 0.1):\n",
    "\t\"\"\"\n",
    "\tQ-Learning algorithm: Off-policy TD control.\n",
    "\tFinds the optimal greedy policy while improving\n",
    "\tfollowing an epsilon-greedy policy\"\"\"\n",
    "\n",
    "\t# Action value function\n",
    "\t# A nested dictionary that maps\n",
    "\t# state -> (action -> action-value).\n",
    "\tQ = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "\t# Keeps track of useful statistics\n",
    "\tstats = {\n",
    "            \"episode_lengths\" : np.zeros(num_episodes),\n",
    "\t\t    \"episode_rewards\" : np.zeros(num_episodes)}\n",
    "\n",
    "\t# Create an epsilon greedy policy function\n",
    "\t# appropriately for environment action space\n",
    "\tpolicy = createEpsilonGreedyPolicy(Q, epsilon, env.action_space.n, env)\n",
    "\n",
    "\t# For every episode\n",
    "\tfor ith_episode in range(num_episodes):\n",
    "\n",
    "\t\t# Reset the environment and pick the first action\n",
    "\t\tstate = env.reset()\n",
    "\n",
    "\t\tfor t in itertools.count():\n",
    "\n",
    "\t\t\t# get probabilities of all actions from current state\n",
    "\t\t\taction_probabilities = policy(state)\n",
    "\n",
    "\t\t\t# choose action according to\n",
    "\t\t\t# the probability distribution\n",
    "\t\t\taction = np.random.choice(np.arange(\n",
    "\t\t\t\t\tlen(action_probabilities)),\n",
    "\t\t\t\t\tp = action_probabilities)\n",
    "\n",
    "\t\t\t# take action and get reward, transit to next state\n",
    "\t\t\tnext_state, reward, done, _ = env.step(action)\n",
    "\n",
    "\t\t\t# Update statistics\n",
    "\t\t\tstats[\"episode_rewards\"][ith_episode] += reward\n",
    "\t\t\tstats[\"episode_lengths\"][ith_episode] = t\n",
    "\n",
    "\t\t\t# TD Update\n",
    "\t\t\tbest_next_action = np.argmax(Q[next_state])\n",
    "\t\t\ttd_target = reward + discount_factor * Q[next_state][best_next_action]\n",
    "\t\t\ttd_delta = td_target - Q[state][action]\n",
    "\t\t\tQ[state][action] += alpha * td_delta\n",
    "\n",
    "\t\t\t# done is True if episode terminated\n",
    "\t\t\tif done:\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\t\tstate = next_state\n",
    "\n",
    "\treturn Q, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Q, stats = qLearning(env1, 1000)\n",
    "plt.plot(stats[\"episode_lengths\"])\n",
    "plt.plot(stats[\"episode_rewards\"])\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Q Learning from web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def createEpsilonGreedyPolicy(Q, epsilon, num_actions, env):\n",
    "\t\"\"\"\n",
    "\tCreates an epsilon-greedy policy based\n",
    "\ton a given Q-function and epsilon.\n",
    "\n",
    "\tReturns a function that takes the state\n",
    "\tas an input and returns the probabilities\n",
    "\tfor each action in the form of a numpy array\n",
    "\tof length of the action space(set of possible actions).\n",
    "\t\"\"\"\n",
    "\tdef policyFunction(state):\n",
    "\n",
    "\t\tAction_probabilities = np.ones(num_actions,\n",
    "\t\t\t\tdtype = float) * epsilon / num_actions\n",
    "\n",
    "\t\tbest_action = np.argmax(Q[state])\n",
    "\t\tAction_probabilities[best_action] += (1.0 - epsilon)\n",
    "\t\treturn Action_probabilities\n",
    "\n",
    "\treturn policyFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def qLearning(env, num_episodes, discount_factor = 1.0,\n",
    "\t\t\t\t\t\t\talpha = 0.6, epsilon = 0.1):\n",
    "\t\"\"\"\n",
    "\tQ-Learning algorithm: Off-policy TD control.\n",
    "\tFinds the optimal greedy policy while improving\n",
    "\tfollowing an epsilon-greedy policy\"\"\"\n",
    "\n",
    "\t# Action value function\n",
    "\t# A nested dictionary that maps\n",
    "\t# state -> (action -> action-value).\n",
    "\tQ = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "\t# Keeps track of useful statistics\n",
    "\tstats = {\n",
    "            \"episode_lengths\" : np.zeros(num_episodes),\n",
    "\t\t    \"episode_rewards\" : np.zeros(num_episodes)}\n",
    "\n",
    "\t# Create an epsilon greedy policy function\n",
    "\t# appropriately for environment action space\n",
    "\tpolicy = createEpsilonGreedyPolicy(Q, epsilon, env.action_space.n, env)\n",
    "\n",
    "\t# For every episode\n",
    "\tfor ith_episode in range(num_episodes):\n",
    "\n",
    "\t\t# Reset the environment and pick the first action\n",
    "\t\tstate = env.reset()\n",
    "\n",
    "\t\tfor t in itertools.count():\n",
    "\n",
    "\t\t\t# get probabilities of all actions from current state\n",
    "\t\t\taction_probabilities = policy(state)\n",
    "\n",
    "\t\t\t# choose action according to\n",
    "\t\t\t# the probability distribution\n",
    "\t\t\taction = np.random.choice(np.arange(\n",
    "\t\t\t\t\tlen(action_probabilities)),\n",
    "\t\t\t\t\tp = action_probabilities)\n",
    "\n",
    "\t\t\t# take action and get reward, transit to next state\n",
    "\t\t\tnext_state, reward, done, _ = env.step(action)\n",
    "\n",
    "\t\t\t# Update statistics\n",
    "\t\t\tstats[\"episode_rewards\"][ith_episode] += reward\n",
    "\t\t\tstats[\"episode_lengths\"][ith_episode] = t\n",
    "\n",
    "\t\t\t# TD Update\n",
    "\t\t\tbest_next_action = np.argmax(Q[next_state])\n",
    "\t\t\ttd_target = reward + discount_factor * Q[next_state][best_next_action]\n",
    "\t\t\ttd_delta = td_target - Q[state][action]\n",
    "\t\t\tQ[state][action] += alpha * td_delta\n",
    "\n",
    "\t\t\t# done is True if episode terminated\n",
    "\t\t\tif done:\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\t\tstate = next_state\n",
    "\n",
    "\treturn Q, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def createEpsilonGreedyPolicy(Q, epsilon, num_actions, env):\n",
    "\t\"\"\"\n",
    "\tCreates an epsilon-greedy policy based\n",
    "\ton a given Q-function and epsilon.\n",
    "\n",
    "\tReturns a function that takes the state\n",
    "\tas an input and returns the probabilities\n",
    "\tfor each action in the form of a numpy array\n",
    "\tof length of the action space(set of possible actions).\n",
    "\t\"\"\"\n",
    "\tdef policyFunction(state):\n",
    "\n",
    "\t\tAction_probabilities = np.ones(num_actions,\n",
    "\t\t\t\tdtype = float) * epsilon / num_actions\n",
    "\n",
    "\t\tbest_action = np.argmax(Q[state])\n",
    "\t\tAction_probabilities[best_action] += (1.0 - epsilon)\n",
    "\t\treturn Action_probabilities\n",
    "\n",
    "\treturn policyFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def qLearning(env, num_episodes, discount_factor = 1.0,\n",
    "\t\t\t\t\t\t\talpha = 0.6, epsilon = 0.1):\n",
    "\t\"\"\"\n",
    "\tQ-Learning algorithm: Off-policy TD control.\n",
    "\tFinds the optimal greedy policy while improving\n",
    "\tfollowing an epsilon-greedy policy\"\"\"\n",
    "\n",
    "\t# Action value function\n",
    "\t# A nested dictionary that maps\n",
    "\t# state -> (action -> action-value).\n",
    "\tQ = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "\t# Keeps track of useful statistics\n",
    "\tstats = {\n",
    "            \"episode_lengths\" : np.zeros(num_episodes),\n",
    "\t\t    \"episode_rewards\" : np.zeros(num_episodes)}\n",
    "\n",
    "\t# Create an epsilon greedy policy function\n",
    "\t# appropriately for environment action space\n",
    "\tpolicy = createEpsilonGreedyPolicy(Q, epsilon, env.action_space.n, env)\n",
    "\n",
    "\t# For every episode\n",
    "\tfor ith_episode in range(num_episodes):\n",
    "\n",
    "\t\t# Reset the environment and pick the first action\n",
    "\t\tstate = env.reset()\n",
    "\n",
    "\t\tfor t in itertools.count():\n",
    "\n",
    "\t\t\t# get probabilities of all actions from current state\n",
    "\t\t\taction_probabilities = policy(state)\n",
    "\n",
    "\t\t\t# choose action according to\n",
    "\t\t\t# the probability distribution\n",
    "\t\t\taction = np.random.choice(np.arange(\n",
    "\t\t\t\t\tlen(action_probabilities)),\n",
    "\t\t\t\t\tp = action_probabilities)\n",
    "\n",
    "\t\t\t# take action and get reward, transit to next state\n",
    "\t\t\tnext_state, reward, done, _ = env.step(action)\n",
    "\n",
    "\t\t\t# Update statistics\n",
    "\t\t\tstats[\"episode_rewards\"][ith_episode] += reward\n",
    "\t\t\tstats[\"episode_lengths\"][ith_episode] = t\n",
    "\n",
    "\t\t\t# TD Update\n",
    "\t\t\tbest_next_action = np.argmax(Q[next_state])\n",
    "\t\t\ttd_target = reward + discount_factor * Q[next_state][best_next_action]\n",
    "\t\t\ttd_delta = td_target - Q[state][action]\n",
    "\t\t\tQ[state][action] += alpha * td_delta\n",
    "\n",
    "\t\t\t# done is True if episode terminated\n",
    "\t\t\tif done:\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\t\tstate = next_state\n",
    "\n",
    "\treturn Q, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Q, stats = qLearning(env1, 1000)\n",
    "plt.plot(stats[\"episode_lengths\"])\n",
    "plt.plot(stats[\"episode_rewards\"])\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Q Learning from web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def createEpsilonGreedyPolicy(Q, epsilon, num_actions, env):\n",
    "\t\"\"\"\n",
    "\tCreates an epsilon-greedy policy based\n",
    "\ton a given Q-function and epsilon.\n",
    "\n",
    "\tReturns a function that takes the state\n",
    "\tas an input and returns the probabilities\n",
    "\tfor each action in the form of a numpy array\n",
    "\tof length of the action space(set of possible actions).\n",
    "\t\"\"\"\n",
    "\tdef policyFunction(state):\n",
    "\n",
    "\t\tAction_probabilities = np.ones(num_actions,\n",
    "\t\t\t\tdtype = float) * epsilon / num_actions\n",
    "\n",
    "\t\tbest_action = np.argmax(Q[state])\n",
    "\t\tAction_probabilities[best_action] += (1.0 - epsilon)\n",
    "\t\treturn Action_probabilities\n",
    "\n",
    "\treturn policyFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def qLearning(env, num_episodes, discount_factor = 1.0,\n",
    "\t\t\t\t\t\t\talpha = 0.6, epsilon = 0.1):\n",
    "\t\"\"\"\n",
    "\tQ-Learning algorithm: Off-policy TD control.\n",
    "\tFinds the optimal greedy policy while improving\n",
    "\tfollowing an epsilon-greedy policy\"\"\"\n",
    "\n",
    "\t# Action value function\n",
    "\t# A nested dictionary that maps\n",
    "\t# state -> (action -> action-value).\n",
    "\tQ = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "\t# Keeps track of useful statistics\n",
    "\tstats = {\n",
    "            \"episode_lengths\" : np.zeros(num_episodes),\n",
    "\t\t    \"episode_rewards\" : np.zeros(num_episodes)}\n",
    "\n",
    "\t# Create an epsilon greedy policy function\n",
    "\t# appropriately for environment action space\n",
    "\tpolicy = createEpsilonGreedyPolicy(Q, epsilon, env.action_space.n, env)\n",
    "\n",
    "\t# For every episode\n",
    "\tfor ith_episode in range(num_episodes):\n",
    "\n",
    "\t\t# Reset the environment and pick the first action\n",
    "\t\tstate = env.reset()\n",
    "\n",
    "\t\tfor t in itertools.count():\n",
    "\n",
    "\t\t\t# get probabilities of all actions from current state\n",
    "\t\t\taction_probabilities = policy(state)\n",
    "\n",
    "\t\t\t# choose action according to\n",
    "\t\t\t# the probability distribution\n",
    "\t\t\taction = np.random.choice(np.arange(\n",
    "\t\t\t\t\tlen(action_probabilities)),\n",
    "\t\t\t\t\tp = action_probabilities)\n",
    "\n",
    "\t\t\t# take action and get reward, transit to next state\n",
    "\t\t\tnext_state, reward, done, _ = env.step(action)\n",
    "\n",
    "\t\t\t# Update statistics\n",
    "\t\t\tstats[\"episode_rewards\"][ith_episode] += reward\n",
    "\t\t\tstats[\"episode_lengths\"][ith_episode] = t\n",
    "\n",
    "\t\t\t# TD Update\n",
    "\t\t\tbest_next_action = np.argmax(Q[next_state])\n",
    "\t\t\ttd_target = reward + discount_factor * Q[next_state][best_next_action]\n",
    "\t\t\ttd_delta = td_target - Q[state][action]\n",
    "\t\t\tQ[state][action] += alpha * td_delta\n",
    "\n",
    "\t\t\t# done is True if episode terminated\n",
    "\t\t\tif done:\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\t\tstate = next_state\n",
    "\n",
    "\treturn Q, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Q, stats = qLearning(env1, 1000)\n",
    "plt.plot(stats[\"episode_lengths\"])\n",
    "plt.plot(stats[\"episode_rewards\"])\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}